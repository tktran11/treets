# AUTOGENERATED! DO NOT EDIT! File to edit: ../00_core.ipynb.

# %% ../00_core.ipynb 3
# allows for type hinting annotations without breaking functionality
from __future__ import annotations

# %% auto 0
__all__ = ['file_loader', 'find_date', 'find_float_time', 'week_from_start', 'find_phase_duration', 'load_food_data',
           'in_good_logging_day', 'FoodParser', 'clean_loggings', 'get_types', 'count_caloric_entries',
           'mean_daily_eating_duration', 'std_daily_eating_duration', 'earliest_entry', 'mean_first_cal',
           'std_first_cal', 'mean_last_cal', 'std_last_cal', 'logging_day_counts', 'find_missing_logging_days',
           'good_lwa_day_counts', 'filtering_usable_data', 'prepare_baseline_and_intervention_usable_data',
           'users_sorted_by_logging', 'eating_intervals_percentile', 'first_cal_analysis_summary',
           'last_cal_analysis_summary', 'summarize_data', 'summarize_data_with_experiment_phases',
           'first_cal_mean_with_error_bar', 'last_cal_mean_with_error_bar', 'first_cal_analysis_variability_plot',
           'last_cal_analysis_variability_plot', 'first_cal_avg_histplot', 'first_cal_sample_distplot',
           'last_cal_avg_histplot', 'last_cal_sample_distplot', 'swarmplot']

# %% ../00_core.ipynb 4
import warnings
warnings.filterwarnings('ignore')

import re
import os
import glob
import string
import datetime
import wordsegment

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import nltk
nltk.download('words', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt', quiet=True)
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords

wordsegment.load()

# %% ../00_core.ipynb 6
def file_loader(data_source: str|pd.DataFrame) -> pd.DataFrame:
    """
    Flexible file loader able to read a single file path or folder path.
    Accepts .csv and .json file format loading.
    
    Parameters
    ----------
    data_source
        String file or folder path. Single .json or .csv paths create a pd.DataFrame. 
        Folder paths with files matching the input pattern are read together into a single pd.DataFrame.
        Existing dataframes are read as is.
        
        
    Returns
    -------
    df
        A single dataframe consisting of all data matching the provided file or folder path.
    
    """
    if isinstance(data_source, str):
        data_lst = glob.glob(data_source)
        dfs = []
        for x in data_lst:
            if x[-4:] == '.csv':
                dfs.append(pd.read_csv(x))
            elif x[-5:] == '.json':
                json_file = pd.read_json(x)
                if not isinstance(json_file, pd.DataFrame):
                    return json_file
                dfs.append(json_file)
        df = pd.concat(dfs).reset_index(drop=True)
    else:
        df = data_source

        
    return df

# %% ../00_core.ipynb 13
def find_date(data_source: str|pd.DataFrame, h:int = 4, date_col:int = 5) -> pd.Series:
    """
    Extracts date from a datetime column and after shifting datetime by 'h' hours.
    (A day starts 'h' hours early if 'h' is negative, or 'h' hours later if 'h' is
    positive.)
    
    Parameters
    ----------
    data_source
        String file or folder path. Single .json or .csv paths create a pd.DataFrame. 
        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing
        dataframes are read as is.
    h
        Number of hours to shift the definition for 'date' by. For example, h = 4 would shift days so that time membership
        to each date starts at 4:00 AM and ends at 3:59:59 AM the next calendar day.
    date_col
        Column number for existing datetime column in provided data source. Default column
        number is defined by the accompanying HOWTO document for TREETS.
    
    
    Returns
    -------
    date
        Series of dates.
    """
    df = file_loader(data_source)
    # fifth column of food log dataframes should represent date/time in a 24 hour system
    col = df.columns[date_col]
    if df[col].dtype == 'O':
        raise TypeError("'{}' column must be converted to datetime object".format(col))
        
    def find_date(d, h):
        if h > 0:
            if d.hour < h:
                return d.date() - pd.Timedelta('1 day')
        if h < 0:
            if d.hour+1 > (24+h):
                return d.date() + pd.Timedelta('1 day')
        return d.date()
    return df[col].apply(find_date, args=([h]))

# %% ../00_core.ipynb 18
def find_float_time(data_source, h = 4, date_col = 5):
    """
    Description:\n
        Extract time information from a column and shift each time by h hours. (Day starts h hours early if h is negative and h hours late if h is positive)\n
        
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or pandas dataframe format\n
        - h(int) : hours to shift the date. For example, when h = 4, everyday starts at 4 and ends at 28. When h = -4, everyday starts at -4 and ends at 20.
        
    Return:\n
        - a pandas series with an index matching data_source; floating point time (clock hours) running from h to h per day.\n

    Requirements:\n
        Elements in col should be pd.datetime objects
        
    jf-updated-2023-04-17
            
    """
    df = file_loader(data_source)
    # fifth column of food log dataframes should represent date/time in a 24 hour system
    col = df.columns[date_col]
    if df[col].dtype == 'O':
        raise TypeError("'{}' column must be converted to datetime object firsly".format(col))
    local_time = df[col].apply(lambda x: pd.Timedelta(x.time().isoformat()).total_seconds() /3600.)
    if h > 0:
        local_time = np.where(local_time < h, 24+ local_time, local_time)
        return pd.Series(local_time, index=df.index) #jf added index= to prevent mistaken assignments when data source and target df have a non-trivial index
    if h < 0:
        local_time = np.where(local_time > (24+h), local_time-24., local_time)
        return pd.Series(local_time, index=df.index)
    return local_time
    

# %% ../00_core.ipynb 23
def week_from_start(data_source, identifier = 1):
        """
        Description:\n
            Calculate the number of weeks for each logging since the first day of the logging for each participant(identifier). The returned values for loggings from the first week are 1. 
        Input:\n
            - data_source (str, pandas df): input path, file in pickle, csv or pandas dataframe format\n
            - col (str): column name that contains date information from the data_source dataframe.
            - identifier (str): unique_id or ID, or name that identifies people.

        Return:\n
            - a numpy array represents the date extracted from col.\n
        """
        
        df = file_loader(data_source)
        if 'date' not in df.columns:
            raise NameError("There must exist a 'date' column.")
        col = df.columns[df.columns.get_loc('date')]
        identifier = df.columns[identifier]
        # Handle week from start
        df_dic = dict(df.groupby(identifier)[col].agg(np.min))

        def count_weeks(s):
            return (s.date - df_dic[s[identifier]]).days // 7 + 1
    
        return df.apply(count_weeks, axis = 1)

# %% ../00_core.ipynb 26
def find_phase_duration(df):
    """
    Description:\n
        This is a function that calculates how many days each phase in the study took. Result includes the start and end date for that phase.
    
    Input:\n
        - df(pandas df) : information dataframe that contains columns: Start_Day, End_day
    
    Output:\n
        - a dataframe contains the phase_duration column.
        
    Requirement:\n
        - 'Start_day' and 'End_day' column exist in the df.
        - 'phase_duration' column exists in the df.
    """
    # column order is specified in our how-to document for data from collaborators
    start_day = df.columns[4]
    end_day = df.columns[5]
    
    # checking if type conversion from string to datetime is necessary
    if df[start_day].apply(lambda x: isinstance(x, str)).any():
        df[start_day] = pd.to_datetime(df[start_day])
    
    if df[end_day].apply(lambda x: isinstance(x, str)).any():
        df[end_day] = pd.to_datetime(df[end_day])
    
    df['phase_duration'] = df[end_day] - df[start_day] + pd.Timedelta("1 days")
    return df

# %% ../00_core.ipynb 29
def load_food_data(data_source, h, identifier = 1, datetime_col = 5):
    """
    Description:\n
        Load food data and output processed data in a dataframe.\n
    
        Process includes:\n
        1. Dropping 'foodimage_file_name' column.\n
        2. Handling the format of time by deleting am/pm by generating a new column, 'original_logtime_notz'\n
        3. Generating the date column with possible hour shifts, 'date'\n
        4. Converting time into float number into a new column with possible hour shifts, 'float_time'\n
        5. Converting time to a format of HH:MM:SS, 'time'\n
        6. Generating the column 'week_from_start' that contains the week number that the participants input the food item.\n
        7. Generating 'year' column based on the input data.\n

    Input:\n
        - data_source (str or pandas df): input path, csv file\n
        - identifier(str): id-like column that's used to identify a subject.\n
        - datetime_col(str): column that contains date and time in string format.\n
        - h(int) : hours to shift the date. For example, when h = 4, everyday starts and ends 4 hours later than normal.
        
    Output:\n
        - the processed dataframe in pandas df format.\n

    Requirements:\n
        data_source file must have the following columns:\n
            - foodimage_file_name\n
            - original_logtime\n
            - date\n
            - unique_code\n
    """
    food_all = file_loader(data_source)
    # identifier column(s) should be 0 and 1, with 1 being study specific
    identifier = food_all.columns[identifier]
    # fifth column of food log dataframes should represent date/time in a 24 hour system
    datetime_col = food_all.columns[datetime_col]
        
    try:
        food_all = food_all.drop(columns = ['foodimage_file_name'])
    except KeyError:
        pass
    
    def handle_time(s):
        """
        helper function to get rid of am/pm in the end of each time string
        """
        tmp_s = s.replace('p.m.', '').replace('a.m.', '')
        try:
            return pd.to_datetime(' '.join(tmp_s.split()[:2]) )
        except:
            try:
                if int(tmp_s.split()[1][:2]) > 12:
                    tmp_s = s.replace('p.m.', '').replace('a.m.', '').replace('PM', '').replace('pm', '')
                return pd.to_datetime(' '.join(tmp_s.split()[:2]) )
            except:
                return np.nan

    food_all[datetime_col] = food_all[datetime_col].apply(handle_time)
    food_all = food_all.dropna().reset_index(drop = True)
    food_all['date'] = find_date(food_all, h)
    
    # Handle the time - Time in floating point format
    
    food_all['float_time'] = find_float_time(food_all, h)
    
    # Handle the time - Time in Datetime object format
    food_all['time'] = pd.DatetimeIndex(food_all[datetime_col]).time
    
    # Handle week from start
    food_all['week_from_start'] = week_from_start(food_all)
    
    food_all['year'] = food_all.date.apply(lambda d: d.year)
    
    return food_all

# %% ../00_core.ipynb 31
def in_good_logging_day(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
        A logging's in a good logging day if the there are more than min_log_num loggings in one day w/ more than min_separation hoursx apart from the earliest logging and the latest logging and False otherwise.\n
        
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or pandas dataframe format.\n
        - identifier (str): id-like column that's used to identify a subject.\n
        - time_col (str): column that contains time in float format.\n
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n
        
    Return:\n
        - A boolean numpy array indicating whether the corresponding row is in a good logging day. Details on the criteria is in the description.\n
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col

    """
    def adherent(s):
        if len(s.values) >= min_log_num and (max(s.values) - min(s.values)) >= min_separation:
            return True
        else:
            return False
        
    df = file_loader(data_source)
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
        
    adherent_dict = dict(df.groupby([identifier, date_col])[time_col].agg(adherent))

    return df.apply(lambda x: adherent_dict[(x[identifier], x.date)], axis = 1)

# %% ../00_core.ipynb 33
class FoodParser:
    """
    Food parser handles taking unprocessed food log entries and adding relevant
    information from a pre-made dictionary (taken from a variety of Panda Lab
    studies) of food items. This includes matching unprocessed terms to
    their likely matches, adding food type and other identifying information.
    """

    def __init__(self):
        """Initializes food parser object."""
        # read in manually annotated file
        parser_keys_df = pd.read_csv("data/09_14_2023_parser_keys.csv")
        all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(
            parser_keys_df
        )
        correction_dic = pd.read_json("data/correction_dic.json", typ="series")
        
        self.all_gram_set = all_gram_set
        self.food_type_dict = food_type_dict
        self._food_phrases = np.unique(
            [
                word
                for key in self.food_type_dict
                for word in key.split()
                if self.food_type_dict[key] in ["f", "b", "m"] and len(word) > 2
            ]
        )
        self.correction_dic = correction_dic
        self.food2tags = food2tags

        # Load common stop words and nlp type objects
        self.wnl = WordNetLemmatizer()

        self.stop_words = stopwords.words("english")
        self.stop_words.remove("out")  # since pre work out is a valid beverage name
        self.stop_words.remove("no")
        self.stop_words.remove("not")
        self.stop_words.remove("and")
        self.stop_words.remove("m")
        self.stop_words.remove("of")
        # preventing common vitamins from being removed
        self.stop_words.remove("d")

    @staticmethod
    def process_parser_keys_df(parser_keys_df):
        """Takes a dataframe of parser keys and processes gram sets and
        food type information.

        Parameters
        ----------
        parser_keys_df: pd.DataFrame
            Dataframe of parser keys.

        Returns
        -------
        all_gram_set: list
            List of sets where each set corresponds to all (n-grams for n in range 1-6)

        food_type_dict: dict
            Dictionary of gram key and food type pairings where gram key
            is the food (e.g. 'milk') and the value is the type (e.g. 'b')

        food2tags: dict
            Dictionary of gram key and corresponding tags. (e.g. key = 'vodka'
            with tags 'alcohol', 'liquor', etc..)
        """
        # f = food, b = beverage, m = medicine, w = water
        # below are the possible valid item types
        parser_keys_df = parser_keys_df.query(
            'food_type in ["f", "b", "m", "w", "modifier", "general", "stopword", "selfcare"]'
        ).reset_index(drop=True)

        # creates an all_gram_set to store n-grams (e.g. 1-grams, 2-grams, etc.)
        all_gram_set = []
        for i in range(1, 5 + 1):
            all_gram_set.append(
                set(parser_keys_df.query("gram_type == " + str(i))["gram_key"].values)
            )

        # pair gram key and corresponding food type for items as a dictionary
        food_type_dict = dict(
            zip(parser_keys_df["gram_key"].values, parser_keys_df["food_type"].values)
        )

        # find all relevant tags
        def find_all_tags(df):
            """Finds tags for each 'food' item.

            Parameters
            -----------
            df: DataFrame
                Dataframe with food tag columns (e.g. tag1)
            """
            tags = []
            for i in range(1, 8):
                # expects column names of the format 'tag#'
                if not isinstance(df["tag" + str(i)], str) and np.isnan(
                    df["tag" + str(i)]
                ):
                    # append empty tag if tag doesn't exist
                    # forced food2tags to have length 8 + can restore tags
                    # in order with a dictionary
                    tags.append("")
                    continue
                # append existing tags
                tags.append(df["tag" + str(i)])
            return tags

        # pair gram_keys (food names) and their corresponding list of informative tags
        food2tags = dict(
            zip(
                parser_keys_df["gram_key"].values,
                parser_keys_df.apply(find_all_tags, axis=1),
            )
        )
        return all_gram_set, food_type_dict, food2tags

    ########## Pre-Processing ##########

    # Function for removing numbers
    @staticmethod
    def handle_numbers(text):
        """Removes numeric characters from text.

        Parameters
        ----------
        text: str
            Text to be cleaned

        Returns
        -------
        text: str
            Text with numbers removed.
        """
        # regex pattern removes all decimal numbers
        text = re.sub("[0-9]+\.[0-9]+", "", text)
        # retaining common items (h2o, co2, v8, ag1)
        if any(sub in text for sub in ["v8", "h2", "ag1", "co2", "0z"]):
            # regex pattern strips any numbers not immediately preceded or followed by a letter
            text = re.sub(r"(?<![a-zA-Z])\d+(?![a-zA-Z])", "", text)
        else:
            # regex pattern strips all whole numbers
            text = re.sub("[0-9+]", "", text)
        # regex pattern truncates any duplicate whitespace (e.g. "  " becomes  " ")
        text = re.sub("\s\s+", " ", text)
        return text

    # Function for removing punctuation
    @staticmethod
    def drop_punc(text):
        """
        Removes punctuation from text.

         Parameters
        ----------
        text: str
            Text to be cleaned

        Returns
        -------
        text: str
            Text with punctuation removed.
        """
        return re.sub("[%s]" % re.escape(string.punctuation), " ", text)

    # Remove normal stopwords
    def remove_stop(self, text):
        """
        Removes english stop words defined by nltk (e.g. 'the') from text.

         Parameters
        ----------
        text: str
            Text to be cleaned

        Returns
        -------
        text: str
            Text with stop words removed.
        """
        return " ".join([word for word in text.split() if word not in self.stop_words])

    def pre_processing(self, text):
        """
        Handles text cleaning overall.

        Parameters
        ----------
        text: str
            Text to be pre-processed/cleaned.

        Returns
        --------
        text: str
            Cleaned text.
        """
        return self.remove_stop(
            self.handle_numbers(self.drop_punc(text.lower()))
        ).strip()

    ########## Handle Format ##########
    @staticmethod
    def handle_front_mixing(sent, front_mixed_tokens):
        """
        Handles cleaned front mixed tokens, which seem to be some sort of digit/numerical.
        Potentially dealing with cleaning out item quantities? (e.g. 12oz?)

        Parameters
        ----------
        sent: str
            Phrase/sentence describing a food item.

        front_mixed_tokens: list
            List of front mixed tokens (I think these are supposed to be amount tokens),
            e.g. 12oz

        Returns
        --------
        sent: str
            Original sentence with any front mixed tokens cleaned (e.g. 12oz --> 12 oz).
        """
        cleaned_tokens = []
        for t in sent.split():
            if t in front_mixed_tokens:
                number = re.findall("\d+[xX]*", t)[0]
                for t_0 in t.replace(number, number + " ").split():
                    cleaned_tokens.append(t_0)
            else:
                cleaned_tokens.append(t)
        return " ".join(cleaned_tokens)

    @staticmethod
    def handle_x2(sent, times_x_tokens):
        """
        Handles variations in spacing or capitalization for the same token.
        e.g. ('x2' vs 'X2' vs 'X 2' vs 'X 2')

        Parameters
        ----------
        sent: str
            Phrase/sentence describing a food item.

        times_x_tokens: list of 'times x' tokens to be dealt with.

        Returns
        --------
        sent: str
            Original sentence with any 'times x' variations cleaned.
        """
        for token in times_x_tokens:
            sent = sent.replace(token, " " + token.replace(" ", "").lower() + " ")

        return " ".join([s for s in sent.split() if s != ""]).strip()

    def clean_format(self, sent):
        """
        Cleans text of any amount indications. Specifically deals with
        front mixing (amount of an item noted at the front of the string)
        and 'times x' where multiple of an item is recorded in the style of
        (item x 2) or some variation of that.

        Parameters
        ----------
        sent: str
            Sentence to be cleaned.

        Returns
        -------
        sent: str
            Sentence after all amount/format style cleaning applied.
        """
        # Problem 1: 'front mixing'
        front_mixed_tokens = re.findall("\d+[^\sxX]+", sent)
        if len(front_mixed_tokens) != 0:
            sent = self.handle_front_mixing(sent, front_mixed_tokens)

        # Problem 2: 'x2', 'X2', 'X 2', 'x 2'
        times_x_tokens = re.findall("[xX]\s*?\d", sent)
        if len(times_x_tokens) != 0:
            sent = self.handle_x2(sent, times_x_tokens)
        return sent

    ########## Handle Typos ##########

    def fix_spelling(self, entry):
        """Corrects spelling mistakes.

        Parameters
        ----------
        entry: str
            String entry to be corrected.

        Returns
        -------
        entry: str
            Original entry with spelling corrections applied.
        """
        result = []

        # if entry is recognized keep it
        if entry in self.food_type_dict:
            return entry
        # try looking for entry in correction dictionary
        if entry in self.correction_dic:
            return " ".join(
                [self.wnl.lemmatize(i) for i in self.correction_dic[entry].split()]
            )
        # try lemmatized version of whole entry
        if self.wnl.lemmatize(entry) in self.food_type_dict:
            return self.wnl.lemmatize(entry)
        # try correcting individual tokens within entry phrase
        for token in entry.split():
            # try lemmatized version of the token
            lem_token = self.wnl.lemmatize(token)
            if (lem_token in self._food_phrases) or (lem_token in self.food_type_dict):
                token = lem_token
            # try looking for token in correction dictionary
            elif token in self.correction_dic:
                token = " ".join(
                    [self.wnl.lemmatize(i) for i in self.correction_dic[token].split()]
                )
            # check if token is incorrectly joined
            # (e.g. blueberrymuffin instead of blueberry muffin)
            elif token not in self._food_phrases:
                temp = []
                token_alt = wordsegment.segment(token)
                for word in token_alt:
                    if self.wnl.lemmatize(word) in self._food_phrases:
                        temp += [word]
                    # check if split word needs spell correction
                    elif (
                        word in self.correction_dic
                        and self.correction_dic[word] in self._food_phrases
                    ):
                        temp += [self.correction_dic[word]]
                # if there are any newly corrected/unjoined tokens add them back to the result
                if len(temp) > 1:
                    token = " ".join([self.wnl.lemmatize(i) for i in temp])
            result.append(token.strip())
        return " ".join(result).strip()

    def handle_all_cleaning(self, entry):
        """
        Performs all types of text cleaning.

        Parameters
        ----------
        entry: str
            String entry to be corrected.

        Returns
        -------
        entry: str
            String entry with all forms of pre-processing and cleaning
            applied.
        """
        entry = self.pre_processing(entry)
        entry = self.clean_format(entry)
        entry = self.fix_spelling(entry)
        entry = re.sub("\s\s+", " ", entry)
        return entry

    ########## Handle Gram Matching ##########
    @staticmethod
    def parse_single_gram(gram_length, gram_set, gram_lst, sentence_tag):
        """
        Parses a single gram, combining words as necessary for the proper gram
        length (e.g. grape juice is a bigram) and then adding any new words
        to a food list.

        Parameters
        ----------
        gram_length: int
            Length of gram to look for (e.g. 2 --> bigram).

        gram_set: list
            Existing gram set.

        gram_list: list
            List of grams to check for entry into the gram set.

        sentence tag: list
            List of tags for the sentence?

        Returns
        -------
            food_lst: list
                All unique items for the given gram being parsed.
        """
        food_lst = []
        for i, gram in enumerate(gram_lst):
            # if not a unigram, combine the words from the gram list into a
            # gram string
            if gram_length > 1:
                curr_word = " ".join(gram)
            else:
                curr_word = gram
            if (
                curr_word in gram_set
                and sum([t != "Unknown" for t in sentence_tag[i : i + gram_length]])
                == 0
            ):
                # add any first occurance of a food to the food list
                sentence_tag[i : i + gram_length] = str(gram_length)
                food_lst.append(curr_word)
        return food_lst

    def parse_single_entry(self, entry, return_sentence_tag=False):
        """
        Handles pre-processing, cleaning, and gram processing for a single entry.

        Parameters
        ----------
        entry: str
            Entry to be processed

        Returns
        -------
        All grams of length 5 or under for the particular entry.
        """
        cleaned = self.handle_all_cleaning(entry)

        # Create tokens and n-grams
        tokens = nltk.word_tokenize(cleaned)
        bigram = list(nltk.ngrams(tokens, 2)) if len(tokens) > 1 else None
        trigram = list(nltk.ngrams(tokens, 3)) if len(tokens) > 2 else None
        quadgram = list(nltk.ngrams(tokens, 4)) if len(tokens) > 3 else None
        pentagram = list(nltk.ngrams(tokens, 5)) if len(tokens) > 4 else None
        all_gram_lst = [tokens, bigram, trigram, quadgram, pentagram]

        # Create an array of tags
        sentence_tag = np.array(["Unknown"] * len(tokens))

        all_food = []
        for gram_length in [5, 4, 3, 2, 1]:
            if len(tokens) < gram_length:
                continue
            tmp_food_lst = self.parse_single_gram(
                gram_length,
                self.all_gram_set[gram_length - 1],
                all_gram_lst[gram_length - 1],
                sentence_tag,
            )
            all_food += tmp_food_lst
        if return_sentence_tag:
            return all_food, sentence_tag
        return all_food

    def parse_food(self, series, calc_unknowns = False):
        """
        Parses a series of single food entries.

        Parameters
        ----------
        series: list or series of str
            Food entries to be parsed.
        
        calc_unknowns: bool
            If true, includes unknown token information in return. Default is false.
            
        Returns
        -------
        Series of parsed food items.
        """
        def parse(x, calc_unknowns = calc_unknowns):
            return self._parse_food(x, calc_unknowns)
        vec = np.vectorize(parse)
        return vec(series)

    def _parse_food(self, entry, calc_unknowns = False):
        """
        Parses a single food entry.

        Parameters
        ----------
        entry: str
            Food entry to be parsed.
        
        calc_unknowns: bool
            If true, includes unknown token information in return. Default is false.
            
        Returns
        -------
        A single parsed food entry.
        """
        result = []
        unknown_tokens = []
        num_unknown = 0
        num_token = 0

        for word in entry.split(","):
            # previous versions of this if statement assume that
            # return_sentence_tag is True and will actually break
            # if the default parameters are used
            all_food, sentence_tag = self.parse_single_entry(word, True)
            result += all_food
            if calc_unknowns:
                if sentence_tag is not None and len(sentence_tag) > 0:
                    num_unknown += sum(np.array(sentence_tag) == "Unknown")
                    num_token += len(sentence_tag)
                    cleaned = nltk.word_tokenize(self.handle_all_cleaning(word))

                    # Return uncaught tokens, grouped into sub-sections
                    tmp_unknown = ""
                    for i, tag in enumerate(sentence_tag):
                        if tag == "Unknown":
                            tmp_unknown += " " + cleaned[i]
                            if i == len(sentence_tag) - 1:
                                unknown_tokens.append(tmp_unknown.strip())
                        elif tmp_unknown != "":
                            unknown_tokens.append(tmp_unknown.strip())
                            tmp_unknown = ""
        if calc_unknowns:
            return np.array(
                [result, num_token, num_unknown, unknown_tokens], dtype = "object"
            )
            
        return np.array(result, dtype = "object")

    def find_food_type(self, food):
        """
        Finds corresponding food-item type for a given food. Types include
        beverage, water, food, medicine.

        Parameters
        ----------
        food: str
            Food to find type for

        Returns
        -------
        Corresponding food type abbrevation or 'u' if not found.
        """
        if food in self.food_type_dict:
            return self.food_type_dict[food]
        # shorthand for unknown
        return "u"

    ################# DataFrame Functions #################
    def expand_entries(self, df):
        """
        Creates an 'exploded' version of the given dataframe, expanding entries
        from within list-style 'desc_text' column entries such that each
        entry in a list has its own corresponding row.

        Parameters
        ----------
        df: pd.Dataframe
            Dataframe to be expanded.

        Returns
        -------
        df: pd.Dataframe
            Expanded dataframe.
        """
        assert "desc_text" in df.columns, '[ERROR] Required a column of "desc_text".'
        df = df.copy()
        df["desc_text"] = df["desc_text"].str.split(",")
        df = df.explode("desc_text")
        # remove entries that are only spaces or digits (entries with no alphabetical characters)
        df = df[~df["desc_text"].str.isspace()]
        df = df[~df["desc_text"].str.isdigit()]
        return df


# %% ../00_core.ipynb 34
def clean_loggings(data_source, identifier = 1):
    """
    Description:\n
       This function convert all the loggings in the data_source file into a list of typo-corrected items based on the text_col column. This function is based on a built-in vocabulary dictionary and an n-gram searcher.\n
       
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - text_col(str): column that will be converted to individual items.
        - identifier(str): participants' unique identifier such as id, name, etc.
        
    Return:\n
        - A dataframe contains the cleaned version of the text_col.\n

    """

    df = file_loader(data_source)
    identifier = df.columns[identifier]
    text_col = df.columns[df.columns.get_loc('desc_text')]
    
    # initialize food parser instance
    fp = FoodParser()
    
    # parse food
    parsed = fp.parse_food(df["desc_text"])
    df_parsed = pd.DataFrame({
    identifier: df[identifier],
    text_col: df[text_col],
    'cleaned': parsed
    })
    
    
    return df_parsed

# %% ../00_core.ipynb 36
def get_types(data_source, food_type):
    """
    Description:\n
       This function filters with the expected food types and return a cleaner version of data_source file.\n 
       
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n
        - food_type (str): expected types of the loggings for filtering, in format of list. Available types:  \n
            1. 'w' : water \n
            2. 'b' : beverage \n
            3. 'f' : food \n
            4. 'm' : medicine \n
        
    Return:\n
        - A filtered dataframe with expected food type/types with five columns: 'unique_code','food_type', 'desc_text', 'date', 'local_time'.\n
        
    Requirements:\n
        data_source file must have the following columns:\n
            - food_type\n
            
    """
    
    df = file_loader(data_source)
        
    if len(food_type) == 0:
        return df
    
    if len(food_type) == 1:
        if food_type[0] not in ['w', 'b', 'f', 'm']:
            raise Exception("not a valid logging type")
        filtered = df[df['food_type']==food_type[0]]
    else:  
        filtered = df[df['food_type'].isin(food_type)]
        
    
    return filtered

# %% ../00_core.ipynb 39
def count_caloric_entries(df):
    """
    Description:\n
        This is a function that counts the number of food and beverage loggings.
    
    Input:\n
        - df(pandas df) : food_logging data.
    
    Output:\n
        - a float representing the number of caloric entries.
        
    Requirement:\n
        - 'date' column existed in the df.
    """
    if 'food_type' not in df.columns:
        raise Exception("'food_type' column must exist in the dataframe.")
    food_type_col = df.columns[df.columns.get_loc('food_type')]
        
    return df[df[food_type_col].isin(['f','b'])].shape[0]

# %% ../00_core.ipynb 41
def mean_daily_eating_duration(df, date_col = 6, time_col = 7):
    """
    Description:\n
        This is a function that calculates the mean daily eating window, which is defined as the duration of first and last caloric intake.
    
    Input:\n
        - df(pandas df) : food_logging data.
        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.
        - time_col(column existed in df, string) : contains the float time data for each logging.
    
    Output:\n
        - a float representing the mean daily eating window.
        
    Requirement:\n
        - 'date' column existed in the df.
        - float time is calculated.
        
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]

    df = df[df['food_type'].isin(['f','b'])]
    breakfast_time = df.groupby(date_col)[time_col].agg(min)
    dinner_time = df.groupby(date_col)[time_col].agg(max)
    return (dinner_time - breakfast_time).mean()

# %% ../00_core.ipynb 43
def std_daily_eating_duration(df, date_col = 6, time_col = 7):
    """
    Description:\n
        This function calculates the standard deviation of daily eating window, which is defined as the duration between the first and last caloric intake.
    
    Input:\n
        - df(pandas df) : food_logging data.
        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.
        - time_col(column existed in df, string) : contains the float time data for each logging.
    
    Output:\n
        - a float representing the standard deviation of daily eating window.
        
    Requirement:\n
        - 'food_type' column existed in the df.
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
        
    df = df[df['food_type'].isin(['f','b'])]
    breakfast_time = df.groupby(date_col)[time_col].agg(min)
    dinner_time = df.groupby(date_col)[time_col].agg(max)

    return (dinner_time - breakfast_time).std()

# %% ../00_core.ipynb 45
def earliest_entry(df, time_col = 7):
    """
    Description:\n
        This function calculates the earliest first calorie on any day in the study period. 
    Input:\n
        - df(pandas df) : food_logging data.
        - time_col(column existed in df, string) : contains information of logging time in float.
    
    Output:\n
        - the earliest caloric time in float on any day in the study period. 
        
    Requirement:\n
        - 'date' column existed in the df.
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    df = get_types(df, ['f', 'b'])
    
    return df[time_col].min()

# %% ../00_core.ipynb 47
def mean_first_cal(df, date_col = 6, time_col = 7):
    """
    Description:\n
        This function calculates the average time of first calory intake. 
    Input:\n
        - df(pandas df) : food_logging data.
        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.
        - time_col(column existed in df, string) : contains information of logging time in float.
    
    Output:\n
        - the mean first caloric intake time in float in the study period. 
        
    Requirement:\n
        - 'date' column existed in the df.
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
    
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    
    return df.groupby([date_col])[time_col].min().mean()

# %% ../00_core.ipynb 50
def std_first_cal(df, date_col = 6, time_col = 7):
    """
    Description:\n
        This function calculates the average time of first calory intake. 
    Input:\n
        - df(pandas df) : food_logging data.
        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.
        - time_col(column existed in df, string) : contains information of logging time in float.
    
    Output:\n
        - the mean first caloric intake time in float in the study period. 
        
    Requirement:\n
        - 'date' column existed in the df.
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
    
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    return df.groupby([date_col])[time_col].min().std()

# %% ../00_core.ipynb 52
def mean_last_cal(df, date_col = None, time_col = None):
    """
    Description:\n
        This function calculates the average time of last calory intake. 
    Input:\n
        - df(pandas df) : food_logging data.
        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.
        - time_col(column existed in df, string) : contains information of logging time in float.
    
    Output:\n
        - the mean last caloric intake time in float in the study period. 
        
    Requirement:\n
        - 'date' column existed in the df.
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[6]
    
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[7]
    
    return df.groupby([date_col])[time_col].max().mean()

# %% ../00_core.ipynb 54
def std_last_cal(df, date_col = 6, time_col = 7):
    """
    Description:\n
        This function calculates the average time of last calory intake. 
    Input:\n
        - df(pandas df) : food_logging data.
        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.
        - time_col(column existed in df, string) : contains information of logging time in float.
    
    Output:\n
        - the mean last caloric intake time in float in the study period. 
        
    Requirement:\n
        - 'date' column existed in the df.
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
    
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    return df.groupby([date_col])[time_col].max().std()

# %% ../00_core.ipynb 56
def logging_day_counts(df):
    """
    Description:\n
        This function calculates the number of days that contains any loggings. 
    Input:\n
        - df(pandas df) : food_logging data.
    
    Output:\n
        - an integer that represents the number of logging days.
        
    Requirement:\n
        - 'date' column existed in the df.
    """
    date_col = df.columns[df.columns.get_loc('date')]
    return df[date_col].nunique()

# %% ../00_core.ipynb 58
def find_missing_logging_days(df, start_date = "not_defined", end_date = "not_defined"):
    """
    Description:\n
        This function finds the days during which there's no logging within the period from start_date to end_date. 
    Input:\n
        - df(panda df): food_logging data.
        - start_date(datetime.date object): start date of the period of calculation. If not defined, it will be automatically set to be the earliest date in df. 
        - end_date(datetime.date object): end date of the period of calculation. If not defined, it will be automatically set to be the latest date in df.
    
    Output:\n
        - a list that contains all of the dates that don't contain loggings.
        
    Requirement:\n
        - 'date' column existed in the df.
    """
    
    # if start_date or end_date is missing, return nan
    # intended behavior for participants when their study phase is ongoing
    if pd.isnull(start_date) or pd.isnull(end_date):
        return np.nan
    
    # if there is no input on start_date or end_date, use earliest date and latest date
    if start_date == "not_defined":
        start_date = df['date'].min()
    if end_date == "not_defined":
        end_date = df['date'].max()
        
    df = df[(df['date']>=start_date) & (df['date']<=end_date)]
    
    # get all the dates between two dates
    lst = []
    for x in pd.date_range(start_date, end_date, freq='d'):
         if x not in df['date'].unique():
                lst.append(x.date())
    
    return lst
        

# %% ../00_core.ipynb 60
def good_lwa_day_counts(df, window_start, window_end, min_log_num = 2, min_separation = 5, buffer_time= '15 minutes', h = 4, start_date = "not_defined", end_date = "not_defined", time_col = 7):
    """
    Description:\n
        This function calculates the number of good logging days, good window days, outside window days and adherent days. Good logging day is defined as a day that the person makes at least min_log_num number of loggings and the time separation between the earliest and the latest logging are greater than min_separation.\n
        A good window day is defined as a date that all the food loggings are within the assigned restricted window. An adherent day is defined as a date that is both a good logging day and a good window day.
    Input:\n
        - df(pandas df): food_logging data.
        - window_start(datetime.time object): start time of the restriction window.
        - window_end(datetime.time object): end time of the restriction window.
        - time_col(str) : the column that represents the eating time.
        - min_log_num(count, int): minimum number of loggings to qualify a day as a good logging day
        - min_separation(hours, int): minimum period of separation between earliest and latest loggings to qualify a day as a good logging day
        - buffer_time(time in string that can be passed into pd.Timedelta()): wiggle room for to be added/subtracted on the ends of windows.
        - h(hours, int): hours to be pushed back
        - start_date(datetime.date object): start date of the period for calculation. If not defined, it will be automatically set to be the earliest date in df.
        - end_date(datetime.date object): end date of the period for calculation. If not defined, it will be automatically set to be the latest date in df.
    Output:\n
        - Two lists.
            - First list contains 4 integers that represent the number of good logging days, good window days, outside window days and adherent_days
            - Second list contains lists consisted of specific dates that are not good logging days, window days and adherent days within the range of start date and end date(both inclusive).
    Requirement:\n
        - 'date' column existed in the df.
        - float time is calculated as time_col.

    """
    # check for treets created float time column, otherwise default to expected column number
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    # if start_date or end_Date is missing, return nan
    # intended behavior for participants when their study phase is ongoing
    if pd.isnull(start_date) or pd.isnull(end_date):
        return [np.nan, np.nan, np.nan, np.nan], [[],[],[]]
    
    # if there is no input on start_date or end_date, use earliest date and latest date
    if start_date == "not_defined":
        start_date = df['date'].min()
    if end_date == "not_defined":
        end_date = df['date'].max()
    
    # if window start or window end are nan, make the windows the same as control's window time.
    if pd.isnull(window_start):
        window_start = datetime.time(0,0)
    if pd.isnull(window_end):
        window_end = datetime.time(23,59)

    # helper function to determine a good logging
    def good_logging(local_time_series):
        return len(local_time_series.values) >= min_log_num and (max(local_time_series.values) - min(local_time_series.values)) >= min_separation
   
    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]
    df = df[df['food_type'].isin(['f','b'])]
    df['original_logtime'] = pd.to_datetime(df['original_logtime']).dt.tz_localize(None)

    buffer_time = pd.Timedelta(buffer_time).total_seconds()/3600.

    in_window_count = []
    daily_count = []
    good_logging_count = []
    cur_dates = df['date'].sort_values(ascending = True).unique()
    for aday in cur_dates:
        window_start_daily = window_start.hour + window_start.minute / 60 - buffer_time
        window_end_daily = window_end.hour + window_end.minute / 60 + buffer_time
        tmp = df[df['date'] == aday]
        if (window_start == datetime.time(0,0)) and (window_end == datetime.time(23,59)):
            in_window_count.append(tmp[(tmp[time_col] >= window_start_daily + h) & (tmp[time_col] <= window_end_daily + h)].shape[0])
        else:
            in_window_count.append(tmp[(tmp[time_col] >= window_start_daily) & (tmp[time_col] <= window_end_daily)].shape[0])
        daily_count.append(df[df['date'] == aday].shape[0])
        good_logging_count.append(good_logging(df[df['date'] == aday][time_col]))

    in_window_count = np.array(in_window_count)
    daily_count = np.array(daily_count)
    good_logging_count = np.array(good_logging_count)
    good_logging_by_date = [cur_dates[i] for i, x in enumerate(good_logging_count) if not x]

    good_window_days = (in_window_count==daily_count)
    good_window_day_counts = good_window_days.sum()
    good_window_by_date = [cur_dates[i] for i, x in enumerate(good_window_days) if not x]
    
    outside_window_days = in_window_count.size - good_window_days.sum()
    good_logging_days = good_logging_count.sum()
    if good_logging_count.size == 0:
        adherent_day_counts = 0
        adherent_days_by_date = []
    else:
        adherent_days = (good_logging_count & (in_window_count == daily_count))
        adherent_days_by_date = [cur_dates[i] for i, x in enumerate(adherent_days) if not x]
        adherent_day_counts = adherent_days.sum()
    
    rows = [good_logging_days, good_window_day_counts, outside_window_days, adherent_day_counts]
    bad_dates = [good_logging_by_date, good_window_by_date, adherent_days_by_date]

    return rows, bad_dates

# %% ../00_core.ipynb 64
def filtering_usable_data(df, num_items, num_days, identifier = 1, date_col = 6):
    '''
    Description:\n
        This function filters the cleaned app data so the users who satisfies the criteria are left. The criteria is that the person is left if the total loggings for that person are more than num_items and at the same time, the total days of loggings are more than num_days.\n
    Input:\n
        - df (pd.DataFrame): the dataframe to be filtered
        - identifier (str): unique_id or ID, or name that identifies people.
        - date_col (str): column name that contains date information from the df dataframe.
        - num_items (int):   number of items to be used as cut-off
        - num_days (int):    number of days to be used as cut-off
    Output:\n
        - df_usable:  a panda DataFrame with filtered rows
        - set_usable: a set of unique_code to be included as "usable"
    Requirements:\n
        df should have the following columns:
            - desc_text
    Optional functions to use to have proper inputs:
        - find_date() for date_col
    '''
    print(' => filtering_usable_data()')
    print('  => using the following criteria:', num_items, 'items and', num_days, 'days.')
    
    identifier = df.columns[1]
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]

    # Item logged
    log_item_count = df.groupby(identifier).agg('count')[['desc_text']].rename(columns = {'desc_text': 'Total Logged'})

    # Day counts
    log_days_count = df[[identifier, date_col]]\
        .drop_duplicates().groupby(identifier).agg('count').rename(columns = {date_col: 'Day Count'})

    item_count_passed = set(log_item_count[log_item_count['Total Logged'] >= num_items].index)
    day_count_passed = set(log_days_count[log_days_count['Day Count'] >= num_days].index)

    print('  => # of users pass the criteria:', end = ' ')
    print(len(item_count_passed & day_count_passed))
    passed_participant_set = item_count_passed & day_count_passed
    df_usable = df.loc[df.unique_code.apply(lambda c: c in passed_participant_set)]\
        .copy().reset_index(drop = True)
    # print('  => Now returning the pd.DataFrame object with the head like the following.')
    # display(df_usable.head(5))
    return df_usable, set(df_usable.unique_code.unique())

# %% ../00_core.ipynb 67
def prepare_baseline_and_intervention_usable_data(data_source, baseline_num_items, baseline_num_days, intervention_num_items, intervention_num_days, identifier = 1, date_col = 6):
    """
    Description:\n
        Filter and create baseline_expanded and intervention groups based on data_source pickle file. Expanded baseline dataset contains the first two weeks data and 13, 14 weeks data that pass the given criteria. Intervention dataset contains 13, 14 weeks data that pass the given criteria.\n
        
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or pandas dataframe format\n
        - identifier (str): unique_id or ID, or name that identifies people.
        - date_col (str): column name that contains date information from the df dataframe.
        - baseline_num_items (int): number of items to be used as cut-off for baseline group. \n
        - baseline_num_days (int): number of days to be used as cut-off for baseline group. \n
        - intervention_num_items (int): number of items to be used as cut-off for intervention group.\n
        - intervention_num_days (int): number of days to be used as cut-off for intervention group. \n
        
    Return:\n
        - a list in which index 0 is the baseline expanded dataframe and 1 is the intervention dataframe.\n

    Requirements:\n
        data_source file must have the following columns:\n
            - week_from_start\n
            - desc_text\n
    """
    
    
    food_all = file_loader(data_source)
    
    # create baseline data
    df_food_baseline = food_all.query('week_from_start <= 2')
    df_food_baseline_usable, food_baseline_usable_id_set = \
    filtering_usable_data(df_food_baseline, num_items = baseline_num_items, num_days = baseline_num_days, identifier = identifier, date_col = date_col)
    
    # create intervention data
    df_food_intervention = food_all.query('week_from_start in [13, 14]')
    df_food_intervention_usable, food_intervention_usable_id_set = \
    filtering_usable_data(df_food_intervention, num_items = intervention_num_items, num_days = intervention_num_days, identifier = identifier, date_col = date_col)
    
    # create df that contains both baseline and intervention id_set that contains data for the first two weeks
    expanded_baseline_usable_id_set = set(list(food_baseline_usable_id_set) + list(food_intervention_usable_id_set))
    df_food_basline_usable_expanded = food_all.loc[food_all.apply(lambda s: s.week_from_start <= 2 \
                                                    and s.unique_code in expanded_baseline_usable_id_set, axis = 1)]
        
    return [df_food_basline_usable_expanded, df_food_intervention_usable]

# %% ../00_core.ipynb 71
def users_sorted_by_logging(data_source, food_type = ["f", "b", "m", "w"], min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
        This function returns a dataframe reports the number of good logging days for each user in the data_source file. The default order is descending.\n
        
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or pandas dataframe format.\n
        - food_type (str): food types to filter in a list format. Default: ["f", "b", "m", "w"]. Available food types:\n
            1. 'w' : water \n
            2. 'b' : beverage \n
            3. 'f' : food \n
            4. 'm' : medicine \n
        
    Return:\n
        - A dataframe contains the number of good logging days for each user.\n
        
    Requirements:\n
        data_source file must have the following columns:\n
            - food_type\n
            - unique_code\n
            - date\n
    
    """

    food_all = file_loader(data_source)
    
    # filter the dataframe so it only contains input food type
    
    filtered_users = food_all.query('food_type in @food_type')
    filtered_users['in_good_logging_day'] = in_good_logging_day(filtered_users, min_log_num, min_separation, identifier, date_col, time_col)
    
    food_top_users_day_counts = pd.DataFrame(filtered_users.query('in_good_logging_day == True')\
                            [['date', 'unique_code']].groupby('unique_code')['date'].nunique())\
                            .sort_values(by = 'date', ascending = False).rename(columns = {'date': 'day_count'})

    
    return food_top_users_day_counts

# %% ../00_core.ipynb 73
def eating_intervals_percentile(data_source, identifier = 1, time_col = 7):
    """
    Description:
       This function calculates the .025, .05, .10, .125, .25, .5, .75, .875, .9, .95, .975 quantile of eating time and mid 95%, mid 90%, mid 80%, mid 75% and mid 50% duration for each user.\n 
    
    Input:
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - time_col(str) : the column that represents the eating time.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        
    Return:\n
        - A summary table with count, mean, std, min, quantiles and mid durations for all subjects from the data_source file.
    
    Optional functions to use to have proper inputs:
        - find_float_time() for time_col
    """
    df = file_loader(data_source)
    
    identifier = df.columns[1]
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    if df.shape[0] == 0:
        return pd.DataFrame(np.array([[np.nan]*21]), columns = ['count', 'mean', 'std', 'min', '2.5%', '5%', '10%', '12.5%', '25%',
       '50%', '75%', '87.5%', '90%', '95%', '97.5%', 'max', 'duration mid 95%',
       'duration mid 90%', 'duration mid 80%', 'duration mid 75%',
       'duration mid 50%'])
    
    ptile = df.groupby(identifier)[time_col].describe(percentiles=[.025, .05, .10, .125, .25, .5, .75, .875, .9, .95, .975])
    ll = ['2.5%','5%','10%','12.5%','25%']
    ul = ['97.5%','95%', '90%','87.5%', '75%']
    mp = ['duration mid 95%', 'duration mid 90%', 'duration mid 80%', 'duration mid 75%','duration mid 50%']
    for low, upp, midp in zip(ll,ul,mp):
        ptile[midp] = ptile[upp] - ptile[low]
        
    return ptile

# %% ../00_core.ipynb 75
def first_cal_analysis_summary(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function takes the loggings in good logging days and calculate the 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time for each user.\n 
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.
        
        
    Return:\n
        - A summary table with 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time for all subjects from the data_source file.\n
        
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """

    df = file_loader(data_source)
    
    # leave only the loggings in a good logging day
    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)
    df = df[df['in_good_logging_day']==True]
    
    identifier = df.columns[identifier]
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    first_cal_series = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])
    first_cal_df = pd.DataFrame(first_cal_series)
    all_rows = []
    for index in first_cal_df.index:
        tmp_dict = dict(first_cal_series[index[0]])
        tmp_dict['id'] = index[0]
        all_rows.append(tmp_dict)
    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\
        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\
        .drop_duplicates().reset_index(drop = True)
    
    return first_cal_summary_df

# %% ../00_core.ipynb 77
def last_cal_analysis_summary(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function takes the loggings in good logging days and calculate the 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time for each user.\n 
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.
        
    Return:\n
        - A summary table with 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time for all subjects from the data_source file.\n
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    
    df = file_loader(data_source)
    
    # leave only the loggings that are in a good logging day
    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)
    df = df[df['in_good_logging_day']==True]
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    last_cal_series = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])
    last_cal_df = pd.DataFrame(last_cal_series)
    all_rows = []
    for index in last_cal_df.index:
        tmp_dict = dict(last_cal_series[index[0]])
        tmp_dict['id'] = index[0]
        all_rows.append(tmp_dict)
    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\
        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\
        .drop_duplicates().reset_index(drop = True)

    
    return last_cal_summary_df

# %% ../00_core.ipynb 79
def summarize_data(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function calculates num_days, num_total_items, num_f_n_b, num_medications, num_water, duration_mid_95, start_95, end_95, first_cal_avg, first_cal_std, last_cal_avg, last_cal_std, eating_win_avg, eating_win_std, adherent_count, first_cal variation (90%-10%), last_cal variation (90%-10%).\n 
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - float_time_col(str) : the column that represents the eating time.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n
        
    Return:\n
        - A summary table with count, mean, std, min, quantiles and mid durations for all subjects from the data_source file.\n
        
    Requirements:\n
        data_source file must have the following columns:\n
            - food_type\n
  
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    df = file_loader(data_source)
    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)
    
    # first_cal variation (90%-10%)
    first_cal_variability = first_cal_analysis_summary(df, min_log_num, min_separation, identifier, date_col, time_col).set_index('id')
    for col in first_cal_variability.columns:
        if col == 'id' or col == '50%':
            continue
        first_cal_variability[col] = first_cal_variability[col] - first_cal_variability['50%']
    first_cal_ser = first_cal_variability['90%'] - first_cal_variability['10%']

    # last_cal variation (90%-10%)
    last_cal_variability = last_cal_analysis_summary(df,min_log_num, min_separation, identifier, date_col, time_col).set_index('id')
    for col in last_cal_variability.columns:
        if col == 'id' or col == '50%':
            continue
        last_cal_variability[col] = last_cal_variability[col] - last_cal_variability['50%']
    last_cal_ser = last_cal_variability['90%'] - last_cal_variability['10%']
    
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    
    # num_days
    num_days = df.groupby(identifier).date.nunique()

    # num_total_items
    num_total_items = df.groupby(identifier).count().iloc[:,0]

    # num_f_n_b
    num_f_n_b = get_types(df, ['f','b']).groupby(identifier).count().iloc[:,0]

    # num_medications
    num_medications = get_types(df, ['m']).groupby(identifier).count().iloc[:,0]

    # num_water
    num_water = get_types(df, ['w']).groupby(identifier).count().iloc[:,0]

    # duration_mid_95, start_95, end_95
    eating_intervals = eating_intervals_percentile(df, time_col, identifier)[['2.5%','95%','duration mid 95%']]

    # first_cal_avg
    first_cal_avg = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).mean()

    # first_cal_std
    first_cal_std = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).std()

    # last_cal_avg
    last_cal_avg = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).mean()

    # last_cal_std
    last_cal_std = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).std()

    # eating_win_avg
    eating_win_avg = last_cal_avg - first_cal_avg

    # eating_win_std
    eating_win_std = (df.groupby([identifier, date_col])[time_col].max()-
                          df.groupby([identifier, date_col])[time_col].min()).groupby(identifier).std()
    
    # good_logging_count
    good_logging_count = df.groupby(identifier)['in_good_logging_day'].sum()


    returned = pd.concat([num_days, num_total_items, num_f_n_b, num_medications, num_water, first_cal_avg, first_cal_std, last_cal_avg, last_cal_std, eating_win_avg, eating_win_std, good_logging_count, first_cal_ser, last_cal_ser], axis=1).reset_index()
    returned.columns = [identifier,'num_days', 'num_total_items', 'num_f_n_b', 'num_medications', 'num_water', 'first_cal_avg', 'first_cal_std', 'last_cal_avg', 'last_cal_std', 'eating_win_avg', 'eating_win_std', 'good_logging_count', 'first_cal variation (90%-10%)', 'last_cal variation (90%-10%)']
    returned = returned.merge(eating_intervals, on = identifier, how='left').fillna(0)
    
    returned['num_medications'] = returned['num_medications'].astype('int')
    returned['num_water'] = returned['num_water'].astype('int')
    
    return returned

# %% ../00_core.ipynb 81
def summarize_data_with_experiment_phases(food_data, ref_tbl, min_log_num = 2, min_separation = 5, buffer_time = '15 minutes', h = 4,report_level = 2, txt = False, time_col = 7):
    """
    Description:\n
        This is a comprehensive function that performs all of the functionalities needed.
    
    Input:\n
        - food_data(panda df): food_logging data.
        - ref_tbl(panda df): table that contains window information and study phase information for each participant.
        - min_log_num(counts): minimum number of loggings to qualify a day as a good logging day.
        - min_seperation(hours): minimum period of separation between earliest and latest loggings to qualify a day as a good logging day
        - buffer_time(time in string that can be passed into pd.Timedelta()): wiggle room for to be added/subtracted on the ends of windows.
        - h(hours): hours to be pushed back.
        - report_level(int): whether to print out the dates of no logging days, bad logging days, bad window days and non-adherent days for each participant. 0 - no report. 1 - report no logging days. 2 - report no logging days, bad logging days, bad window days and non adherent days.
        - txt(boolean): if True, a txt format report will be saved in the current directory named "treets_warning_dates.txt".
        
    Output:\n
        - df : dataframe that has all the variables needed and has the same row number as the ref_tbl.
        
    Requirement:\n
        - food logging data is already read from all files in the directories into a dataframe, which will be passed in as the variable food_data.
        - Columns 'Start_Day', 'End_day', 'mCC_ID', 'Eating_Window_Start', 'Eating_Window_End' existed in the ref_tbl.
    
    Sidenote: 
        - For eating window without restriction(HABIT or TRE not in intervention period), Eating_Window_Start is 0:00, Eating_Window_End is 23:59.
    
    """
    df = food_data.copy()
    
    mcc_id = ref_tbl.columns[0]
    start_day = ref_tbl.columns[4]
    end_day = ref_tbl.columns[5]
    window_start = ref_tbl.columns[6]
    window_end = ref_tbl.columns[7]
    
    
    
    # checking for necessary typecasting for reference table
    if not ref_tbl[start_day].apply(lambda x: isinstance(x, datetime.date)).all():
        ref_tbl[start_day] = pd.to_datetime(ref_tbl[start_day]).dt.date
        
    if not ref_tbl['End_day'].apply(lambda x: isinstance(x, datetime.date)).all():
        ref_tbl[end_day] = pd.to_datetime(ref_tbl[end_day]).dt.date
    
    
    # preprocess to get the date and float_time column
    df['original_logtime'] = pd.to_datetime(df['original_logtime'])
    df['date'] =  find_date(df, h, df.columns.get_loc('original_logtime'))
    df['float_time'] =  find_float_time(df, h, df.columns.get_loc('original_logtime'))
    
    # get study phase duration
    result = find_phase_duration(ref_tbl)
    
    # reset the index of ref_tbl to avoid issues during concatenation
    ref_tbl = ref_tbl.reset_index(drop=True)
    
    # loop through each row and get 'caloric_entries', 'mean_daily_eating_window', 'std_daily_eating_window', 'eariliest_entry', 'logging_day_counts',
    # and 'good_logging_days', 'good_window_days', 'outside_window_days' and 'adherent_days' and find missing dates
    matrix = []
    missing_dates = {}
    bad_dates_dic = {}
   
    for index, row in ref_tbl.iterrows():
        id_ = row[mcc_id]
        rows = []
        temp_df = df[df["PID"] == id_]
        temp_df = temp_df[(temp_df['date'] >= row[start_day]) & (temp_df['date'] <= row[end_day])]
        # num of caloric entries
        rows.append(count_caloric_entries(temp_df))
        # num of medication
        rows.append(get_types(temp_df, ['m']).shape[0])
        # num of water
        rows.append(get_types(temp_df, ['w']).shape[0])
        # first cal average
        rows.append(mean_first_cal(temp_df,'date', 'float_time'))
        #first cal std
        rows.append(std_first_cal(temp_df, 'date', 'float_time'))
        # last cal average
        rows.append(mean_last_cal(temp_df,'date', 'float_time'))
        # last cal std
        rows.append(std_last_cal(temp_df, 'date', 'float_time'))
        # mean eating window
        rows.append(mean_daily_eating_duration(temp_df,'date','float_time'))
        
        rows.append(std_daily_eating_duration(temp_df,'date','float_time'))
        rows.append(earliest_entry(temp_df, time_col))

        rows.append(logging_day_counts(temp_df))
        row_day_num, bad_dates = good_lwa_day_counts(df[df["PID"]==id_]
                                           , window_start=row[window_start]
                                           , window_end = row[window_end]
                                           , min_log_num=min_log_num
                                           , min_separation=min_separation
                                           , buffer_time= buffer_time
                                           , start_date=row[start_day]
                                           , end_date=row[start_day]
                                            , h=h)
        for x in row_day_num:
            rows.append(x)
        bad_logging = bad_dates[0]
        bad_window = bad_dates[1]
        non_adherent = bad_dates[2]

        if '{}_bad_logging'.format(id_) not in bad_dates_dic:
            bad_dates_dic['{}_bad_logging'.format(id_)]=bad_logging
            bad_dates_dic['{}_bad_window'.format(id_)]=bad_window
            bad_dates_dic['{}_non_adherent'.format(id_)]=non_adherent
        else:
            bad_dates_dic['{}_bad_logging'.format(id_)]+=bad_logging
            bad_dates_dic['{}_bad_window'.format(id_)]+=bad_window
            bad_dates_dic['{}_non_adherent'.format(id_)]+=non_adherent
                
        matrix.append(rows)
        date_lst = find_missing_logging_days(df[df["PID"]==id_], row[start_day],row[end_day])
        # only consider when the result is not nan
        if isinstance(date_lst, list)==True:
            if id_ in missing_dates:
                missing_dates[id_] += date_lst
            else:
                missing_dates[id_] = date_lst

    # create a temp dataframe
    tmp = pd.DataFrame(matrix, columns = ['caloric_entries_num','medication_num', 'water_num','first_cal_avg','first_cal_std',
                                          'last_cal_avg', 'last_cal_std', 'mean_daily_eating_window', 'std_daily_eating_window', 'earliest_entry', 'logging_day_counts'\
                                         ,'good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days'])
    
    # concat these two tables
    returned = pd.concat([ref_tbl, tmp], axis=1)
    
    # loop through each row and get 2.5%, 97.5%, duration mid 95% column
    column_025 = []
    column_975 = []
    for index, row in ref_tbl.iterrows():
        id_ = row[mcc_id]
        temp_df = df[df["PID"] == id_]
        temp_df = temp_df[(temp_df['date'] >= row[start_day]) & (temp_df['date'] <= row[end_day])]
        series = eating_intervals_percentile(temp_df, 'float_time', "PID")
        try:
            column_025.append(series.iloc[0]['2.5%'])
        except:
            column_025.append(np.nan)
        try:
            column_975.append(series.iloc[0]['97.5%'])
        except:
            column_975.append(np.nan)
    returned['2.5%'] = column_025
    returned['97.5%'] = column_975
    returned['duration mid 95%'] = returned['97.5%'] - returned['2.5%']
    
    def convert_to_percentage(ser, col):
        if pd.isnull(ser[col]):
            return ser[col]
        else:
            return str(round(ser[col]/ser['phase_duration'].days * 100, 2)) + '%'
    
    # calculate percentage for 
    for x in returned.columns:
        if x in ['logging_day_counts','good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days']:
            returned['%_'+x] = returned.apply(convert_to_percentage, col = x, axis = 1)

    # reorder the columns
    returned = returned[['mCC_ID', 'Participant_Study_ID', 'Study Phase',
       'Intervention group (TRE or HABIT)', 'Start_Day', 'End_day',
       'Eating_Window_Start','Eating_Window_End', 'phase_duration',
       'caloric_entries_num','medication_num', 'water_num','first_cal_avg',
        'first_cal_std','last_cal_avg', 'last_cal_std', 
        'mean_daily_eating_window', 'std_daily_eating_window',
       'earliest_entry', '2.5%', '97.5%', 'duration mid 95%',
       'logging_day_counts', '%_logging_day_counts', 'good_logging_days',
        '%_good_logging_days','good_window_days', '%_good_window_days', 
        'outside_window_days','%_outside_window_days', 'adherent_days',
       '%_adherent_days']]    
    
    if report_level == 0:
        return returned
    
    if txt:
        with open('treets_warning_dates.txt', 'w') as f:
            # print out missing dates with participant's id
            for x in missing_dates:
                if len(missing_dates[x])>0:
                    f.write("Participant {} didn't log any food items in the following day(s):\n".format(x))
                    print("Participant {} didn't log any food items in the following day(s):".format(x))
                    for date in missing_dates[x]:
                        f.write(str(date)+'\n')
                        print(date)
    else:
        for x in missing_dates:
            if len(missing_dates[x])>0:
                print("Participant {} didn't log any food items in the following day(s):".format(x))
                for date in missing_dates[x]:
                    print(date)
                
    if report_level == 1:
        return returned
    
    if txt:
        with open('treets_warning_dates.txt', 'a') as f:
            # print out bad logging, bad window and non-adherent dates with participant's id
            for x in bad_dates_dic:
                if len(bad_dates_dic[x])>0:
                    strings = x.split('_')
                    f.write("Participant {} have {} day(s) in the following day(s):\n".format(strings[0], strings[1]+' '+strings[2]))
                    print("Participant {} have {} day(s) in the following day(s):".format(strings[0], strings[1]+' '+strings[2]))
                    for date in bad_dates_dic[x]:
                        f.write(str(date)+'\n')
                        print(date)
    else:
        for x in bad_dates_dic:
            if len(bad_dates_dic[x])>0:
                strings = x.split('_')
                print("Participant {} have {} day(s) in the following day(s):".format(strings[0], strings[1]+' '+strings[2]))
                for date in bad_dates_dic[x]:
                    print(date)
    
    return returned

# %% ../00_core.ipynb 84
def first_cal_mean_with_error_bar(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function takes the loggings in good logging days, calculates the means and standard deviations of first_cal time for each participant and represent the calculated data with a scatter plot where the x axis is participants and the y axis is hours in a day.\n 
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.
        
    Return:\n
        - None.\n
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    df = file_loader(data_source)
    
    # leave only the loggings that are in a good logging day
    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)
    df = df[df['in_good_logging_day']==True]
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]

    first_cal_series = df.groupby([identifier, date_col])[time_col].min()
    
    
    # find means and stds for each person
    means = first_cal_series.groupby(identifier).mean().to_frame().rename(columns={'local_time':'mean'})
    stds = first_cal_series.groupby(identifier).std().fillna(0).to_frame().rename(columns={'local_time':'std'})
    
    if means.shape[0] > 50:
        print("More than 50 people are present which might make the graph look messy")
    
    temp = pd.concat([means,stds], axis=1)
    temp.sort_values('mean', inplace=True)
    
    # plot scatter plot with error bars
    plt.scatter(range(temp.shape[0]),temp['mean'])
    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt="o")
    plt.xticks([])
    plt.ylabel("Hours in a day")
    plt.title('first_cal Time per Person in Ascending Order')

# %% ../00_core.ipynb 86
def last_cal_mean_with_error_bar(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function takes the loggings in good logging days, calculates the means and standard deviations of last_cal time for each participant and represent the calculated data with a scatter plot where the x axis is participants and the y axis is hours in a day.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.
        
    Return:\n
        - None.\n
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    df = file_loader(data_source)

    # leave only the loggings that are in a good logging day
    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)
    df = df[df['in_good_logging_day']==True]
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]

    first_cal_series = df.groupby([identifier, date_col])[time_col].min()
    

    last_cal_series = df.groupby([identifier, date_col])[time_col].max()
    
    
    # find means and stds for each person
    means = last_cal_series.groupby(identifier).mean().to_frame().rename(columns={'local_time':'mean'})
    stds = last_cal_series.groupby(identifier).std().fillna(0).to_frame().rename(columns={'local_time':'std'})
    
    if means.shape[0] > 50:
        print("More than 50 people are present which might make the graph look messy")
    
    temp = pd.concat([means,stds], axis=1)
    temp.sort_values('mean', inplace=True)
    
    # plot scatter plot with error bars
    plt.scatter(range(temp.shape[0]),temp['mean'])
    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt="o")
    plt.xticks([])
    plt.ylabel("Hours in a day")
    plt.title('last_cal Time per Person in Ascending Order')

# %% ../00_core.ipynb 88
def first_cal_analysis_variability_plot(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function calculates the variability of loggings in good logging day by subtracting 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time from the 50% first_cal time. It can also make a histogram that represents the 90%-10% interval for all subjects.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.
        - plot(bool) : Whether generating a histogram for first_cal variability. Default = True.
        
    Return:\n
        - A dataframe that contains 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time minus 50% time for each subjects from the data_source file.\n
        
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    

    df = file_loader(data_source)
        
    # leave only the loggings in a good logging day
    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)
    df = df[df['in_good_logging_day']==True]
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    first_cal_series = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])
    first_cal_df = pd.DataFrame(first_cal_series)
    all_rows = []
    for index in first_cal_df.index:
        tmp_dict = dict(first_cal_series[index[0]])
        tmp_dict['id'] = index[0]
        all_rows.append(tmp_dict)
    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\
        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\
        .drop_duplicates().reset_index(drop = True)
    first_cal_variability_df = first_cal_summary_df.copy()
    
    for col in first_cal_variability_df.columns:
        if col == 'id' or col == '50%':
            continue
        first_cal_variability_df[col] = first_cal_variability_df[col] - first_cal_variability_df['50%']
    first_cal_variability_df['50%'] = first_cal_variability_df['50%'] - first_cal_variability_df['50%']
    
    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)
    sns_plot = sns.distplot( first_cal_variability_df['90%'] - first_cal_variability_df['10%'] )
    ax.set(xlabel='Variation Distribution for first_cal (90% - 10%)', ylabel='Kernel Density Estimation')
    

# %% ../00_core.ipynb 90
def last_cal_analysis_variability_plot(data_source, min_log_num = 2, min_separation = 4, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function calculates the variability of loggings in good logging day by subtracting 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time from the 50% last_cal time and makes a histogram that represents the 90%-10% interval for all subjects.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.
        - min_separation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.
        - plot(bool) : Whether generating a histogram for first_cal variability. Default = True.
    Return:\n
        - None\n
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    
    df = file_loader(data_source)
        
    # leave only the loggings that are in a good logging day
    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)
    df = df[df['in_good_logging_day']==True]
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    last_cal_series = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])
    last_cal_df = pd.DataFrame(last_cal_series)
    all_rows = []
    for index in last_cal_df.index:
        tmp_dict = dict(last_cal_series[index[0]])
        tmp_dict['id'] = index[0]
        all_rows.append(tmp_dict)
    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\
        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\
        .drop_duplicates().reset_index(drop = True)
    last_cal_variability_df = last_cal_summary_df.copy()
    
    for col in last_cal_variability_df.columns:
        if col == 'id' or col == '50%':
            continue
        last_cal_variability_df[col] = last_cal_variability_df[col] - last_cal_variability_df['50%']
    last_cal_variability_df['50%'] = last_cal_variability_df['50%'] - last_cal_variability_df['50%']
    
    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)
    sns_plot = sns.distplot( last_cal_variability_df['90%'] - last_cal_variability_df['10%'] )
    ax.set(xlabel='Variation Distribution for last_cal (90% - 10%)', ylabel='Kernel Density Estimation')
    

# %% ../00_core.ipynb 92
def first_cal_avg_histplot(data_source, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function take the first caloric event (no water or med) and calculate average event's time for each participant.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.

    Return:\n
        - None
        
    Requirements:\n
        data_source file must have the following columns:\n
            - food_type\n
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    df = file_loader(data_source)
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    df = df.query('food_type in ["f", "b"]')
    first_cal_time = df.groupby([identifier, date_col])[time_col].min()
    avg_first_cal_time = first_cal_time.reset_index().groupby(identifier)[time_col].mean()
    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)
    sns.distplot(avg_first_cal_time, kde = False)
    ax.set(xlabel='First Meal Time - Averaged by Person', ylabel='Frequency Count')

# %% ../00_core.ipynb 94
def first_cal_sample_distplot(data_source, n, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function plots the distplot for the first_cal time from n participants that will be randomly selected with replacement.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n
        - n (int): the number of distplot in the output figure
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        
    Return:\n
        - None
        
    Requirements:\n
        data_source file must have the following columns:\n
            - food_type\n
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    df = file_loader(data_source)
    df = df[df['food_type'].isin(['f','b'])]
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    first_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\
                                       [time_col].min())
    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)
    
    print('Plotting distplots for the following users:')
    for i in np.random.choice(np.array(list(set(first_cal_by_person.index.droplevel(date_col)))), n):
        print(i)
        sns.distplot(first_cal_by_person[time_col].loc[i])

# %% ../00_core.ipynb 96
def last_cal_avg_histplot(data_source, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function take the last caloric event (no water or med) and calculate average event's time for each participant.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.

    Return:\n
        - None
        
    Requirements:\n
            - food_type\n
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    df = file_loader(data_source)
    df = df.query('food_type in ["f", "b"]')
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    last_cal_time = df.groupby([identifier, date_col])[time_col].max()
    avg_last_cal_time = last_cal_time.reset_index().groupby(identifier)[time_col].mean()
    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)
    sns.distplot(avg_last_cal_time, kde = False)
    ax.set(xlabel='Last Meal Time - Averaged by Person', ylabel='Frequency Count')

# %% ../00_core.ipynb 98
def last_cal_sample_distplot(data_source, n, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function plots the distplot for the last_cal time from n participants that will be randomly selected with replacement.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.
        - n (int): the number of participants that will be randomly selected in the output figure
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
        
    Return:\n
        - None
        
    Requirements:\n
        - food_type
    
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
        
    """
    df = file_loader(data_source)
    df = df[df['food_type'].isin(['f','b'])]
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    last_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\
                                       [time_col].max())
    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)
    
    print('Plotting distplots for the following users:')
    for i in np.random.choice(np.array(list(set(last_cal_by_person.index.droplevel(date_col)))), n):
        print(i)
        sns.distplot(last_cal_by_person[time_col].loc[i])

# %% ../00_core.ipynb 100
def swarmplot(data_source, max_loggings, identifier = 1, date_col = 6, time_col = 7):
    """
    Description:\n
       This function plots the swarmplot the participants from the data_source file.\n
    
    Input:\n
        - data_source (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n
        - max_loggings (int): the max number of loggings to be plotted for each participants, loggings will be randomly selected.
        - identitfier(str) : participants' unique identifier such as id, name, etc.
        - date_col(str) : the column that represents the dates.
        - time_col(str) : the column that represents the float time.
    Return:\n
        - None
        
    Requirements:\n
        data_source file must have the following columns:\n
            - food_type\n
    Optional functions to use to have proper inputs:
        - find_date() for date_col
        - find_float_time() for time_col
    """
    
    df = file_loader(data_source)
    
    identifier = df.columns[identifier]
    # if treets functions have been used (in any order) to generate columns
    # find appropriate column names, if not check for expected column position
    if 'date' in df.columns:
        date_col = df.columns[df.columns.get_loc('date')]
    else:
        date_col = df.columns[date_col]
        
    if 'float_time' in df.columns:
        time_col = df.columns[df.columns.get_loc('float_time')]
    else:
        time_col = df.columns[time_col]
    
    def subsamp_by_cond(alldat):
        alld = []
        for apart in alldat[identifier].unique():
            dat = alldat[alldat[identifier]==apart]
            f_n_b = dat.query('food_type in ["f", "b"]')
            n = min([f_n_b.shape[0], max_loggings])
            sub = f_n_b.sample(n = n, axis=0)
            alld.append(sub)
        return pd.concat(alld)

    sample = subsamp_by_cond(df)
    fig, ax = plt.subplots(1, 1, figsize = (10, 30), dpi=300)


    ax.axvspan(3.5,6, alpha=0.2, color=[0.8, 0.8, 0.8]  )
    ax.axvspan(18,28.5, alpha=0.2, color=[0.8, 0.8, 0.8]  )
    # plt.xlabel('Hour of day')
    plt.xticks([4,8,12,16,20,24,28],[4,8,12,16,20,24,4])
    plt.title('Caloric Events for TRE Group')

    ax = sns.swarmplot(data = sample, 
                  y = identifier, 
                  x = time_col, 
                  dodge = True, 
                  color = sns.xkcd_rgb['golden rod'],
                 )

    ax.set(
        facecolor = 'white', 
        title = 'Caloric Events (Food & Bev)',
        ylabel = 'Participant',
        xlabel = 'Local Time of Consumption'
    )
