{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Process collected data from the myCircadianClock app.\n",
    "output-file: core.html\n",
    "title: Time Restricted Eating ExperimenTS API\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "\n",
    "# allows for type hinting annotations without breaking functionality\n",
    "from __future__ import annotations\n",
    "import matplotlib.figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import string\n",
    "import datetime\n",
    "import wordsegment\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "wordsegment.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils \n",
    "\n",
    "These functions primarily serve as parts of other functions, but are provided here for utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def file_loader(data_source:str|pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Flexible file loader able to read a single file path or folder path.\n",
    "    Accepts .csv and .json file format loading.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame.\n",
    "        Existing dataframes are read as is.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df\n",
    "        A single dataframe consisting of all data matching the provided file or folder path.\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(data_source, str):\n",
    "        data_lst = glob.glob(data_source)\n",
    "        dfs = []\n",
    "        for x in data_lst:\n",
    "            if x[-4:] == '.csv':\n",
    "                dfs.append(pd.read_csv(x))\n",
    "            elif x[-5:] == '.json':\n",
    "                json_file = pd.read_json(x)\n",
    "                if not isinstance(json_file, pd.DataFrame):\n",
    "                    return json_file\n",
    "                dfs.append(json_file)\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "    else:\n",
    "        df = data_source\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Providing the file loader with a specific file path outputs a single Pandas dataframe generated from that data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-12 02:30:00 +0000</td>\n",
       "      <td>milk</td>\n",
       "      <td>b</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-12 02:45:00 +0000</td>\n",
       "      <td>some medication</td>\n",
       "      <td>m</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            original_logtime        desc_text food_type      PID\n",
       "0  2021-05-12 02:30:00 +0000             milk         b  yrt1999\n",
       "1  2021-05-12 02:45:00 +0000  some medication         m  yrt1999"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader(\"data/col_test_data/toy_data_2000.csv\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file loader can also accept string patterns to read in multiple files at once. Providing a patterened path such as yrt\\*_food_data\\*.csv would load all data matching this pattern. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-12 02:30:00 +0000</td>\n",
       "      <td>Milk</td>\n",
       "      <td>b</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-12 02:45:00 +0000</td>\n",
       "      <td>Some Medication</td>\n",
       "      <td>m</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            original_logtime        desc_text food_type      PID\n",
       "0  2021-05-12 02:30:00 +0000             Milk         b  yrt1999\n",
       "1  2021-05-12 02:45:00 +0000  Some Medication         m  yrt1999"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader('data/col_test_data/yrt*_food_data*.csv').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also handle reading mixed file types. The below dataframe consists of data read from all .json and .csv files in the *data/output/* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>local_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>day_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7572733.0</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>411111.0</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>b</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID      unique_code  research_info_id     desc_text food_type  \\\n",
       "0  7572733.0  alqt14018795225             150.0         Water         w   \n",
       "1   411111.0  alqt14018795225             150.0  Coffee White         b   \n",
       "\n",
       "            original_logtime       date  local_time      time  \\\n",
       "0  2017-12-08 17:30:00+00:00 2017-12-08   17.500000  17:30:00   \n",
       "1  2017-12-09 00:01:00+00:00 2017-12-08   24.016667  00:01:00   \n",
       "\n",
       "   week_from_start    year cleaned  day_count  \n",
       "0              1.0  2017.0     NaN        NaN  \n",
       "1              1.0  2017.0     NaN        NaN  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader('data/output/*').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_date(data_source:str|pd.DataFrame,\n",
    "              h:int = 4,\n",
    "              date_col:int = 5) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extracts date from a datetime column after shifting datetime by 'h' hours.\n",
    "    A day starts 'h' hours early if 'h' is negative, or 'h' hours later if 'h' is\n",
    "    positive.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is.\n",
    "    h\n",
    "        Number of hours to shift the definition for 'date' by. h = 4 would shift days so that time membership\n",
    "        to each date starts at 4:00 AM and ends at 3:59:59 AM the next calendar day.\n",
    "    date_col\n",
    "        Column number for existing datetime column in provided data source. Data exported from mCC typically\n",
    "        has datetime as its 5th column (with indexing starting from 0).\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    date\n",
    "        Series of dates in ISO 8601 format.\n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    # fifth column of food log dataframes should represent date/time in a 24 hour system\n",
    "    col = df.columns[date_col]\n",
    "    if df[col].dtype == 'O':\n",
    "        raise TypeError(\"'{}' column must be converted to datetime object\".format(col))\n",
    "        \n",
    "    def find_date(d, h):\n",
    "        if h > 0:\n",
    "            if d.hour < h:\n",
    "                return d.date() - pd.Timedelta('1 day')\n",
    "        if h < 0:\n",
    "            if d.hour+1 > (24+h):\n",
    "                return d.date() + pd.Timedelta('1 day')\n",
    "        return d.date()\n",
    "    return df[col].apply(find_date, args=([h]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, find_date expects log dates for studies to begin at 4:00 AM. To use regular calendar dates, remember to set h = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-09\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-09"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = file_loader('data/test_food_details.csv')\n",
    "df['original_logtime'] = pd.to_datetime(df['original_logtime'])\n",
    "df['date'] = find_date(df, h = 0)\n",
    "df[['original_logtime', 'date']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, with log dates starting at the default value of 4 (4:00 AM), we see that two logs from very early morning on 2017-12-09 are counted as being logged on 2017-12-08 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-08\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-08"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = find_date(df, h = 4)\n",
    "df[['original_logtime', 'date']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, in an example where we start log days four hours earlier, the last two rows have dates that are shifted so their log date is one day later than their exact calendar datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-22 22:53:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-09\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-09\n",
       "3 2018-02-22 21:52:00+00:00  2018-02-23\n",
       "4 2018-02-22 22:53:00+00:00  2018-02-23"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = find_date(df, h = -4)\n",
    "df[['original_logtime', 'date']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_float_time(data_source:str|pd.DataFrame,\n",
    "                    h:int = 4,\n",
    "                    date_col:int = 5) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extracts time from a datetime column after shifting datetime by 'h' hours.\n",
    "    A day starts 'h' hours early if 'h' is negative, or 'h' hours later if 'h' is\n",
    "    positive.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is.\n",
    "    h\n",
    "        Number of hours to shift the definition for 'time' by. h = 4 would allow float representations of time\n",
    "        between 4 (inclusive) and 28 (exclusive), representing time that goes from 4:00 AM to 3:59:59 AM the next\n",
    "        calendar day. NOTE: h value for this function should match the h value used for generating dates.\n",
    "    date_col\n",
    "        Column number for existing datetime column in provided data source. Data exported from mCC typically\n",
    "        has datetime as its 5th column (with indexing starting from 0).\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    local_time\n",
    "        Series of times in float format (e.g. 4:36 AM -> 4.6).\n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    # fifth column of food log dataframes should represent date/time in a 24 hour system\n",
    "    col = df.columns[date_col]\n",
    "    if df[col].dtype == 'O':\n",
    "        raise TypeError(\"'{}' column must be converted to datetime object firsly\".format(col))\n",
    "    local_time = df[col].apply(lambda x: pd.Timedelta(x.time().isoformat()).total_seconds() / 3600.)\n",
    "    if h > 0:\n",
    "        local_time = np.where(local_time < h, 24 + local_time, local_time)\n",
    "        # index= to prevent mistaken assignments when data source and target df have a non-trivial index\n",
    "        return pd.Series(local_time, index=df.index) \n",
    "    if h < 0:\n",
    "        local_time = np.where(local_time > (24 + h), local_time-24., local_time)\n",
    "        return pd.Series(local_time, index=df.index)\n",
    "    return local_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_loader('data/test_food_details.csv')\n",
    "df['original_logtime'] = pd.to_datetime(df['original_logtime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, find_float_time expects studies to begin at 4:00 AM. To preserve regular calendar dates\n",
    "use h = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>float_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime  float_time\n",
       "0 2017-12-08 17:30:00+00:00   17.500000\n",
       "1 2017-12-09 00:01:00+00:00    0.016667\n",
       "2 2017-12-09 00:58:00+00:00    0.966667"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['float_time'] = find_float_time(df, h = 0)\n",
    "df[['original_logtime', 'float_time']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using positive values for h for both date and float time functions changes date ownership for a row based on its\n",
    "original logtime. Float time should be shifted by the same h value as date membership so that times belonging to a different calendar\n",
    "date can be differentiated when necessary (e.g. 2:00 AM --> 2.0, whereas 2:00 AM the next calendar day --> 26.0, for cases\n",
    "where these rows should still be grouped together on the same logging date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date  float_time\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08   17.500000\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-08   24.016667\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-08   24.966667"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['float_time'] = find_float_time(df, h = 4)\n",
    "df['date'] = find_date(df, h = 4)\n",
    "df[['original_logtime','date', 'float_time']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In rare cases, it may be valuable to shift date and time by negative values. In this example where a log date starts\n",
    "at 8:00 PM the previous calendar day and ends at 8:00 PM the current calendar day, note that the last two rows have negative float times and their date membership is shifted one date further than their original calendar datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>-2.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-22 22:53:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>-1.116667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date  float_time\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08   17.500000\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-09    0.016667\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-09    0.966667\n",
       "3 2018-02-22 21:52:00+00:00  2018-02-23   -2.133333\n",
       "4 2018-02-22 22:53:00+00:00  2018-02-23   -1.116667"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['float_time'] = find_float_time(df, h = -4)\n",
    "df['date'] = find_date(df, h = -4)\n",
    "df[['original_logtime','date', 'float_time']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def week_from_start(data_source:str|pd.DataFrame,\n",
    "                    identifier:int = 1) -> np.array:\n",
    "    \"\"\"\n",
    "    Calculates the number of weeks between each logging entry and the first logging entry\n",
    "    for each participant. A 'date' column must exist in the provided data source. \n",
    "    Using the provided find_date function is recommended.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    count_weeks\n",
    "        Array of weeks passed from log date to the minimum date for each participant.\n",
    "    \"\"\"\n",
    "        \n",
    "    df = file_loader(data_source)\n",
    "    if 'date' not in df.columns:\n",
    "        raise NameError(\"There must exist a 'date' column.\")\n",
    "    col = df.columns[df.columns.get_loc('date')]\n",
    "    identifier = df.columns[identifier]\n",
    "    \n",
    "    # Handle week from start\n",
    "    df_dic = dict(df.groupby(identifier)[col].agg(np.min))\n",
    "\n",
    "    def count_weeks(s):\n",
    "        return (s.date - df_dic[s[identifier]]).days // 7 + 1\n",
    "\n",
    "    return df.apply(count_weeks, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_loader('data/test_food_details.csv')\n",
    "df['original_logtime'] = pd.to_datetime(df['original_logtime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using find_date to ensure that a date column exists in the data source is recommended. A column labeled\n",
    "'date' is a requirement of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_code</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>week_from_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_code          original_logtime  week_from_start\n",
       "2  alqt14018795225 2017-12-09 00:58:00+00:00                1\n",
       "3  alqt14018795225 2018-02-22 21:52:00+00:00               11"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = find_date(df)\n",
    "df['week_from_start'] = week_from_start(df)\n",
    "df[['unique_code','original_logtime','week_from_start']][2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_phase_duration(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the duration (in days) of the study phase for each row.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Participant information dataframe with columns for start and ending date for that row's study phase.\n",
    "        The expected column numbers for starting and ending dates are outlined in the HOWTO document that accompanies TREETS.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df\n",
    "        Dataframe with an additional column describing study phase duration.\n",
    "    \"\"\"\n",
    "    # column order is specified in our how-to document for data from collaborators\n",
    "    start_day = df.columns[4]\n",
    "    end_day = df.columns[5]\n",
    "    \n",
    "    # checking if type conversion from string to datetime is necessary\n",
    "    if df[start_day].apply(lambda x: isinstance(x, str)).any():\n",
    "        df[start_day] = pd.to_datetime(df[start_day])\n",
    "    \n",
    "    if df[end_day].apply(lambda x: isinstance(x, str)).any():\n",
    "        df[end_day] = pd.to_datetime(df[end_day])\n",
    "    \n",
    "    df['phase_duration'] = df[end_day] - df[start_day] + pd.Timedelta(\"1 days\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  phase_duration\n",
       "0         3 days\n",
       "1         4 days\n",
       "2         3 days\n",
       "3         4 days\n",
       "4            NaT"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_phase_duration(pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'))[['phase_duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_food_data(data_source:str|pd.DataFrame,\n",
    "                   h:int,\n",
    "                   identifier:int = 1,\n",
    "                   datetime_col:int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads and processes existing logging data, adding specific datetime information in formats\n",
    "    more suitable for TREETS functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is.\n",
    "    h\n",
    "        Number of hours to shift the definition of 'date' by. h = 4 would indicate that a log date begins at\n",
    "        4:00 AM and ends the following calendar day at 3:59:59 AM. Float representations of time would therefore\n",
    "        go from 4.0 (inclusive) to 28.0 (exclusive) to represent 'date' membership for days shifted from their\n",
    "        original calendar date.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    datetime_col\n",
    "        Column number for an existing datetime column in provided data source. Data exported from mCC typically\n",
    "        has datetime as its 5th column (with indexing starting from 0).\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    food_data\n",
    "        Dataframe with additional date, float time, and week from start columns.\n",
    "    \"\"\"\n",
    "    food_data = file_loader(data_source)\n",
    "    # identifier column(s) should be 0 and 1, with 1 being the study specific identifier\n",
    "    identifier = food_data.columns[identifier]\n",
    "    # fifth column of food log dataframes should represent date/time in a 24 hour system\n",
    "    datetime_col = food_data.columns[datetime_col]\n",
    "        \n",
    "    try:\n",
    "        food_data = food_data.drop(columns = ['foodimage_file_name'])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    def handle_time(s):\n",
    "        \"\"\"\n",
    "        helper function to get rid of am/pm in the end of each time string\n",
    "        \"\"\"\n",
    "        tmp_s = s.replace('p.m.', '').replace('a.m.', '')\n",
    "        try:\n",
    "            return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "        except:\n",
    "            try:\n",
    "                if int(tmp_s.split()[1][:2]) > 12:\n",
    "                    tmp_s = s.replace('p.m.', '').replace('a.m.', '').replace('PM', '').replace('pm', '')\n",
    "                return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "    food_data[datetime_col] = food_data[datetime_col].apply(handle_time)\n",
    "    food_data = food_data.dropna().reset_index(drop = True)\n",
    "    food_data['date'] = find_date(food_data, h)\n",
    "    \n",
    "    # Handle the time - Time in floating point format\n",
    "    food_data['float_time'] = find_float_time(food_data, h)\n",
    "    \n",
    "    # Handle the time - Time in Datetime object format\n",
    "    food_data['time'] = pd.DatetimeIndex(food_data[datetime_col]).time\n",
    "    \n",
    "    # Handle week from start\n",
    "    food_data['week_from_start'] = week_from_start(food_data)\n",
    "    \n",
    "    food_data['year'] = food_data.date.apply(lambda d: d.year)\n",
    "    \n",
    "    return food_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7572733</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>411111</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>b</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID      unique_code  research_info_id     desc_text food_type  \\\n",
       "0  7572733  alqt14018795225               150         Water         w   \n",
       "1   411111  alqt14018795225               150  Coffee White         b   \n",
       "\n",
       "           original_logtime        date  float_time      time  \\\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08   17.500000  17:30:00   \n",
       "1 2017-12-09 00:01:00+00:00  2017-12-08   24.016667  00:01:00   \n",
       "\n",
       "   week_from_start  year  \n",
       "0                1  2017  \n",
       "1                1  2017  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_food_data('data/test_food_details.csv', h = 4).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def in_good_logging_day(data_source:str|pd.DataFrame,\n",
    "                        min_log_num:int = 2,\n",
    "                        min_separation:int = 5,\n",
    "                        identifier:int = 1,\n",
    "                        date_col:int = 6,\n",
    "                        time_col:int = 7) -> np.array:\n",
    "    \"\"\"\n",
    "    Calculates if each log is considered to be within a 'good logging day'. A log day is considered 'good' if there \n",
    "    are at least the minimum number of required logs, with a minimum specified hour separation between the first and last\n",
    "    log for that log date. It is recommended that you use find_date and find_float_time to generate necessary date and\n",
    "    time columns for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is.\n",
    "    \n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    \n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    \n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    in_good_logging_day\n",
    "        Boolean array describing whether each log is a 'good' logging day.\n",
    "    \"\"\"\n",
    "    def adherent(s):\n",
    "        return len(s.values) >= min_log_num and (max(s.values) - min(s.values)) >= min_separation\n",
    "        \n",
    "    df = file_loader(data_source)\n",
    "    identifier = df.columns[identifier]\n",
    "    \n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "        \n",
    "    adherent_dict = dict(df.groupby([identifier, date_col])[time_col].agg(adherent))\n",
    "\n",
    "    return df.apply(lambda x: adherent_dict[(x[identifier], x.date)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "      <th>in_good_logging_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7572733</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>411111</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>b</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID      unique_code  research_info_id     desc_text food_type  \\\n",
       "0  7572733  alqt14018795225               150         Water         w   \n",
       "1   411111  alqt14018795225               150  Coffee White         b   \n",
       "\n",
       "           original_logtime        date  float_time      time  \\\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08   17.500000  17:30:00   \n",
       "1 2017-12-09 00:01:00+00:00  2017-12-08   24.016667  00:01:00   \n",
       "\n",
       "   week_from_start  year  in_good_logging_day  \n",
       "0                1  2017                 True  \n",
       "1                1  2017                 True  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "df['in_good_logging_day'] = in_good_logging_day(df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FoodParser:\n",
    "    \"\"\"\n",
    "    Food parser handles taking unprocessed food log entries and adding relevant\n",
    "    information from a pre-made dictionary. This includes matching unprocessed terms to\n",
    "    their likely matches, adding food type and other identifying information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes food parser object.\"\"\"\n",
    "        # read in manually annotated file\n",
    "        parser_keys_df = pd.read_csv(\"data/12_08_2023_parser_keys.csv\")\n",
    "        all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(\n",
    "            parser_keys_df\n",
    "        )\n",
    "        correction_dic = pd.read_json(\"data/correction_dic.json\", typ=\"series\")\n",
    "        \n",
    "        self.all_gram_set = all_gram_set\n",
    "        self.food_type_dict = food_type_dict\n",
    "        self._food_phrases = np.unique(\n",
    "            [\n",
    "                word\n",
    "                for key in self.food_type_dict\n",
    "                for word in key.split()\n",
    "                if self.food_type_dict[key] in [\"f\", \"b\", \"m\"] and len(word) > 2\n",
    "            ]\n",
    "        )\n",
    "        self.correction_dic = correction_dic\n",
    "        self.food2tags = food2tags\n",
    "\n",
    "        # Load common stop words and nlp type objects\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "        self.stop_words = stopwords.words(\"english\")\n",
    "        self.stop_words.remove(\"out\")  # since pre work out is a valid beverage name\n",
    "        self.stop_words.remove(\"no\")\n",
    "        self.stop_words.remove(\"not\")\n",
    "        self.stop_words.remove(\"and\")\n",
    "        self.stop_words.remove(\"m\")\n",
    "        self.stop_words.remove(\"of\")\n",
    "        # preventing common vitamins from being removed\n",
    "        self.stop_words.remove(\"d\")\n",
    "\n",
    "    @staticmethod\n",
    "    def process_parser_keys_df(parser_keys_df):\n",
    "        \"\"\"Takes a dataframe of parser keys and processes gram sets and\n",
    "        food type information.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        parser_keys_df: pd.DataFrame\n",
    "            Dataframe of parser keys.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        all_gram_set: list\n",
    "            List of sets where each set corresponds to all (n-grams for n in range 1-6)\n",
    "\n",
    "        food_type_dict: dict\n",
    "            Dictionary of gram key and food type pairings where gram key\n",
    "            is the food (e.g. 'milk') and the value is the type (e.g. 'b')\n",
    "\n",
    "        food2tags: dict\n",
    "            Dictionary of gram key and corresponding tags. (e.g. key = 'vodka'\n",
    "            with tags 'alcohol', 'liquor', etc..)\n",
    "        \"\"\"\n",
    "        # f = food, b = beverage, m = medicine, w = water\n",
    "        # below are the possible valid item types\n",
    "        parser_keys_df = parser_keys_df.query(\n",
    "            'food_type in [\"f\", \"b\", \"m\", \"w\", \"modifier\", \"general\", \"stopword\", \"selfcare\"]'\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # creates an all_gram_set to store n-grams (e.g. 1-grams, 2-grams, etc.)\n",
    "        all_gram_set = []\n",
    "        for i in range(1, 5 + 1):\n",
    "            all_gram_set.append(\n",
    "                set(parser_keys_df.query(\"gram_type == \" + str(i))[\"gram_key\"].values)\n",
    "            )\n",
    "\n",
    "        # pair gram key and corresponding food type for items as a dictionary\n",
    "        food_type_dict = dict(\n",
    "            zip(parser_keys_df[\"gram_key\"].values, parser_keys_df[\"food_type\"].values)\n",
    "        )\n",
    "\n",
    "        # find all relevant tags\n",
    "        def find_all_tags(df):\n",
    "            \"\"\"Finds tags for each 'food' item.\n",
    "\n",
    "            Parameters\n",
    "            -----------\n",
    "            df: DataFrame\n",
    "                Dataframe with food tag columns (e.g. tag1)\n",
    "            \"\"\"\n",
    "            tags = []\n",
    "            for i in range(1, 8):\n",
    "                # expects column names of the format 'tag#'\n",
    "                if not isinstance(df[\"tag\" + str(i)], str) and np.isnan(\n",
    "                    df[\"tag\" + str(i)]\n",
    "                ):\n",
    "                    # append empty tag if tag doesn't exist\n",
    "                    # forced food2tags to have length 8 + can restore tags\n",
    "                    # in order with a dictionary\n",
    "                    tags.append(\"\")\n",
    "                    continue\n",
    "                # append existing tags\n",
    "                tags.append(df[\"tag\" + str(i)])\n",
    "            return tags\n",
    "\n",
    "        # pair gram_keys (food names) and their corresponding list of informative tags\n",
    "        food2tags = dict(\n",
    "            zip(\n",
    "                parser_keys_df[\"gram_key\"].values,\n",
    "                parser_keys_df.apply(find_all_tags, axis=1),\n",
    "            )\n",
    "        )\n",
    "        return all_gram_set, food_type_dict, food2tags\n",
    "\n",
    "    ########## Pre-Processing ##########\n",
    "\n",
    "    # Function for removing numbers\n",
    "    @staticmethod\n",
    "    def handle_numbers(text):\n",
    "        \"\"\"Removes numeric characters from text.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text: str\n",
    "            Text to be cleaned\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text: str\n",
    "            Text with numbers removed.\n",
    "        \"\"\"\n",
    "        # regex pattern removes all decimal numbers\n",
    "        text = re.sub(\"[0-9]+\\.[0-9]+\", \"\", text)\n",
    "        # retaining common items (h2o, co2, v8, ag1)\n",
    "        if any(sub in text for sub in [\"v8\", \"h2\", \"ag1\", \"co2\", \"0z\"]):\n",
    "            # regex pattern strips any numbers not immediately preceded or followed by a letter\n",
    "            text = re.sub(r\"(?<![a-zA-Z])\\d+(?![a-zA-Z])\", \"\", text)\n",
    "        else:\n",
    "            # regex pattern strips all whole numbers\n",
    "            text = re.sub(\"[0-9+]\", \"\", text)\n",
    "        # regex pattern truncates any duplicate whitespace (e.g. \"  \" becomes  \" \")\n",
    "        text = re.sub(\"\\s\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    # Function for removing punctuation\n",
    "    @staticmethod\n",
    "    def drop_punc(text):\n",
    "        \"\"\"\n",
    "        Removes punctuation from text.\n",
    "\n",
    "         Parameters\n",
    "        ----------\n",
    "        text: str\n",
    "            Text to be cleaned\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text: str\n",
    "            Text with punctuation removed.\n",
    "        \"\"\"\n",
    "        return re.sub(\"[%s]\" % re.escape(string.punctuation), \" \", text)\n",
    "\n",
    "    # Remove normal stopwords\n",
    "    def remove_stop(self, text):\n",
    "        \"\"\"\n",
    "        Removes english stop words defined by nltk (e.g. 'the') from text.\n",
    "\n",
    "         Parameters\n",
    "        ----------\n",
    "        text: str\n",
    "            Text to be cleaned\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text: str\n",
    "            Text with stop words removed.\n",
    "        \"\"\"\n",
    "        return \" \".join([word for word in text.split() if word not in self.stop_words])\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        \"\"\"\n",
    "        Handles text cleaning overall.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        text: str\n",
    "            Text to be pre-processed/cleaned.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        text: str\n",
    "            Cleaned text.\n",
    "        \"\"\"\n",
    "        return self.remove_stop(\n",
    "            self.handle_numbers(self.drop_punc(text.lower()))\n",
    "        ).strip()\n",
    "\n",
    "    ########## Handle Format ##########\n",
    "    @staticmethod\n",
    "    def handle_front_mixing(sent, front_mixed_tokens):\n",
    "        \"\"\"\n",
    "        Handles cleaned front mixed tokens, which seem to be some sort of digit/numerical.\n",
    "        Potentially dealing with cleaning out item quantities? (e.g. 12oz?)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sent: str\n",
    "            Phrase/sentence describing a food item.\n",
    "\n",
    "        front_mixed_tokens: list\n",
    "            List of front mixed tokens (I think these are supposed to be amount tokens),\n",
    "            e.g. 12oz\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        sent: str\n",
    "            Original sentence with any front mixed tokens cleaned (e.g. 12oz --> 12 oz).\n",
    "        \"\"\"\n",
    "        cleaned_tokens = []\n",
    "        for t in sent.split():\n",
    "            if t in front_mixed_tokens:\n",
    "                number = re.findall(\"\\d+[xX]*\", t)[0]\n",
    "                for t_0 in t.replace(number, number + \" \").split():\n",
    "                    cleaned_tokens.append(t_0)\n",
    "            else:\n",
    "                cleaned_tokens.append(t)\n",
    "        return \" \".join(cleaned_tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_x2(sent, times_x_tokens):\n",
    "        \"\"\"\n",
    "        Handles variations in spacing or capitalization for the same token.\n",
    "        e.g. ('x2' vs 'X2' vs 'X 2' vs 'X 2')\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sent: str\n",
    "            Phrase/sentence describing a food item.\n",
    "\n",
    "        times_x_tokens: list of 'times x' tokens to be dealt with.\n",
    "\n",
    "        Returns\n",
    "        --------\n",
    "        sent: str\n",
    "            Original sentence with any 'times x' variations cleaned.\n",
    "        \"\"\"\n",
    "        for token in times_x_tokens:\n",
    "            sent = sent.replace(token, \" \" + token.replace(\" \", \"\").lower() + \" \")\n",
    "\n",
    "        return \" \".join([s for s in sent.split() if s != \"\"]).strip()\n",
    "\n",
    "    def clean_format(self, sent):\n",
    "        \"\"\"\n",
    "        Cleans text of any amount indications. Specifically deals with\n",
    "        front mixing (amount of an item noted at the front of the string)\n",
    "        and 'times x' where multiple of an item is recorded in the style of\n",
    "        (item x 2) or some variation of that.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        sent: str\n",
    "            Sentence to be cleaned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        sent: str\n",
    "            Sentence after all amount/format style cleaning applied.\n",
    "        \"\"\"\n",
    "        # Problem 1: 'front mixing'\n",
    "        front_mixed_tokens = re.findall(\"\\d+[^\\sxX]+\", sent)\n",
    "        if len(front_mixed_tokens) != 0:\n",
    "            sent = self.handle_front_mixing(sent, front_mixed_tokens)\n",
    "\n",
    "        # Problem 2: 'x2', 'X2', 'X 2', 'x 2'\n",
    "        times_x_tokens = re.findall(\"[xX]\\s*?\\d\", sent)\n",
    "        if len(times_x_tokens) != 0:\n",
    "            sent = self.handle_x2(sent, times_x_tokens)\n",
    "        return sent\n",
    "\n",
    "    ########## Handle Typos ##########\n",
    "\n",
    "    def fix_spelling(self, entry):\n",
    "        \"\"\"Corrects spelling mistakes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        entry: str\n",
    "            String entry to be corrected.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        entry: str\n",
    "            Original entry with spelling corrections applied.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "\n",
    "        # if entry is recognized keep it\n",
    "        if entry in self.food_type_dict:\n",
    "            return entry\n",
    "        # try looking for entry in correction dictionary\n",
    "        if entry in self.correction_dic:\n",
    "            return \" \".join(\n",
    "                [self.wnl.lemmatize(i) for i in self.correction_dic[entry].split()]\n",
    "            )\n",
    "        # try lemmatized version of whole entry\n",
    "        if self.wnl.lemmatize(entry) in self.food_type_dict:\n",
    "            return self.wnl.lemmatize(entry)\n",
    "        # try correcting individual tokens within entry phrase\n",
    "        for token in entry.split():\n",
    "            # try lemmatized version of the token\n",
    "            lem_token = self.wnl.lemmatize(token)\n",
    "            if (lem_token in self._food_phrases) or (lem_token in self.food_type_dict):\n",
    "                token = lem_token\n",
    "            # try looking for token in correction dictionary\n",
    "            elif token in self.correction_dic:\n",
    "                token = \" \".join(\n",
    "                    [self.wnl.lemmatize(i) for i in self.correction_dic[token].split()]\n",
    "                )\n",
    "            # check if token is incorrectly joined\n",
    "            # (e.g. blueberrymuffin instead of blueberry muffin)\n",
    "            elif token not in self._food_phrases:\n",
    "                temp = []\n",
    "                token_alt = wordsegment.segment(token)\n",
    "                for word in token_alt:\n",
    "                    if self.wnl.lemmatize(word) in self._food_phrases:\n",
    "                        temp += [word]\n",
    "                    # check if split word needs spell correction\n",
    "                    elif (\n",
    "                        word in self.correction_dic\n",
    "                        and self.correction_dic[word] in self._food_phrases\n",
    "                    ):\n",
    "                        temp += [self.correction_dic[word]]\n",
    "                # if there are any newly corrected/unjoined tokens add them back to the result\n",
    "                if len(temp) > 1:\n",
    "                    token = \" \".join([self.wnl.lemmatize(i) for i in temp])\n",
    "            result.append(token.strip())\n",
    "        return \" \".join(result).strip()\n",
    "\n",
    "    def handle_all_cleaning(self, entry):\n",
    "        \"\"\"\n",
    "        Performs all types of text cleaning.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        entry: str\n",
    "            String entry to be corrected.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        entry: str\n",
    "            String entry with all forms of pre-processing and cleaning\n",
    "            applied.\n",
    "        \"\"\"\n",
    "        entry = self.pre_processing(entry)\n",
    "        entry = self.clean_format(entry)\n",
    "        entry = self.fix_spelling(entry)\n",
    "        entry = re.sub(\"\\s\\s+\", \" \", entry)\n",
    "        return entry\n",
    "\n",
    "    ########## Handle Gram Matching ##########\n",
    "    @staticmethod\n",
    "    def parse_single_gram(gram_length, gram_set, gram_lst, sentence_tag):\n",
    "        \"\"\"\n",
    "        Parses a single gram, combining words as necessary for the proper gram\n",
    "        length (e.g. grape juice is a bigram) and then adding any new words\n",
    "        to a food list.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gram_length: int\n",
    "            Length of gram to look for (e.g. 2 --> bigram).\n",
    "\n",
    "        gram_set: list\n",
    "            Existing gram set.\n",
    "\n",
    "        gram_list: list\n",
    "            List of grams to check for entry into the gram set.\n",
    "\n",
    "        sentence tag: list\n",
    "            List of tags for the sentence?\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            food_lst: list\n",
    "                All unique items for the given gram being parsed.\n",
    "        \"\"\"\n",
    "        food_lst = []\n",
    "        for i, gram in enumerate(gram_lst):\n",
    "            # if not a unigram, combine the words from the gram list into a\n",
    "            # gram string\n",
    "            if gram_length > 1:\n",
    "                curr_word = \" \".join(gram)\n",
    "            else:\n",
    "                curr_word = gram\n",
    "            if (\n",
    "                curr_word in gram_set\n",
    "                and sum([t != \"Unknown\" for t in sentence_tag[i : i + gram_length]])\n",
    "                == 0\n",
    "            ):\n",
    "                # add any first occurance of a food to the food list\n",
    "                sentence_tag[i : i + gram_length] = str(gram_length)\n",
    "                food_lst.append(curr_word)\n",
    "        return food_lst\n",
    "\n",
    "    def parse_single_entry(self, entry, return_sentence_tag=False):\n",
    "        \"\"\"\n",
    "        Handles pre-processing, cleaning, and gram processing for a single entry.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        entry: str\n",
    "            Entry to be processed\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        All grams of length 5 or under for the particular entry.\n",
    "        \"\"\"\n",
    "        cleaned = self.handle_all_cleaning(entry)\n",
    "\n",
    "        # Create tokens and n-grams\n",
    "        tokens = nltk.word_tokenize(cleaned)\n",
    "        bigram = list(nltk.ngrams(tokens, 2)) if len(tokens) > 1 else None\n",
    "        trigram = list(nltk.ngrams(tokens, 3)) if len(tokens) > 2 else None\n",
    "        quadgram = list(nltk.ngrams(tokens, 4)) if len(tokens) > 3 else None\n",
    "        pentagram = list(nltk.ngrams(tokens, 5)) if len(tokens) > 4 else None\n",
    "        all_gram_lst = [tokens, bigram, trigram, quadgram, pentagram]\n",
    "\n",
    "        # Create an array of tags\n",
    "        sentence_tag = np.array([\"Unknown\"] * len(tokens))\n",
    "\n",
    "        all_food = []\n",
    "        for gram_length in [5, 4, 3, 2, 1]:\n",
    "            if len(tokens) < gram_length:\n",
    "                continue\n",
    "            tmp_food_lst = self.parse_single_gram(\n",
    "                gram_length,\n",
    "                self.all_gram_set[gram_length - 1],\n",
    "                all_gram_lst[gram_length - 1],\n",
    "                sentence_tag,\n",
    "            )\n",
    "            all_food += tmp_food_lst\n",
    "        if return_sentence_tag:\n",
    "            return all_food, sentence_tag\n",
    "        return all_food\n",
    "\n",
    "    def parse_food(self, series, calc_unknowns = False):\n",
    "        \"\"\"\n",
    "        Parses a series of single food entries.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        series: list or series of str\n",
    "            Food entries to be parsed.\n",
    "        \n",
    "        calc_unknowns: bool\n",
    "            If true, includes unknown token information in return. Default is false.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Series of parsed food items.\n",
    "        \"\"\"\n",
    "        def parse(x, calc_unknowns = calc_unknowns):\n",
    "            return self._parse_food(x, calc_unknowns)\n",
    "        vec = np.vectorize(parse)\n",
    "        return vec(series)\n",
    "\n",
    "    def _parse_food(self, entry, calc_unknowns = False):\n",
    "        \"\"\"\n",
    "        Parses a single food entry.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        entry: str\n",
    "            Food entry to be parsed.\n",
    "        \n",
    "        calc_unknowns: bool\n",
    "            If true, includes unknown token information in return. Default is false.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        A single parsed food entry.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        unknown_tokens = []\n",
    "        num_unknown = 0\n",
    "        num_token = 0\n",
    "\n",
    "        for word in entry.split(\",\"):\n",
    "            # previous versions of this if statement assume that\n",
    "            # return_sentence_tag is True and will actually break\n",
    "            # if the default parameters are used\n",
    "            all_food, sentence_tag = self.parse_single_entry(word, True)\n",
    "            result += all_food\n",
    "            if calc_unknowns:\n",
    "                if sentence_tag is not None and len(sentence_tag) > 0:\n",
    "                    num_unknown += sum(np.array(sentence_tag) == \"Unknown\")\n",
    "                    num_token += len(sentence_tag)\n",
    "                    cleaned = nltk.word_tokenize(self.handle_all_cleaning(word))\n",
    "\n",
    "                    # Return uncaught tokens, grouped into sub-sections\n",
    "                    tmp_unknown = \"\"\n",
    "                    for i, tag in enumerate(sentence_tag):\n",
    "                        if tag == \"Unknown\":\n",
    "                            tmp_unknown += \" \" + cleaned[i]\n",
    "                            if i == len(sentence_tag) - 1:\n",
    "                                unknown_tokens.append(tmp_unknown.strip())\n",
    "                        elif tmp_unknown != \"\":\n",
    "                            unknown_tokens.append(tmp_unknown.strip())\n",
    "                            tmp_unknown = \"\"\n",
    "        if calc_unknowns:\n",
    "            return np.array(\n",
    "                [result, num_token, num_unknown, unknown_tokens], dtype = \"object\"\n",
    "            )\n",
    "            \n",
    "        return np.array(result, dtype = \"object\")\n",
    "\n",
    "    def find_food_type(self, food):\n",
    "        \"\"\"\n",
    "        Finds corresponding food-item type for a given food. Types include\n",
    "        beverage, water, food, medicine.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        food: str\n",
    "            Food to find type for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Corresponding food type abbrevation or 'u' if not found.\n",
    "        \"\"\"\n",
    "        if food in self.food_type_dict:\n",
    "            return self.food_type_dict[food]\n",
    "        # shorthand for unknown\n",
    "        return \"u\"\n",
    "\n",
    "    ################# DataFrame Functions #################\n",
    "    def expand_entries(self, df):\n",
    "        \"\"\"\n",
    "        Creates an 'exploded' version of the given dataframe, expanding entries\n",
    "        from within list-style 'desc_text' column entries such that each\n",
    "        entry in a list has its own corresponding row.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pd.Dataframe\n",
    "            Dataframe to be expanded.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        df: pd.Dataframe\n",
    "            Expanded dataframe.\n",
    "        \"\"\"\n",
    "        assert \"desc_text\" in df.columns, '[ERROR] Required a column of \"desc_text\".'\n",
    "        df = df.copy()\n",
    "        df[\"desc_text\"] = df[\"desc_text\"].str.split(\",\")\n",
    "        df = df.explode(\"desc_text\")\n",
    "        # remove entries that are only spaces or digits (entries with no alphabetical characters)\n",
    "        df = df[~df[\"desc_text\"].str.isspace()]\n",
    "        df = df[~df[\"desc_text\"].str.isdigit()]\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def clean_loggings(data_source:str|pd.DataFrame,\n",
    "                   identifier:int = 1) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and attempts typo correction for all logging text entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_parsed\n",
    "        Dataframe with an additional column containing cleaned and typo corrected item entries.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = file_loader(data_source)\n",
    "    identifier = df.columns[identifier]\n",
    "    text_col = df.columns[df.columns.get_loc('desc_text')]\n",
    "    \n",
    "    # initialize food parser instance\n",
    "    fp = FoodParser()\n",
    "    \n",
    "    # parse food\n",
    "    parsed = fp.parse_food(df[\"desc_text\"])\n",
    "    df_parsed = pd.DataFrame({\n",
    "    identifier: df[identifier],\n",
    "    text_col: df[text_col],\n",
    "    'cleaned': parsed\n",
    "    })\n",
    "    \n",
    "    return df_parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text descriptions of food items are cleaned using a built-in dictionary of common typos and corrections for each phrase. Phrases are then matched using a dictionary of known n-gram item names. The resulting item(s) are provided as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_code</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Water</td>\n",
       "      <td>[water]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>[coffee, white]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Salad</td>\n",
       "      <td>[salad]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_code     desc_text          cleaned\n",
       "0  alqt14018795225         Water          [water]\n",
       "1  alqt14018795225  Coffee White  [coffee, white]\n",
       "2  alqt14018795225         Salad          [salad]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_loggings('data/output/public.json').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_types(data_source:str|pd.DataFrame,\n",
    "              food_type:str|list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters logs for only logs of specified type(s).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. Folder paths\n",
    "        with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. A column 'food_type' is required to be within the data.\n",
    "    \n",
    "    food_type\n",
    "        A single food type, or list of food types. Valid types are 'f': food, 'b': beverage, 'w': water,\n",
    "        and 'm': medication.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filtered\n",
    "        Dataframe filtered for only logs of specific type(s).        \n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_source)\n",
    "        \n",
    "    if len(food_type) == 0:\n",
    "        return df\n",
    "    \n",
    "    if isinstance(food_type, str):\n",
    "        food_type = [food_type]\n",
    "        \n",
    "    if any([i not in ['f', 'b', 'w', 'm'] for i in food_type]):\n",
    "        raise Exception(\"one or more logging types are invalid.\")\n",
    "    filtered = df[df['food_type'].isin(food_type)]\n",
    "        \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type selection accepts multiple types at once as a list of entry types. All types chosen must be valid.\n",
    "\n",
    "Available food types include:\n",
    "\n",
    "1. 'f': Food\n",
    "\n",
    "1. 'b': Beverage\n",
    "\n",
    "1. 'w': Water\n",
    "\n",
    "1. 'm': Medication\n",
    "\n",
    "\n",
    "Flavored water beverages such as La Croix are counted as 'water' and not as 'beverage'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_code</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Salad</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alqt78896444285</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_code desc_text food_type\n",
       "0  alqt14018795225     Water         w\n",
       "2  alqt14018795225     Salad         f\n",
       "3  alqt78896444285     Water         w"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_types('data/output/baseline.json',['w', 'f'])[['unique_code','desc_text','food_type']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering for a single type is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_code</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Caffeine</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Caffeine</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>Caffeine</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         unique_code desc_text food_type\n",
       "323  alqt14018795225  Caffeine         m\n",
       "361  alqt14018795225  Caffeine         m\n",
       "420  alqt14018795225  Caffeine         m"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "get_types(df, 'm')[['unique_code','desc_text','food_type']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def count_caloric_entries(df:pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of food ('f') and beverage ('b') loggings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    num_caloric_entries\n",
    "        Number of caloric (food or beverage) entries found.\n",
    "    \"\"\"\n",
    "    if 'food_type' not in df.columns:\n",
    "        raise Exception(\"'food_type' column must exist in the dataframe.\")\n",
    "        \n",
    "    food_type_col = df.columns[df.columns.get_loc('food_type')]\n",
    "    num_caloric = df[df[food_type_col].isin(['f','b'])].shape[0]\n",
    "    \n",
    "    return num_caloric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4603"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "count_caloric_entries(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_daily_eating_duration(df:pd.DataFrame,\n",
    "                               date_col:int = 6,\n",
    "                               time_col:int = 7) -> float:\n",
    "    \"\"\"\n",
    "    Calculates mean daily eating window by taking the average of each day's eating window. An eating window\n",
    "    is defined as the duration of time between first and last caloric (food or beverage) intake. It is\n",
    "    recommended that you use find_date and find_float_time to generate necessary date and time columns for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_daily_eating_duration\n",
    "        Float representation of average daily eating window duration.\n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    breakfast_time = df.groupby(date_col)[time_col].agg(min)\n",
    "    dinner_time = df.groupby(date_col)[time_col].agg(max)\n",
    "    return (dinner_time - breakfast_time).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.038679245283017"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "mean_daily_eating_duration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def std_daily_eating_duration(df:pd.DataFrame,\n",
    "                              date_col:int = 6,\n",
    "                              time_col:int = 7) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation of the daily eating window. An eating window\n",
    "    is defined as the duration of time between first and last caloric (food or beverage) intake. \n",
    "    It is recommended that you use find_date and find_float_time to generate necessary date and time\n",
    "    columns for this function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    std_daily_eating_duration\n",
    "        Float representation of the standard deviation of daily eating window duration.\n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "        \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    breakfast_time = df.groupby(date_col)[time_col].agg(min)\n",
    "    dinner_time = df.groupby(date_col)[time_col].agg(max)\n",
    "\n",
    "    return (dinner_time - breakfast_time).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.018679942775867"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "std_daily_eating_duration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def earliest_entry(df:pd.DataFrame,\n",
    "                   time_col:int = 7) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the earliest recorded caloric (food or beverage) entry. It is recommended that \n",
    "    you use find_float_time to generate necessary the time column for this function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    earliest_entry\n",
    "        Float representation of the earliest logtime on any date.\n",
    "    \"\"\"\n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "        \n",
    "    df = get_types(df, ['f', 'b'])\n",
    "    return df[time_col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "earliest_entry(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_first_cal(df:pd.DataFrame,\n",
    "                   date_col:int = 6,\n",
    "                   time_col:int = 7) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the average time of first caloric intake. It is recommended that you use\n",
    "    find_date and find_float_time to generate necessary date and time columns for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_first_cal\n",
    "        Float representation of average first caloric entry time. \n",
    "    \"\"\"\n",
    "    \n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    return df.groupby([date_col])[time_col].min().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.22680817610063"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "mean_first_cal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_code\n",
       "alqt1148284857      7.315278\n",
       "alqt14018795225     7.635938\n",
       "alqt16675467779     6.153904\n",
       "alqt21525720972    13.211957\n",
       "alqt45631586569    15.056295\n",
       "alqt5833085442     12.551515\n",
       "alqt62359040167     7.252137\n",
       "alqt6695047873      7.573077\n",
       "alqt78896444285     6.347510\n",
       "alqt8668165687      9.702555\n",
       "Name: ID, dtype: float64"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the average mean first cal time for each participant\n",
    "df.groupby(['unique_code']).agg(mean_first_cal, date_col = 6, time_col = 7).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def std_first_cal(df:pd.DataFrame,\n",
    "                  date_col:int = 6,\n",
    "                  time_col:int = 7) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation for time of first caloric intake. It is recommended that\n",
    "    you use find_date and find_float_time to generate necessary date and time columns for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    std_first_cal\n",
    "        Float representation of the standard deviation of first caloric entry time. \n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "        \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    return df.groupby([date_col])[time_col].min().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.591417471559444"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "std_first_cal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_last_cal(df:pd.DataFrame,\n",
    "                  date_col:int = 6,\n",
    "                  time_col:int = 7) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the average time of last caloric intake. It is recommended that you use\n",
    "    find_date and find_float_time to generate necessary date and time columns for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_last_cal\n",
    "        Float representation of average last caloric entry time.\n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[6]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[7]\n",
    "        \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    return df.groupby([date_col])[time_col].max().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.265487421383646"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "mean_last_cal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def std_last_cal(df:pd.DataFrame,\n",
    "                 date_col:int = 6,\n",
    "                 time_col:int = 7) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation for time of last caloric intake. It is recommended that\n",
    "    you use find_date and find_float_time to generate necessary date and time columns for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    std_last_cal\n",
    "        Float representation of the standard deviation of last caloric entry time. \n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "        \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    return df.groupby([date_col])[time_col].max().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.359435007580498"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "std_last_cal(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_daily_eating_occasions(df: pd.DataFrame,\n",
    "                                date_col:int = 6,\n",
    "                                time_col:int = 7) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the average number of daily eating occasions. An eating occasion is a single caloric (food or beverage)\n",
    "    log. It is recommended that you use find_date and find_float_time to generate necessary date and time columns \n",
    "    for this function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_daily_eating_occasion\n",
    "        Average number of daily eating occasions. \n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    \n",
    "    return df.groupby([date_col])[time_col].nunique().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8915094339622645"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "mean_daily_eating_occasions(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def std_daily_eating_occasions(df: pd.DataFrame,\n",
    "                                date_col:int = 6,\n",
    "                                time_col:int = 7) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation of the number of daily eating occasions. An eating occasion is a single caloric (food or beverage)\n",
    "    log. It is recommended that you use find_date and find_float_time to generate necessary date and time columns \n",
    "    for this function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    std_daily_eating_occasion\n",
    "        Standard deviation of the number of daily eating occasions. \n",
    "    \"\"\"\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    \n",
    "    return df.groupby([date_col])[time_col].nunique().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.44839423402741"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "std_daily_eating_occasions(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_daily_eating_midpoint(df: pd.DataFrame,\n",
    "                                date_col:int = 6,\n",
    "                                time_col:int = 7) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the average daily midpoint eating occasion time. It is recommended that\n",
    "    you use find_date and find_float_time to generate necessary date and time columns for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mean_daily_eating_midpoint\n",
    "        Float representation of the average daily midpoint eating occasion time. \n",
    "    \"\"\" \n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    \n",
    "    return df.groupby([date_col])[time_col].median().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.536425576519914"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "mean_daily_eating_midpoint(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def std_daily_eating_midpoint(df: pd.DataFrame,\n",
    "                                date_col:int = 6,\n",
    "                                time_col:int = 7) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the standard deviation of the daily midpoint eating occasion time. It is recommended that\n",
    "    you use find_date and find_float_time to generate necessary date and time columns for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'food_type' must exist within the data.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    std_daily_eating_midpoint\n",
    "        Float representation of the standard deviation of the daily midpoint eating occasion time. \n",
    "    \"\"\" \n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    \n",
    "    return df.groupby([date_col])[time_col].median().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.107072970435106"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "std_daily_eating_midpoint(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def logging_day_counts(df:pd.DataFrame) -> int:\n",
    "    \"\"\"\n",
    "    Calculates the number of days that contain any logs. It is recommended that\n",
    "    you use find_date to generate the necessary date column for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column for 'date' must exist within the data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    logging_day_counts\n",
    "        Number of days with at least one log on that day.\n",
    "    \"\"\"\n",
    "    date_col = df.columns[df.columns.get_loc('date')]\n",
    "    return df[date_col].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "logging_day_counts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def find_missing_logging_days(df:pd.DataFrame,\n",
    "                              start_date:datetime.date = \"not_defined\",\n",
    "                              end_date:datetime.date = \"not_defined\") -> list:\n",
    "    \"\"\"\n",
    "    Finds days that have no log entries between a start (inclusive) and end date (inclusive).\n",
    "    It is recommended that you use find_date to generate the necessary date column for this\n",
    "    function. \n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data.\n",
    "    start_date\n",
    "        Starting date for missing day evaluation. By default the earliest date in the data will be used.\n",
    "    end_date\n",
    "        Ending date for missing day evaluation. By default the latest date in the data will be used.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    missing_days\n",
    "        List of days within the given timeframe that have no log entries.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if start_date or end_date is missing, return nan\n",
    "    # intended behavior for participants when their study phase is ongoing\n",
    "    if pd.isnull(start_date) or pd.isnull(end_date):\n",
    "        return np.nan\n",
    "    \n",
    "    # if there is no input on start_date or end_date, use earliest date and latest date\n",
    "    if start_date == \"not_defined\":\n",
    "        start_date = df['date'].min()\n",
    "    if end_date == \"not_defined\":\n",
    "        end_date = df['date'].max()\n",
    "    \n",
    "    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "    missing_days = []\n",
    "    for x in pd.date_range(start_date, end_date, freq='d'):\n",
    "        if x.to_pydatetime().date() not in df['date'].unique():\n",
    "            missing_days.append(x.to_pydatetime().date())\n",
    "    \n",
    "    return missing_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrase 'not_defined' is the intended default value for start and end dates to signify that the earliest and/or latest date within the data should be used. If a participant is missing a valid start or end date, null is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2017, 12, 7),\n",
       " datetime.date(2017, 12, 9),\n",
       " datetime.date(2017, 12, 10)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "find_missing_logging_days(df, datetime.date(2017, 12, 7), datetime.date(2017, 12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7572733</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>411111</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>b</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8409118</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Salad</td>\n",
       "      <td>f</td>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.966667</td>\n",
       "      <td>00:58:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID      unique_code  research_info_id     desc_text food_type  \\\n",
       "0  7572733  alqt14018795225               150         Water         w   \n",
       "1   411111  alqt14018795225               150  Coffee White         b   \n",
       "2  8409118  alqt14018795225               150         Salad         f   \n",
       "\n",
       "           original_logtime        date  float_time      time  \\\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08   17.500000  17:30:00   \n",
       "1 2017-12-09 00:01:00+00:00  2017-12-08   24.016667  00:01:00   \n",
       "2 2017-12-09 00:58:00+00:00  2017-12-08   24.966667  00:58:00   \n",
       "\n",
       "   week_from_start  year  \n",
       "0                1  2017  \n",
       "1                1  2017  \n",
       "2                1  2017  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['date'].astype(str).str.contains(\"2017-12\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def good_lwa_day_counts(df: pd.DataFrame,\n",
    "                        window_start:datetime.time,\n",
    "                        window_end:datetime.time,\n",
    "                        min_log_num:int = 2,\n",
    "                        min_separation:int = 5,\n",
    "                        buffer_time:str = '15 minutes',\n",
    "                        h:int = 4,\n",
    "                        start_date:datetime.date = \"not_defined\",\n",
    "                        end_date:datetime.date = \"not_defined\",\n",
    "                        time_col:int = 7) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Calculates the number of 'good' logging days, 'good' window days, 'outside' window days and adherent days.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data.\n",
    "    window_start\n",
    "        Starting time for a time restriction window.\n",
    "    window_end\n",
    "        Ending time for a time restriction window.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    buffer_time\n",
    "        pd.Timedelta parsable string, representing 'wiggle room' for adherence.\n",
    "    h\n",
    "        Number of hours to shift the definition of 'date' by. h = 4 would indicate that a log date begins at\n",
    "        4:00 AM and ends the following calendar day at 3:59:59. Float representations of time would therefore\n",
    "        go from 4.0 (inclusive) to 28.0 (exclusive) to represent 'date' membership for days shifted from their\n",
    "        original calendar date.\n",
    "    start_date\n",
    "        Starting date for missing day evaluation. By default the earliest date in the data will be used.\n",
    "    end_date\n",
    "        Ending date for missing day evaluation. By default the latest date in the data will be used.\n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    rows, bad_dates\n",
    "        List containing number of 'good' logging days, 'good' window days, 'outside' window days, and adherent days.\n",
    "        List of three lists. The lists contains dates that are not considered 'good' logging days, 'good' window days,\n",
    "        or adherent days (in that order).\n",
    "    \"\"\"\n",
    "    # check for treets created float time column, otherwise default to expected column number\n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    # if start_date or end_Date is missing, return nan\n",
    "    # intended behavior for participants when their study phase is ongoing\n",
    "    if pd.isnull(start_date) or pd.isnull(end_date):\n",
    "        return [np.nan, np.nan, np.nan, np.nan], [[],[],[]]\n",
    "    \n",
    "    # if there is no input on start_date or end_date, use earliest date and latest date\n",
    "    if start_date == \"not_defined\":\n",
    "        start_date = df['date'].min()\n",
    "    if end_date == \"not_defined\":\n",
    "        end_date = df['date'].max()\n",
    "    \n",
    "    # if window start or window end are nan, make the windows the same as control's window time.\n",
    "    if pd.isnull(window_start):\n",
    "        window_start = datetime.time(0,0)\n",
    "    if pd.isnull(window_end):\n",
    "        window_end = datetime.time(23,59)\n",
    "\n",
    "    # helper function to determine a good logging\n",
    "    def good_logging(local_time_series):\n",
    "        return len(local_time_series.values) >= min_log_num and (max(local_time_series.values) - min(local_time_series.values)) >= min_separation\n",
    "   \n",
    "    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    df['original_logtime'] = pd.to_datetime(df['original_logtime']).dt.tz_localize(None)\n",
    "\n",
    "    buffer_time = pd.Timedelta(buffer_time).total_seconds()/3600.\n",
    "\n",
    "    in_window_count = []\n",
    "    daily_count = []\n",
    "    good_logging_count = []\n",
    "    cur_dates = df['date'].sort_values(ascending = True).unique()\n",
    "    for aday in cur_dates:\n",
    "        window_start_daily = window_start.hour + window_start.minute / 60 - buffer_time\n",
    "        window_end_daily = window_end.hour + window_end.minute / 60 + buffer_time\n",
    "        tmp = df[df['date'] == aday]\n",
    "        \n",
    "        if (window_start == datetime.time(0,0)) and (window_end == datetime.time(23,59)):\n",
    "            in_window_count.append(tmp[(tmp[time_col] >= window_start_daily + h) & (tmp[time_col] <= window_end_daily + h)].shape[0])\n",
    "        else:\n",
    "            in_window_count.append(tmp[(tmp[time_col] >= window_start_daily) & (tmp[time_col] <= window_end_daily)].shape[0])\n",
    "        \n",
    "        daily_count.append(df[df['date'] == aday].shape[0])\n",
    "        good_logging_count.append(good_logging(df[df['date'] == aday][time_col]))\n",
    "\n",
    "    in_window_count = np.array(in_window_count)\n",
    "    daily_count = np.array(daily_count)\n",
    "    good_logging_count = np.array(good_logging_count)\n",
    "    good_logging_by_date = [cur_dates[i] for i, x in enumerate(good_logging_count) if not x]\n",
    "\n",
    "    good_window_days = (in_window_count==daily_count)\n",
    "    good_window_day_counts = good_window_days.sum()\n",
    "    good_window_by_date = [cur_dates[i] for i, x in enumerate(good_window_days) if not x]\n",
    "    \n",
    "    outside_window_days = in_window_count.size - good_window_days.sum()\n",
    "    good_logging_days = good_logging_count.sum()\n",
    "    if good_logging_count.size == 0:\n",
    "        adherent_day_counts = 0\n",
    "        adherent_days_by_date = []\n",
    "    else:\n",
    "        adherent_days = (good_logging_count & (in_window_count == daily_count))\n",
    "        adherent_days_by_date = [cur_dates[i] for i, x in enumerate(adherent_days) if not x]\n",
    "        adherent_day_counts = adherent_days.sum()\n",
    "    \n",
    "    rows = [good_logging_days, good_window_day_counts, outside_window_days, adherent_day_counts]\n",
    "    bad_dates = [good_logging_by_date, good_window_by_date, adherent_days_by_date]\n",
    "\n",
    "    return rows, bad_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main use of this function is to calculate window and logging adherence. These are represented as 'good' (valid)\n",
    "logging days, 'good' window days, 'outside' (invalid) window days, and adherent days.\n",
    "\n",
    "The definition of each is:\n",
    "\n",
    "1. 'Good' Logging Day\n",
    "\n",
    "    - A day with at least a specified minimum number of caloric (food or beverage) logs with a minimum specified number of hours between the first and last log for that day.\n",
    "    \n",
    "1. 'Good' Window Day\n",
    "    - A day where all food loggings are within the participant's assigned eating restriction window plus any wiggle room, if allowed.\n",
    "        \n",
    "1. Adherent Day\n",
    "\n",
    "    - A day that is both a 'good' logging day and a 'good' window day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "dates, bad_dates = good_lwa_day_counts(df, datetime.time(8,0,0), datetime.time(23,59,59))\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second product of this function is three lists that outline which days are not compliant with one of the definitions above. The first list (index 0) consists of dates that are not 'good' logging days, the second contains days that are not 'good' window days. The final list consists of dates that are not adherent (neither 'good' window nor 'good' logging dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dates[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Design\n",
    "\n",
    "This group of functions provides methods for filtering participant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filtering_usable_data(df:pd.DataFrame,\n",
    "                          num_items:int,\n",
    "                          num_days:int,\n",
    "                          identifier:int = 1,\n",
    "                          date_col:int = 6) -> tuple[pd.DataFrame, set]:\n",
    "    \"\"\"\n",
    "    Filters data for only participants who's data satisfies the minimum number of days and logs.\n",
    "    It is recommended that you use find_date to generate the necessary date column for this\n",
    "    function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df\n",
    "        Dataframe of food logging data. A column 'desc_text', typically found in mCC data\n",
    "        is required.\n",
    "    num_items\n",
    "        Minimum number of logs required to pass filter criteria.\n",
    "    num_days\n",
    "        Minimum number of unique logging days required to pass filter criteria.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_usable, set_usable\n",
    "        Data filtered to only include data from participants that have passed filtering criteria.\n",
    "        Set of participants that passed filtering criteria.\n",
    "    \"\"\"\n",
    "    print(' => filtering_usable_data()')\n",
    "    print('  => using the following criteria:', num_items, 'items and', num_days, 'days.')\n",
    "    \n",
    "    identifier = df.columns[1]\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "\n",
    "    # Item logged\n",
    "    log_item_count = df.groupby(identifier).agg('count')[['desc_text']].rename(columns = {'desc_text': 'Total Logged'})\n",
    "\n",
    "    # Day counts\n",
    "    log_days_count = df[[identifier, date_col]]\\\n",
    "        .drop_duplicates().groupby(identifier).agg('count').rename(columns = {date_col: 'Day Count'})\n",
    "\n",
    "    item_count_passed = set(log_item_count[log_item_count['Total Logged'] >= num_items].index)\n",
    "    day_count_passed = set(log_days_count[log_days_count['Day Count'] >= num_days].index)\n",
    "\n",
    "    print('  => # of users pass the criteria:', end = ' ')\n",
    "    print(len(item_count_passed & day_count_passed))\n",
    "    \n",
    "    passed_participant_set = item_count_passed & day_count_passed\n",
    "    df_usable = df.loc[df.unique_code.apply(lambda c: c in passed_participant_set)]\\\n",
    "        .copy().reset_index(drop = True)\n",
    "    \n",
    "    return df_usable, set(df_usable.unique_code.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_loader('data/output/public.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_usable_data(df, num_items = 1000, num_days = 14)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepare_baseline_and_intervention_usable_data(data_source:str|pd.DataFrame,\n",
    "                                                  baseline_num_items:int,\n",
    "                                                  baseline_num_days:int,\n",
    "                                                  intervention_num_items:int,\n",
    "                                                  intervention_num_days:int,\n",
    "                                                  identifier:int = 1,\n",
    "                                                  date_col:int = 6) -> list:\n",
    "    \"\"\"\n",
    "    Filters data for 'usable' data within baseline and last two weeks of intervention (weeks 13 and 14).\n",
    "    It is recommended that you use the function 'week_from_start' to generate the necessary week column\n",
    "    for this function.\n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is.\n",
    "    baseline_num_items\n",
    "        Number of logs for a participant's baseline data to pass filter criteria.\n",
    "    baseline_num_days\n",
    "        Number of unique logging days for a participant's baseline data to pass filter criteria.\n",
    "    intervention_num_items\n",
    "        Number of logs for a participant's intervention data to pass filter criteria.\n",
    "    intervention_num_days\n",
    "        Number of unique logging days for a participant's intervention data to pass filter criteria.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dfs\n",
    "        List of two dataframes: usable baseline data, usable intervention data.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    food_all = file_loader(data_source)\n",
    "    \n",
    "    # create baseline data\n",
    "    df_food_baseline = food_all.query('week_from_start <= 2')\n",
    "    df_food_baseline_usable, food_baseline_usable_id_set = \\\n",
    "    filtering_usable_data(df_food_baseline, num_items = baseline_num_items, num_days = baseline_num_days, identifier = identifier, date_col = date_col)\n",
    "    \n",
    "    # create intervention data\n",
    "    df_food_intervention = food_all.query('week_from_start in [13, 14]')\n",
    "    df_food_intervention_usable, food_intervention_usable_id_set = \\\n",
    "    filtering_usable_data(df_food_intervention, num_items = intervention_num_items, num_days = intervention_num_days, identifier = identifier, date_col = date_col)\n",
    "    \n",
    "    # create df that contains both baseline and intervention id_set that contains data for the first two weeks\n",
    "    expanded_baseline_usable_id_set = set(list(food_baseline_usable_id_set) + list(food_intervention_usable_id_set))\n",
    "    df_food_basline_usable_expanded = food_all.loc[food_all.apply(lambda s: s.week_from_start <= 2 \\\n",
    "                                                    and s.unique_code in expanded_baseline_usable_id_set, axis = 1)]\n",
    "        \n",
    "    return [df_food_basline_usable_expanded, df_food_intervention_usable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= prepare_baseline_and_intervention_usable_data('data/output/public.json', 20, 10, 40, 12)[0]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Data Summaries\n",
    "\n",
    "Data analysis and summary functions, including summary functions for specific statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def users_sorted_by_logging(data_source:str|pd.DataFrame,\n",
    "                            food_type:list = [\"f\", \"b\", \"m\", \"w\"],\n",
    "                            min_log_num:int = 2,\n",
    "                            min_separation:int = 4,\n",
    "                            identifier:int = 1,\n",
    "                            date_col:int = 6,\n",
    "                            time_col:int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reports the number of 'good' logging days for each user, in descending order based on number of 'good' logging days.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. \n",
    "    food_type\n",
    "        A single food type, or list of food types. Valid types are 'f': food, 'b': beverage,\n",
    "        'w': water, and 'm': medication.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    food_top_users_day_counts\n",
    "        Dataframe containing the number of good logging days for each user.    \n",
    "    \"\"\"\n",
    "\n",
    "    food_all = file_loader(data_source)\n",
    "    # filter the dataframe so it only contains input food type\n",
    "    \n",
    "    filtered_users = food_all.query('food_type in @food_type')\n",
    "    filtered_users['in_good_logging_day'] = in_good_logging_day(filtered_users, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    \n",
    "    identifier = df.columns[1]\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    \n",
    "    food_top_users_day_counts = pd.DataFrame(filtered_users.query('in_good_logging_day == True')\\\n",
    "                            [[date_col, identifier]].groupby(identifier)[date_col].nunique())\\\n",
    "                            .sort_values(by = date_col, ascending = False).rename(columns = {date_col: 'day_count'})\n",
    "\n",
    "    \n",
    "    return food_top_users_day_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_sorted_by_logging('data/output/public.json', ['f','b']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eating_intervals_percentile(data_source:str|pd.DataFrame,\n",
    "                                identifier:int = 1,\n",
    "                                time_col:int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the 2.5, 5, 10, 12.5, 25, 50, 75, 87.5, 90, 95, and 97.5 percentile eating time for each participant.\n",
    "    It also calculates the middle 95, 90, 80, 75, and 50 percentile eating windows for each participant. It is\n",
    "    recommended that you use find_float_time to generate necessary the time column for this function. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. \n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ptile\n",
    "        Dataframe with count, mean, std, min, quantiles and mid XX%tile eating window durations for all participants.\n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    df = df[df[\"food_type\"].isin([\"f\", \"b\"])]\n",
    "    identifier = df.columns[1]\n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(np.array([[np.nan]*21]), columns = ['count', 'mean', 'std', 'min', '2.5%', '5%', '10%', '12.5%', '25%',\n",
    "       '50%', '75%', '87.5%', '90%', '95%', '97.5%', 'max', 'duration mid 95%',\n",
    "       'duration mid 90%', 'duration mid 80%', 'duration mid 75%',\n",
    "       'duration mid 50%'])\n",
    "    \n",
    "    ptile = df.groupby(identifier)[time_col].describe(percentiles=[.025, .05, .10, .125, .25, .5, .75, .875, .9, .95, .975])\n",
    "    ll = ['2.5%','5%','10%','12.5%','25%']\n",
    "    ul = ['97.5%','95%', '90%','87.5%', '75%']\n",
    "    mp = ['duration mid 95%', 'duration mid 90%', 'duration mid 80%', 'duration mid 75%','duration mid 50%']\n",
    "    for low, upp, midp in zip(ll,ul,mp):\n",
    "        ptile[midp] = ptile[upp] - ptile[low]\n",
    "        \n",
    "    return ptile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "eating_intervals_percentile(df).iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def first_cal_analysis_summary(data_source:str|pd.DataFrame,\n",
    "                               min_log_num:int = 2,\n",
    "                               min_separation:int = 4,\n",
    "                               identifier:int = 1,\n",
    "                               date_col:int = 6,\n",
    "                               time_col:int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the 5, 10, 25 , 50, 75, 90, 95 percentile of first caloric entry time for each participant on\n",
    "    'good' logging days. It is recommended that you use find_date and find_float_time to generate necessary date and\n",
    "    time columns for this function.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. \n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    first_cal_summary_df\n",
    "        Dataframe with 5, 10, 25, 50, 75, 90, 95 percentile of first caloric entry time for all participants.\n",
    "    \"\"\"\n",
    "\n",
    "    df = file_loader(data_source)\n",
    "    \n",
    "    # leave only the loggings in a good logging day\n",
    "    df = df[df[\"food_type\"].isin([\"f\", \"b\"])]\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    first_cal_series = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    first_cal_df = pd.DataFrame(first_cal_series)\n",
    "    all_rows = []\n",
    "    for index in first_cal_df.index:\n",
    "        tmp_dict = dict(first_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    return first_cal_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_analysis_summary('data/output/baseline.json').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def last_cal_analysis_summary(data_source:str|pd.DataFrame,\n",
    "                              min_log_num:int = 2,\n",
    "                              min_separation:int = 4,\n",
    "                              identifier:int = 1,\n",
    "                              date_col:int = 6,\n",
    "                              time_col:int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the 5, 10, 25 , 50, 75, 90, 95 percentile of last caloric entry time for each participant on\n",
    "    'good' logging days. It is recommended that you use find_date and find_float_time to generate necessary date and\n",
    "    time columns for this function.\n",
    "\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. \n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    last_cal_summary_df\n",
    "        Dataframe with 5, 10, 25, 50, 75, 90, 95 percentile of last caloric entry time for all participants.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_source)\n",
    "    \n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df = df[df[\"food_type\"].isin([\"f\", \"b\"])]\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    last_cal_series = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    last_cal_df = pd.DataFrame(last_cal_series)\n",
    "    all_rows = []\n",
    "    for index in last_cal_df.index:\n",
    "        tmp_dict = dict(last_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "    \n",
    "    return last_cal_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_analysis_summary('data/output/baseline.json').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarize_data(data_source:str|pd.DataFrame,\n",
    "                   min_log_num:int = 2,\n",
    "                   min_separation:int = 4,\n",
    "                   identifier:int = 1,\n",
    "                   date_col:int = 6,\n",
    "                   time_col:int = 7) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarizes participant data, including number of days, total number of logs, number of food/beverage logs,\n",
    "    number of medication logs, number of water logs, eating window duration information, first and last caloric log\n",
    "    information, and adherence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column (with indexing starting from 0).\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "   \n",
    "    Returns\n",
    "    -------    \n",
    "    summary\n",
    "        Summary dataframe.\n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    \n",
    "    # first_cal variation (90%-10%)\n",
    "    first_cal_variability = first_cal_analysis_summary(df, min_log_num, min_separation, identifier, date_col, time_col).set_index('id')\n",
    "    for col in first_cal_variability.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        first_cal_variability[col] = first_cal_variability[col] - first_cal_variability['50%']\n",
    "    first_cal_ser = first_cal_variability['90%'] - first_cal_variability['10%']\n",
    "\n",
    "    # last_cal variation (90%-10%)\n",
    "    last_cal_variability = last_cal_analysis_summary(df,min_log_num, min_separation, identifier, date_col, time_col).set_index('id')\n",
    "    for col in last_cal_variability.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        last_cal_variability[col] = last_cal_variability[col] - last_cal_variability['50%']\n",
    "    last_cal_ser = last_cal_variability['90%'] - last_cal_variability['10%']\n",
    "    \n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    \n",
    "    # num_days\n",
    "    num_days = df.groupby(identifier).date.nunique()\n",
    "\n",
    "    # num_total_items\n",
    "    num_total_items = df.groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    # num_f_n_b\n",
    "    num_f_n_b = get_types(df, ['f','b']).groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    # num_medications\n",
    "    num_medications = get_types(df, ['m']).groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    # num_water\n",
    "    num_water = get_types(df, ['w']).groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    # duration_mid_95, start_95, end_95\n",
    "    eating_intervals = eating_intervals_percentile(df, time_col, identifier)[['2.5%','95%','duration mid 95%']]\n",
    "\n",
    "    # first_cal_avg\n",
    "    first_cal_avg = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).mean()\n",
    "\n",
    "    # first_cal_std\n",
    "    first_cal_std = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).std()\n",
    "\n",
    "    # last_cal_avg\n",
    "    last_cal_avg = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).mean()\n",
    "\n",
    "    # last_cal_std\n",
    "    last_cal_std = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).std()\n",
    "\n",
    "    # eating_win_avg\n",
    "    eating_win_avg = last_cal_avg - first_cal_avg\n",
    "\n",
    "    # eating_win_std\n",
    "    eating_win_std = (df.groupby([identifier, date_col])[time_col].max()-\n",
    "                          df.groupby([identifier, date_col])[time_col].min()).groupby(identifier).std()\n",
    "    \n",
    "    # good_logging_count\n",
    "    good_logging_count = df.groupby(identifier)['in_good_logging_day'].sum()\n",
    "\n",
    "\n",
    "    summary = pd.concat([num_days, num_total_items, num_f_n_b, num_medications, num_water, first_cal_avg, first_cal_std, last_cal_avg, last_cal_std, eating_win_avg, eating_win_std, good_logging_count, first_cal_ser, last_cal_ser], axis=1).reset_index()\n",
    "    summary.columns = [identifier,'num_days', 'num_total_items', 'num_f_n_b', 'num_medications', 'num_water', 'first_cal_avg', 'first_cal_std', 'last_cal_avg', 'last_cal_std', 'eating_win_avg', 'eating_win_std', 'good_logging_count', 'first_cal variation (90%-10%)', 'last_cal variation (90%-10%)']\n",
    "    summary = summary.merge(eating_intervals, on = identifier, how='left').fillna(0)\n",
    "    \n",
    "    summary['num_medications'] = summary['num_medications'].astype('int')\n",
    "    summary['num_water'] = summary['num_water'].astype('int')\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function provides summary data for an entire study, without separating for study phases. Summaries include statistics for first and last caloric log, eating window, and relevant calculations for middle 95 percentile eating window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', h = 4)\n",
    "summarize_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def summarize_data_with_experiment_phases(food_data:pd.DataFrame,\n",
    "                                          ref_tbl:pd.DataFrame,\n",
    "                                          min_log_num:int = 2,\n",
    "                                          min_separation:int = 5,\n",
    "                                          buffer_time:str = '15 minutes',\n",
    "                                          h:int = 4,\n",
    "                                          report_level:int = 2,\n",
    "                                          txt:bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarizes participant data for each experiment phase and eating window assignment. Summary includes number of days,\n",
    "    total number of logs, number of food/beverage logs, number of medication logs, number of water logs,\n",
    "    eating window duration information, first and last caloric log information, and adherence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    food_data\n",
    "        Dataframe of food logging data. A column for \"original_logtime\" must exist within the data. mCC output style\n",
    "        data is expected.\n",
    "    ref_tbl\n",
    "        Participant data reference table. See the accompanying HOWTO document for required column positions and\n",
    "        formatting.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    buffer_time\n",
    "        pd.Timedelta parsable string, representing 'wiggle room' for adherence.\n",
    "    h\n",
    "        Number of hours to shift the definition of 'date' by. h = 4 would indicate that a log date begins at\n",
    "        4:00 AM and ends the following calendar day at 3:59:59. Float representations of time would therefore\n",
    "        go from 4.0 (inclusive) to 28.0 (exclusive) to represent 'date' membership for days shifted from their\n",
    "        original calendar date.\n",
    "    report_level\n",
    "        Additional printed info detail level. 0 = No Report. 1 = Report 'No Logging Days'. \n",
    "        2 = Report 'No Logging Days', 'Bad Logging Days', 'Bad Window Days', and 'Non-Adherent Days'.\n",
    "    txt\n",
    "        If True, a text format (.txt) report will be saved in the current directory, with the name\n",
    "        'treets_warning_dates.txt'\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df\n",
    "        Summary dataframe, where each row represents the summary for a participant during a particular\n",
    "        study phase. Participants can have multiple rows for a single study phase if, during that study phase,\n",
    "        their assigned eating window is altered.\n",
    "    \"\"\"\n",
    "    df = food_data.copy()\n",
    "    \n",
    "    mcc_id = ref_tbl.columns[0]\n",
    "    start_day = ref_tbl.columns[4]\n",
    "    end_day = ref_tbl.columns[5]\n",
    "    window_start = ref_tbl.columns[6]\n",
    "    window_end = ref_tbl.columns[7]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # checking for necessary typecasting for reference table\n",
    "    if not ref_tbl[start_day].apply(lambda x: isinstance(x, datetime.date)).all():\n",
    "        ref_tbl[start_day] = pd.to_datetime(ref_tbl[start_day]).dt.date\n",
    "        \n",
    "    if not ref_tbl[end_day].apply(lambda x: isinstance(x, datetime.date)).all():\n",
    "        ref_tbl[end_day] = pd.to_datetime(ref_tbl[end_day]).dt.date\n",
    "    \n",
    "    \n",
    "    # preprocess to get the date and float_time column\n",
    "    df['original_logtime'] = pd.to_datetime(df['original_logtime'])\n",
    "    df['date'] =  find_date(df, h, df.columns.get_loc('original_logtime'))\n",
    "    df['float_time'] =  find_float_time(df, h, df.columns.get_loc('original_logtime'))\n",
    "    \n",
    "    # get study phase duration\n",
    "    result = find_phase_duration(ref_tbl)\n",
    "    \n",
    "    # reset the index of ref_tbl to avoid issues during concatenation\n",
    "    ref_tbl = ref_tbl.reset_index(drop=True)\n",
    "    \n",
    "    # loop through each row and get 'caloric_entries', 'mean_daily_eating_window', 'std_daily_eating_window', 'eariliest_entry', 'logging_day_counts',\n",
    "    # and 'good_logging_days', 'good_window_days', 'outside_window_days' and 'adherent_days' and find missing dates\n",
    "    matrix = []\n",
    "    missing_dates = {}\n",
    "    bad_dates_dic = {}\n",
    "\n",
    "    for index, row in ref_tbl.iterrows():\n",
    "        id_ = row[mcc_id]\n",
    "        rows = []\n",
    "        temp_df = df[df[\"PID\"] == id_]\n",
    "        temp_df = temp_df[(temp_df['date'] >= row[start_day]) & (temp_df['date'] <= row[end_day])]\n",
    "        # num of caloric entries\n",
    "        rows.append(count_caloric_entries(temp_df))\n",
    "        # num of medication\n",
    "        rows.append(get_types(temp_df, ['m']).shape[0])\n",
    "        # num of water\n",
    "        rows.append(get_types(temp_df, ['w']).shape[0])\n",
    "        # first cal average\n",
    "        rows.append(mean_first_cal(temp_df,'date', 'float_time'))\n",
    "        #first cal std\n",
    "        rows.append(std_first_cal(temp_df, 'date', 'float_time'))\n",
    "        # last cal average\n",
    "        rows.append(mean_last_cal(temp_df,'date', 'float_time'))\n",
    "        # last cal std\n",
    "        rows.append(std_last_cal(temp_df, 'date', 'float_time'))\n",
    "        # mean eating window\n",
    "        rows.append(mean_daily_eating_duration(temp_df,'date','float_time'))\n",
    "        \n",
    "        rows.append(std_daily_eating_duration(temp_df,'date','float_time'))\n",
    "        rows.append(earliest_entry(temp_df))\n",
    "        \n",
    "        rows.append(mean_daily_eating_occasions(temp_df, 'date', 'float_time'))\n",
    "        rows.append(std_daily_eating_occasions(temp_df, 'date', 'float_time'))\n",
    "        rows.append(mean_daily_eating_midpoint(temp_df, 'date', 'float_time'))\n",
    "        rows.append(std_daily_eating_midpoint(temp_df, 'date', 'float_time'))\n",
    "        \n",
    "        rows.append(logging_day_counts(temp_df))\n",
    "        row_day_num, bad_dates = good_lwa_day_counts(df[df[\"PID\"]==id_]\n",
    "                                           , window_start=row[window_start]\n",
    "                                           , window_end = row[window_end]\n",
    "                                           , min_log_num=min_log_num\n",
    "                                           , min_separation=min_separation\n",
    "                                           , buffer_time= buffer_time\n",
    "                                           , start_date=row[start_day]\n",
    "                                           , end_date=row[start_day]\n",
    "                                            , h=h)\n",
    "        for x in row_day_num:\n",
    "            rows.append(x)\n",
    "        bad_logging = bad_dates[0]\n",
    "        bad_window = bad_dates[1]\n",
    "        non_adherent = bad_dates[2]\n",
    "\n",
    "        if '{}_bad_logging'.format(id_) not in bad_dates_dic:\n",
    "            bad_dates_dic['{}_bad_logging'.format(id_)]=bad_logging\n",
    "            bad_dates_dic['{}_bad_window'.format(id_)]=bad_window\n",
    "            bad_dates_dic['{}_non_adherent'.format(id_)]=non_adherent\n",
    "        else:\n",
    "            bad_dates_dic['{}_bad_logging'.format(id_)]+=bad_logging\n",
    "            bad_dates_dic['{}_bad_window'.format(id_)]+=bad_window\n",
    "            bad_dates_dic['{}_non_adherent'.format(id_)]+=non_adherent\n",
    "                \n",
    "        matrix.append(rows)\n",
    "        date_lst = find_missing_logging_days(df[df[\"PID\"]==id_], row[start_day],row[end_day])\n",
    "        # only consider when the result is not nan\n",
    "        if isinstance(date_lst, list)==True:\n",
    "            if id_ in missing_dates:\n",
    "                missing_dates[id_] += date_lst\n",
    "            else:\n",
    "                missing_dates[id_] = date_lst\n",
    "\n",
    "    # create a temp dataframe\n",
    "    tmp = pd.DataFrame(matrix, columns = ['caloric_entries_num','medication_num', 'water_num','first_cal_avg','first_cal_std',\n",
    "                                          'last_cal_avg', 'last_cal_std', 'mean_daily_eating_window', 'std_daily_eating_window', 'earliest_entry',\n",
    "                                          'mean_daily_eating_occasions', 'std_daily_eating_occasions', 'mean_daily_eating_midpoint', 'std_daily_eating_midpoint',\n",
    "                                          'logging_day_counts','good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days'])\n",
    "    \n",
    "    # concat these two tables\n",
    "    returned = pd.concat([ref_tbl, tmp], axis=1)\n",
    "    \n",
    "    # loop through each row and get 2.5%, 97.5%, duration mid 95% column\n",
    "    column_025 = []\n",
    "    column_975 = []\n",
    "    for index, row in ref_tbl.iterrows():\n",
    "        id_ = row[mcc_id]\n",
    "        temp_df = df[df[\"PID\"] == id_]\n",
    "        temp_df = temp_df[(temp_df['date'] >= row[start_day]) & (temp_df['date'] <= row[end_day])]\n",
    "        series = eating_intervals_percentile(temp_df, 'float_time', \"PID\")\n",
    "        try:\n",
    "            column_025.append(series.iloc[0]['2.5%'])\n",
    "        except:\n",
    "            column_025.append(np.nan)\n",
    "        try:\n",
    "            column_975.append(series.iloc[0]['97.5%'])\n",
    "        except:\n",
    "            column_975.append(np.nan)\n",
    "    returned['2.5%'] = column_025\n",
    "    returned['97.5%'] = column_975\n",
    "    returned['duration mid 95%'] = returned['97.5%'] - returned['2.5%']\n",
    "    \n",
    "    def convert_to_percentage(ser, col):\n",
    "        if pd.isnull(ser[col]):\n",
    "            return ser[col]\n",
    "        else:\n",
    "            return (round(ser[col]/ser['phase_duration'].days * 100, 2))\n",
    "    \n",
    "    # calculate percentage for \n",
    "    for x in returned.columns:\n",
    "        if x in ['logging_day_counts','good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days']:\n",
    "            returned['%_'+x] = returned.apply(convert_to_percentage, col = x, axis = 1)\n",
    "\n",
    "    # reorder the columns\n",
    "    returned = returned[['mCC_ID', 'Participant_Study_ID', 'Study Phase',\n",
    "       'Intervention group (TRE or HABIT)', 'Start_Day', 'End_day',\n",
    "       'Eating_Window_Start','Eating_Window_End', 'phase_duration',\n",
    "       'caloric_entries_num','medication_num', 'water_num','first_cal_avg',\n",
    "        'first_cal_std','last_cal_avg', 'last_cal_std', \n",
    "        'mean_daily_eating_window', 'std_daily_eating_window',\n",
    "       'earliest_entry', 'mean_daily_eating_occasions', 'std_daily_eating_occasions',\n",
    "        'mean_daily_eating_midpoint', 'std_daily_eating_midpoint', '2.5%', '97.5%', 'duration mid 95%',\n",
    "       'logging_day_counts', '%_logging_day_counts', 'good_logging_days',\n",
    "        '%_good_logging_days','good_window_days', '%_good_window_days', \n",
    "        'outside_window_days','%_outside_window_days', 'adherent_days',\n",
    "       '%_adherent_days']]    \n",
    "    \n",
    "    if report_level == 0:\n",
    "        return returned\n",
    "    \n",
    "    if txt:\n",
    "        with open('treets_warning_dates.txt', 'w') as f:\n",
    "            # print out missing dates with participant's id\n",
    "            for x in missing_dates:\n",
    "                if len(missing_dates[x])>0:\n",
    "                    f.write(\"Participant {} didn't log any food items in the following day(s):\\n\".format(x))\n",
    "                    print(\"Participant {} didn't log any food items in the following day(s):\".format(x))\n",
    "                    for date in missing_dates[x]:\n",
    "                        f.write(str(date)+'\\n')\n",
    "                        print(date)\n",
    "    else:\n",
    "        for x in missing_dates:\n",
    "            if len(missing_dates[x])>0:\n",
    "                print(\"Participant {} didn't log any food items in the following day(s):\".format(x))\n",
    "                for date in missing_dates[x]:\n",
    "                    print(date)\n",
    "                \n",
    "    if report_level == 1:\n",
    "        return returned\n",
    "    \n",
    "    if txt:\n",
    "        with open('treets_warning_dates.txt', 'a') as f:\n",
    "            # print out bad logging, bad window and non-adherent dates with participant's id\n",
    "            for x in bad_dates_dic:\n",
    "                if len(bad_dates_dic[x])>0:\n",
    "                    strings = x.split('_')\n",
    "                    f.write(\"Participant {} have {} day(s) in the following day(s):\\n\".format(strings[0], strings[1]+' '+strings[2]))\n",
    "                    print(\"Participant {} have {} day(s) in the following day(s):\".format(strings[0], strings[1]+' '+strings[2]))\n",
    "                    for date in bad_dates_dic[x]:\n",
    "                        f.write(str(date)+'\\n')\n",
    "                        print(date)\n",
    "    else:\n",
    "        for x in bad_dates_dic:\n",
    "            if len(bad_dates_dic[x])>0:\n",
    "                strings = x.split('_')\n",
    "                print(\"Participant {} have {} day(s) in the following day(s):\".format(strings[0], strings[1]+' '+strings[2]))\n",
    "                for date in bad_dates_dic[x]:\n",
    "                    print(date)\n",
    "    \n",
    "    return returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = summarize_data_with_experiment_phases(pd.read_csv('data/col_test_data/toy_data_2000.csv')\\\n",
    "                      , pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'), report_level = 2)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Plotting functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def first_cal_mean_with_error_bar(data_source:str|pd.DataFrame,\n",
    "                                  min_log_num:int = 2,\n",
    "                                  min_separation:int = 4,\n",
    "                                  identifier:int = 1,\n",
    "                                  date_col:int = 6,\n",
    "                                  time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Represents mean and standard deviation of first caloric intake time for each participant\n",
    "    as a scatter plot, with participants as the x-axis and time as the y-axis.\n",
    "    It is recommended that you use find_date and find_float_time to generate necessary date and\n",
    "    time columns for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object.\n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    \n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df = df[df[\"food_type\"].isin([\"f\", \"b\"])]\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "\n",
    "    first_cal_series = df.groupby([identifier, date_col])[time_col].min()\n",
    "    \n",
    "    \n",
    "    # find means and stds for each person\n",
    "    means = first_cal_series.groupby(identifier).mean().to_frame().rename(columns={'local_time':'mean'})\n",
    "    stds = first_cal_series.groupby(identifier).std().fillna(0).to_frame().rename(columns={'local_time':'std'})\n",
    "    \n",
    "    if means.shape[0] > 50:\n",
    "        print(\"More than 50 people are present which might make the graph look messy\")\n",
    "    \n",
    "    temp = pd.concat([means,stds], axis=1)\n",
    "    temp.sort_values('mean', inplace=True)\n",
    "    \n",
    "    # plot scatter plot with error bars\n",
    "    plt.scatter(range(temp.shape[0]),temp['mean'])\n",
    "    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt=\"o\")\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Time (Hour)\")\n",
    "    plt.title('Mean First Caloric Intake Time per Person')\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_mean_fig = first_cal_mean_with_error_bar('data/output/baseline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def last_cal_mean_with_error_bar(data_source:str|pd.DataFrame,\n",
    "                                 min_log_num:int = 2,\n",
    "                                 min_separation:int = 4,\n",
    "                                 identifier:int = 1,\n",
    "                                 date_col:int = 6,\n",
    "                                 time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Represents mean and standard deviation of last caloric intake time for each participant\n",
    "    as a scatter plot, with the x-axis as participants and the y-axis as time.\n",
    "    It is recommended that you use find_date and find_float_time to generate necessary date and\n",
    "    time columns for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object.\n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "\n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df = df[df[\"food_type\"].isin([\"f\", \"b\"])]\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "\n",
    "    first_cal_series = df.groupby([identifier, date_col])[time_col].min()\n",
    "    \n",
    "\n",
    "    last_cal_series = df.groupby([identifier, date_col])[time_col].max()\n",
    "    \n",
    "    \n",
    "    # find means and stds for each person\n",
    "    means = last_cal_series.groupby(identifier).mean().to_frame().rename(columns={'local_time':'mean'})\n",
    "    stds = last_cal_series.groupby(identifier).std().fillna(0).to_frame().rename(columns={'local_time':'std'})\n",
    "    \n",
    "    if means.shape[0] > 50:\n",
    "        print(\"More than 50 people are present which might make the graph look messy\")\n",
    "    \n",
    "    temp = pd.concat([means,stds], axis=1)\n",
    "    temp.sort_values('mean', inplace=True)\n",
    "    \n",
    "    # plot scatter plot with error bars\n",
    "    plt.scatter(range(temp.shape[0]),temp['mean'])\n",
    "    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt=\"o\")\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Time (Hour)\")\n",
    "    plt.title('Mean Last Caloric Intake Time per Person')\n",
    "    \n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_mean_fig = last_cal_mean_with_error_bar('data/output/baseline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def first_cal_analysis_variability_plot(data_source:str|pd.DataFrame,\n",
    "                                        min_log_num:int = 2,\n",
    "                                        min_separation:int = 4,\n",
    "                                        identifier:int = 1,\n",
    "                                        date_col:int = 6,\n",
    "                                        time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Calculates first caloric log time variability for 'good' logging days by subtracting 5, 10, 25, 50, 75, 90, 95\n",
    "    percentile of first caloric intake time from the 50th percentile first caloric intake time.\n",
    "    It also produces a histogram that represents the 90%-10% interval for all participants. It is recommended\n",
    "    that you use find_date and find_float_time to generate necessary date and time columns for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object.    \n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "        \n",
    "    # leave only the loggings in a good logging day\n",
    "    df = df[df[\"food_type\"].isin([\"f\", \"b\"])]\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    first_cal_series = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    first_cal_df = pd.DataFrame(first_cal_series)\n",
    "    all_rows = []\n",
    "    for index in first_cal_df.index:\n",
    "        tmp_dict = dict(first_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "    first_cal_variability_df = first_cal_summary_df.copy()\n",
    "    \n",
    "    for col in first_cal_variability_df.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        first_cal_variability_df[col] = first_cal_variability_df[col] - first_cal_variability_df['50%']\n",
    "    first_cal_variability_df['50%'] = first_cal_variability_df['50%'] - first_cal_variability_df['50%']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns_plot = sns.distplot( first_cal_variability_df['90%'] - first_cal_variability_df['10%'] )\n",
    "    ax.set(xlabel='Variation of First Caloric Intake Time (90% - 10%)', ylabel='Kernel Density Estimation')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_var_plot = first_cal_analysis_variability_plot('data/output/baseline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def last_cal_analysis_variability_plot(data_source:str|pd.DataFrame,\n",
    "                                       min_log_num:int = 2,\n",
    "                                       min_separation:int = 4,\n",
    "                                       identifier:int = 1,\n",
    "                                       date_col:int = 6,\n",
    "                                       time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Calculates last caloric log time variability for 'good' logging days by subtracting 5, 10, 25, 50, 75, 90, 95\n",
    "    percentile of last caloric intake time from the 50th percentile last caloric intake time.\n",
    "    It also produces a histogram that represents the 90%-10% interval for all participants. It is recommended\n",
    "    that you use find_date and find_float_time to generate necessary date and time columns for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    min_log_num\n",
    "        Minimum number of logs required for a day to be considered a 'good' logging day.\n",
    "    min_separation\n",
    "        Minimum number of hours between first and last log on a log day for it to be considered a 'good' logging day.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object. \n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_source)\n",
    "        \n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df = df[df[\"food_type\"].isin([\"f\", \"b\"])]\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, min_log_num, min_separation, identifier, date_col, time_col)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    last_cal_series = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    last_cal_df = pd.DataFrame(last_cal_series)\n",
    "    all_rows = []\n",
    "    for index in last_cal_df.index:\n",
    "        tmp_dict = dict(last_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "    last_cal_variability_df = last_cal_summary_df.copy()\n",
    "    \n",
    "    for col in last_cal_variability_df.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        last_cal_variability_df[col] = last_cal_variability_df[col] - last_cal_variability_df['50%']\n",
    "    last_cal_variability_df['50%'] = last_cal_variability_df['50%'] - last_cal_variability_df['50%']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns_plot = sns.distplot( last_cal_variability_df['90%'] - last_cal_variability_df['10%'] )\n",
    "    ax.set(xlabel='Variation of Last Caloric Intake Time (90% - 10%)', ylabel='Kernel Density Estimation')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_var_plot = last_cal_analysis_variability_plot('data/output/baseline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def first_cal_avg_histplot(data_source:str|pd.DataFrame,\n",
    "                           identifier:int = 1,\n",
    "                           date_col:int = 6,\n",
    "                           time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Plots a histogram of average first caloric intake for all participants. It is recommended\n",
    "    that you use find_date and find_float_time to generate necessary date and time columns for\n",
    "    this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object. \n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    df = df.query('food_type in [\"f\", \"b\"]')\n",
    "    first_cal_time = df.groupby([identifier, date_col])[time_col].min()\n",
    "    avg_first_cal_time = first_cal_time.reset_index().groupby(identifier)[time_col].mean()\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns.distplot(avg_first_cal_time, kde = False)\n",
    "    ax.set(xlabel='First Meal Time - Averaged by Person', ylabel='Frequency Count')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_avg_plot = first_cal_avg_histplot('data/output/baseline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def first_cal_sample_distplot(data_source:str|pd.DataFrame,\n",
    "                              n:int,\n",
    "                              replace:bool = False,\n",
    "                              identifier:int = 1,\n",
    "                              date_col:int = 6,\n",
    "                              time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Creates a distplot for the first caloric intake time for a random selection of 'n' number of \n",
    "    participants. It is recommended that you use find_date and find_float_time to generate necessary\n",
    "    date and time columns for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    n\n",
    "        Number of participants to plot for, selected randomly without replacement.\n",
    "    replace\n",
    "        If true, samples with replacement. Samples without replacement by default.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object. \n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    first_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\\\n",
    "                                       [time_col].min())\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    \n",
    "    print('Plotting distplots for the following users:')\n",
    "    for i in np.random.choice(np.array(list(set(first_cal_by_person.index.droplevel(date_col)))), n, replace=replace):\n",
    "        print(i)\n",
    "        sns.distplot(first_cal_by_person[time_col].loc[i])\n",
    "    ax.set_xlabel('Time')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_distplot = first_cal_sample_distplot('data/output/intervention.json', n = 5, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def last_cal_avg_histplot(data_source:str|pd.DataFrame,\n",
    "                          identifier:int = 1,\n",
    "                          date_col:int = 6,\n",
    "                          time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Plots a histogram of average last caloric intake for all participants. It is recommended\n",
    "    that you use find_date and find_float_time to generate necessary date and time columns for\n",
    "    this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object. \n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    df = df.query('food_type in [\"f\", \"b\"]')\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    last_cal_time = df.groupby([identifier, date_col])[time_col].max()\n",
    "    avg_last_cal_time = last_cal_time.reset_index().groupby(identifier)[time_col].mean()\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns.distplot(avg_last_cal_time, kde = False)\n",
    "    ax.set(xlabel='Last Meal Time - Averaged by Person', ylabel='Frequency Count')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_avg_hist = last_cal_avg_histplot('data/output/baseline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def last_cal_sample_distplot(data_source:str|pd.DataFrame,\n",
    "                             n:int,\n",
    "                             replace:bool = False,\n",
    "                             identifier:int = 1,\n",
    "                             date_col:int = 6,\n",
    "                             time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Creates a distplot for the last caloric intake time for a random selection of 'n' number of \n",
    "    participants. It is recommended that you use find_date and find_float_time to generate necessary\n",
    "    date and time columns for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    n\n",
    "        Number of participants to plot for, selected randomly without replacement.\n",
    "    replace\n",
    "        If true, samples with replacement. Samples without replacement by default.\n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object. \n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    last_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\\\n",
    "                                       [time_col].max())\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    \n",
    "    print('Plotting distplots for the following users:')\n",
    "    for i in np.random.choice(np.array(list(set(last_cal_by_person.index.droplevel(date_col)))), n, replace=replace):\n",
    "        print(i)\n",
    "        sns.distplot(last_cal_by_person[time_col].loc[i])\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_distplot = last_cal_sample_distplot('data/output/intervention.json', n = 5, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def swarmplot(data_source:str|pd.DataFrame,\n",
    "              max_loggings:int,\n",
    "              identifier:int = 1,\n",
    "              date_col:int = 6,\n",
    "              time_col:int = 7) -> matplotlib.figure.Figure:\n",
    "    \"\"\"\n",
    "    Creates a swarmplot for participants logging data. It is recommended that you\n",
    "    use find_date and find_float_time to generate necessary date and time columns for\n",
    "    this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_source\n",
    "        String file or folder path. Single .json or .csv paths create a pd.DataFrame. \n",
    "        Folder paths with files matching the input pattern are read together into a single pd.DataFrame. Existing\n",
    "        dataframes are read as is. Must have a column for 'food_type' within the data.\n",
    "    max_loggings\n",
    "        Maximum number of randomly selected logs to be plotted for each participant. \n",
    "    identifier\n",
    "        Column number for an existing unique identifier column in provided data source. Data exported from mCC typically\n",
    "        has a unique identifier as its 1st column.\n",
    "    date_col\n",
    "        Column number for an existing date column in provided data source. \n",
    "    time_col\n",
    "        Column number for an existing time column in provided data source.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fig\n",
    "        Matplotlib figure object.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_source)\n",
    "    \n",
    "    identifier = df.columns[identifier]\n",
    "    # if treets functions have been used (in any order) to generate columns\n",
    "    # find appropriate column names, if not check for expected column position\n",
    "    if 'date' in df.columns:\n",
    "        date_col = df.columns[df.columns.get_loc('date')]\n",
    "    else:\n",
    "        date_col = df.columns[date_col]\n",
    "        \n",
    "    if 'float_time' in df.columns:\n",
    "        time_col = df.columns[df.columns.get_loc('float_time')]\n",
    "    else:\n",
    "        time_col = df.columns[time_col]\n",
    "    \n",
    "    def subsamp_by_cond(alldat):\n",
    "        alld = []\n",
    "        for apart in alldat[identifier].unique():\n",
    "            dat = alldat[alldat[identifier]==apart]\n",
    "            f_n_b = dat.query('food_type in [\"f\", \"b\"]')\n",
    "            n = min([f_n_b.shape[0], max_loggings])\n",
    "            sub = f_n_b.sample(n = n, axis=0)\n",
    "            alld.append(sub)\n",
    "        return pd.concat(alld)\n",
    "\n",
    "    sample = subsamp_by_cond(df)\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 30), dpi=300)\n",
    "\n",
    "\n",
    "    ax.axvspan(3.5,6, alpha=0.2, color=[0.8, 0.8, 0.8]  )\n",
    "    ax.axvspan(18,28.5, alpha=0.2, color=[0.8, 0.8, 0.8]  )\n",
    "    # plt.xlabel('Hour of day')\n",
    "    plt.xticks([4,8,12,16,20,24,28],[4,8,12,16,20,24,4])\n",
    "    plt.title('Caloric Events for TRE Group')\n",
    "\n",
    "    ax = sns.swarmplot(data = sample, \n",
    "                  y = identifier, \n",
    "                  x = time_col, \n",
    "                  dodge = True, \n",
    "                  color = sns.xkcd_rgb['golden rod'],\n",
    "                 )\n",
    "\n",
    "    ax.set(\n",
    "        facecolor = 'white', \n",
    "        title = 'Caloric Events',\n",
    "        ylabel = 'Participant',\n",
    "        xlabel = 'Local Time'\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm = swarmplot('data/output/public.json', max_loggings = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
