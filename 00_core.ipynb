{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Restricted Eating ExperimenTS API\n",
    "\n",
    "> Process collected data from mcc app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from datetime import date \n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from collections import defaultdict \n",
    "import nltk\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, words\n",
    "import glob\n",
    "\n",
    "import re\n",
    "import string\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def file_loader(data_source):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This is a helper function that 1. if data_scource is a single file path in pickle/csv format, it reads it into a pd dataframe. 2. if it is a folder path, it reads csv and pickle files that match the pattern parameter in the data_scource folder into one dataframe. \\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_source(str, pandas df): input path, csv or pickle file or a pattern to be matched when searching csv/pickle files that will be read into one dataframe.\\n\n",
    "    Output:\\n\n",
    "        - a pandas dataframe.\\n\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(data_source, str):\n",
    "        data_lst = glob.glob(data_source)\n",
    "        dfs = []\n",
    "        for x in data_lst:\n",
    "            if x[-4:] == '.csv':\n",
    "                dfs.append(pd.read_csv(x))\n",
    "            elif x[-7:] == '.pickle':\n",
    "                pickle_file = open(x, 'rb')\n",
    "                pickle_file = pickle.load(pickle_file)\n",
    "                if not isinstance(pickle_file, pd.DataFrame):\n",
    "                    return pickle_file\n",
    "                dfs.append(pickle_file)\n",
    "        df = pd.concat(dfs).reset_index(drop=True)\n",
    "    else:\n",
    "        df = data_source\n",
    "\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads a specific csv file into a pd dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-12 02:30:00 +0000</td>\n",
       "      <td>milk</td>\n",
       "      <td>b</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-12 02:45:00 +0000</td>\n",
       "      <td>some medication</td>\n",
       "      <td>m</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           original_logtime        desc_text food_type      PID\n",
       "0           0  2021-05-12 02:30:00 +0000             milk         b  yrt1999\n",
       "1           1  2021-05-12 02:45:00 +0000  some medication         m  yrt1999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader(\"data/col_test_data/toy_data_2000.csv\").head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads in all files that matches yrt\\*_food_data\\*.csv pattern in data/col_test_data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>PID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021-05-12 02:30:00 +0000</td>\n",
       "      <td>Milk</td>\n",
       "      <td>b</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2021-05-12 02:45:00 +0000</td>\n",
       "      <td>Some Medication</td>\n",
       "      <td>m</td>\n",
       "      <td>yrt1999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           original_logtime        desc_text food_type      PID\n",
       "0           0  2021-05-12 02:30:00 +0000             Milk         b  yrt1999\n",
       "1           1  2021-05-12 02:45:00 +0000  Some Medication         m  yrt1999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader('data/col_test_data/yrt*_food_data*.csv').head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reads all pickle and csv files in ./data/output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>original_logtime_notz</th>\n",
       "      <th>date</th>\n",
       "      <th>local_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>day_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340217.0</td>\n",
       "      <td>3806061.0</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Vegetarian Chili</td>\n",
       "      <td>f</td>\n",
       "      <td>2018-03-02 04:21:00+00:00</td>\n",
       "      <td>2018-03-02 04:21:00+00:00</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>04:21:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1340218.0</td>\n",
       "      <td>8677718.0</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Icecream</td>\n",
       "      <td>f</td>\n",
       "      <td>2018-03-02 04:41:00+00:00</td>\n",
       "      <td>2018-03-02 04:41:00+00:00</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>4.683333</td>\n",
       "      <td>04:41:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         ID      unique_code  research_info_id         desc_text  \\\n",
       "0   1340217.0  3806061.0  alqt14018795225             150.0  Vegetarian Chili   \n",
       "1   1340218.0  8677718.0  alqt14018795225             150.0          Icecream   \n",
       "\n",
       "  food_type           original_logtime     original_logtime_notz        date  \\\n",
       "0         f  2018-03-02 04:21:00+00:00 2018-03-02 04:21:00+00:00  2018-03-02   \n",
       "1         f  2018-03-02 04:41:00+00:00 2018-03-02 04:41:00+00:00  2018-03-02   \n",
       "\n",
       "   local_time      time  week_from_start    year cleaned  day_count  \n",
       "0    4.350000  04:21:00             13.0  2018.0     NaN        NaN  \n",
       "1    4.683333  04:41:00             13.0  2018.0     NaN        NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader('data/output/*').head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_date(data_source, col, h=0):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        Extract date information from a column and shift each date in the column by h hours. (Day starts h hours early if h is negative and h hours late if h is positive)\\n\n",
    "        \n",
    "    Input:\\n\n",
    "        - data_scource(str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n",
    "        - col(str) : column that contains the information of date and time, which is 24 hours system.\n",
    "        - h(int) : hours to shift the date. For example, when h = 4, everyday starts and ends 4 hours later than normal.\n",
    "        \n",
    "    Return:\\n\n",
    "        - a pandas series represents the date extracted from col.\\n\n",
    "\n",
    "    Requirements:\\n\n",
    "        Elements in col should be pd.datetime objects\n",
    "            \n",
    "    \"\"\"\n",
    "    df = file_loader(data_source)\n",
    "    \n",
    "    if df[col].dtype == 'O':\n",
    "        raise TypeError(\"'{}' column must be converted to datetime object\".format(col))\n",
    "        \n",
    "    def find_date(d, h):\n",
    "        if h > 0:\n",
    "            if d.hour < h:\n",
    "                return d.date() - pd.Timedelta('1 day')\n",
    "        if h < 0:\n",
    "            if d.hour+1 > (24+h):\n",
    "                return d.date() + pd.Timedelta('1 day')\n",
    "        return d.date()\n",
    "    return df[col].apply(find_date, args=([h]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-09\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-09"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = file_loader('data/test_food_details.csv')\n",
    "df['original_logtime'] = pd.to_datetime(df['original_logtime'])\n",
    "df['date'] = find_date(df, 'original_logtime')\n",
    "df[['original_logtime', 'date']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-08\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-08"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = find_date(df, 'original_logtime', 4)\n",
    "df[['original_logtime', 'date']].head(3)\n",
    "# last two rows shift one day earlier due to that a day starts at 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-22 22:53:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-09\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-09\n",
       "3 2018-02-22 21:52:00+00:00  2018-02-23\n",
       "4 2018-02-22 22:53:00+00:00  2018-02-23"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['date'] = find_date(df, 'original_logtime', -4)\n",
    "df[['original_logtime', 'date']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that last two rows shift one day later due to that a day ends at 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_float_time(data_scource, col, h=0):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        Extract time information from a column and shift each time by h hours. (Day starts h hours early if h is negative and h hours late if h is positive)\\n\n",
    "        \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n",
    "        - col(str) : column that contains the information of date and time that's 24 hours system.\n",
    "        - h(int) : hours to shift the date. For example, when h = 4, everyday starts at 4 and ends at 28. When h = -4, everyday starts at -4 and ends at 20.\n",
    "        \n",
    "    Return:\\n\n",
    "        - a pandas series represents the date extracted from col.\\n\n",
    "\n",
    "    Requirements:\\n\n",
    "        Elements in col should be pd.datetime objects\n",
    "            \n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "    if df[col].dtype == 'O':\n",
    "        raise TypeError(\"'{}' column must be converted to datetime object firsly\".format(col))\n",
    "    local_time = df[col].apply(lambda x: pd.Timedelta(x.time().isoformat()).total_seconds() /3600.)\n",
    "    if h > 0:\n",
    "        local_time = np.where(local_time < h, 24+ local_time, local_time)\n",
    "        return pd.Series(local_time)\n",
    "    if h < 0:\n",
    "        local_time = np.where(local_time > (24+h), local_time-24., local_time)\n",
    "        return pd.Series(local_time)\n",
    "    return local_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_loader('data/test_food_details.csv')\n",
    "df['original_logtime'] = pd.to_datetime(df['original_logtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>float_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime  float_time\n",
       "0 2017-12-08 17:30:00+00:00   17.500000\n",
       "1 2017-12-09 00:01:00+00:00    0.016667\n",
       "2 2017-12-09 00:58:00+00:00    0.966667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['float_time'] = find_float_time(df, 'original_logtime')\n",
    "df[['original_logtime', 'float_time']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.966667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date  float_time\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08   17.500000\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-08   24.016667\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-08   24.966667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['float_time'] = find_float_time(df, 'original_logtime', 4)\n",
    "df['date'] = find_date(df, 'original_logtime', 4)\n",
    "df[['original_logtime','date', 'float_time']].head(3)\n",
    "# last two rows shift one day earlier due to that a day starts at 4 and ends at 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>0.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>-2.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-22 22:53:00+00:00</td>\n",
       "      <td>2018-02-23</td>\n",
       "      <td>-1.116667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           original_logtime        date  float_time\n",
       "0 2017-12-08 17:30:00+00:00  2017-12-08   17.500000\n",
       "1 2017-12-09 00:01:00+00:00  2017-12-09    0.016667\n",
       "2 2017-12-09 00:58:00+00:00  2017-12-09    0.966667\n",
       "3 2018-02-22 21:52:00+00:00  2018-02-23   -2.133333\n",
       "4 2018-02-22 22:53:00+00:00  2018-02-23   -1.116667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['float_time'] = find_float_time(df, 'original_logtime', -4)\n",
    "df['date'] = find_date(df, 'original_logtime', -4)\n",
    "df[['original_logtime','date', 'float_time']].head(5)\n",
    "# last two rows shift one day later and local_time starts at -4 and ends at 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def week_from_start(data_scource, col, identifier):\n",
    "        \"\"\"\n",
    "        Description:\\n\n",
    "            Calculate the number of weeks for each logging since the first day of the logging for each participant(identifier). The returned values for loggings from the first week are 1. \n",
    "        Input:\\n\n",
    "            - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n",
    "            - col (str): column name that contains date information from the data_scource dataframe.\n",
    "            - identifier (str): unique_id or ID, or name that identifies people.\n",
    "\n",
    "        Return:\\n\n",
    "            - a numpy array represents the date extracted from col.\\n\n",
    "        \"\"\"\n",
    "        \n",
    "        df = file_loader(data_scource)\n",
    "        if 'date' not in df.columns:\n",
    "            raise NameError(\"There must exist a 'date' column.\")\n",
    "        # Handle week from start\n",
    "        df_dic = dict(df.groupby(identifier).agg(np.min)[col])\n",
    "\n",
    "        def count_weeks(s):\n",
    "            return (s.date - df_dic[s[identifier]]).days // 7 + 1\n",
    "    \n",
    "        return df.apply(count_weeks, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_loader('data/test_food_details.csv')\n",
    "df['original_logtime'] = pd.to_datetime(df['original_logtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_code</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>week_from_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>2017-12-09 00:58:00+00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>2018-02-22 21:52:00+00:00</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       unique_code          original_logtime  week_from_start\n",
       "2  alqt14018795225 2017-12-09 00:58:00+00:00                1\n",
       "3  alqt14018795225 2018-02-22 21:52:00+00:00               11"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# date column is a pre-requisite\n",
    "df['date'] = find_date(df, 'original_logtime')\n",
    "df['week_from_start'] = week_from_start(df, 'date', 'unique_code')\n",
    "df[['unique_code','original_logtime','week_from_start']][2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_phase_duration(df):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This is a function that calculates how many days each phase in the study took. Result includes the start and end date for that phase.\n",
    "    \n",
    "    Input:\\n\n",
    "        - df(pandas df) : information dataframe that contains columns: Start_Day, End_day\n",
    "    \n",
    "    Output:\\n\n",
    "        - a dataframe contains the phase_duration column.\n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'Start_day' and 'End_day' column exist in the df.\n",
    "        - 'phase_duration' column exists in the df.\n",
    "    \"\"\"\n",
    "    df['phase_duration'] = df['End_day'] - df['Start_Day']+ pd.Timedelta(\"1 days\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase_duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  phase_duration\n",
       "0         3 days\n",
       "1         4 days\n",
       "2         3 days\n",
       "3         4 days\n",
       "4            NaT"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_phase_duration(pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'))[['phase_duration']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def load_food_data(data_scource, identifier, datetime_col, h):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        Load food data and output processed data in a dataframe.\\n\n",
    "    \n",
    "        Process includes:\\n\n",
    "        1. Dropping 'foodimage_file_name' column.\\n\n",
    "        2. Handling the format of time by deleting am/pm by generating a new column, 'original_logtime_notz'\\n\n",
    "        3. Generating the date column with possible hour shifts, 'date'\\n\n",
    "        4. Converting time into float number into a new column with possible hour shifts, 'float_time'\\n\n",
    "        5. Converting time to a format of HH:MM:SS, 'time'\\n\n",
    "        6. Generating the column 'week_from_start' that contains the week number that the participants input the food item.\\n\n",
    "        7. Generating 'year' column based on the input data.\\n\n",
    "\n",
    "    Input:\\n\n",
    "        - data_scource (str or pandas df): input path, csv file\\n\n",
    "        - identifier(str): id-like column that's used to identify a subject.\\n\n",
    "        - datetime_col(str): column that contains date and time in string format.\\n\n",
    "        - h(int) : hours to shift the date. For example, when h = 4, everyday starts and ends 4 hours later than normal.\n",
    "        \n",
    "    Output:\\n\n",
    "        - the processed dataframe in pandas df format.\\n\n",
    "\n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - foodimage_file_name\\n\n",
    "            - original_logtime\\n\n",
    "            - date\\n\n",
    "            - unique_code\\n\n",
    "    \"\"\"\n",
    "    food_all = file_loader(data_scource)\n",
    "    \n",
    "    try:\n",
    "        food_all = food_all.drop(columns = ['foodimage_file_name'])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    def handle_time(s):\n",
    "        \"\"\"\n",
    "        helper function to get rid of am/pm in the end of each time string\n",
    "        \"\"\"\n",
    "        tmp_s = s.replace('p.m.', '').replace('a.m.', '')\n",
    "        try:\n",
    "            return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "        except:\n",
    "            try:\n",
    "                if int(tmp_s.split()[1][:2]) > 12:\n",
    "                    tmp_s = s.replace('p.m.', '').replace('a.m.', '').replace('PM', '').replace('pm', '')\n",
    "                return pd.to_datetime(' '.join(tmp_s.split()[:2]) )\n",
    "            except:\n",
    "                return np.nan\n",
    "\n",
    "    food_all[datetime_col] = food_all[datetime_col].apply(handle_time)\n",
    "    \n",
    "    food_all = food_all.dropna().reset_index(drop = True)\n",
    "    \n",
    "\n",
    "    food_all['date'] = find_date(food_all, datetime_col, h)\n",
    "    \n",
    "    \n",
    "    # Handle the time - Time in floating point format\n",
    "    \n",
    "    food_all['float_time'] = find_float_time(food_all, datetime_col, h)\n",
    "    \n",
    "    # Handle the time - Time in Datetime object format\n",
    "    food_all['time'] = pd.DatetimeIndex(food_all[datetime_col]).time\n",
    "    \n",
    "    # Handle week from start\n",
    "    food_all['week_from_start'] = week_from_start(food_all,'date',identifier)\n",
    "    \n",
    "    food_all['year'] = food_all.date.apply(lambda d: d.year)\n",
    "    \n",
    "    return food_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340147</td>\n",
       "      <td>7572733</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1340148</td>\n",
       "      <td>411111</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>b</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       ID      unique_code  research_info_id     desc_text  \\\n",
       "0     1340147  7572733  alqt14018795225               150         Water   \n",
       "1     1340148   411111  alqt14018795225               150  Coffee White   \n",
       "\n",
       "  food_type          original_logtime        date  float_time      time  \\\n",
       "0         w 2017-12-08 17:30:00+00:00  2017-12-08   17.500000  17:30:00   \n",
       "1         b 2017-12-09 00:01:00+00:00  2017-12-08   24.016667  00:01:00   \n",
       "\n",
       "   week_from_start  year  \n",
       "0                1  2017  \n",
       "1                1  2017  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_food_data('data/test_food_details.csv','unique_code', 'original_logtime', h=4).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def in_good_logging_day(data_scource, identifier, date_col, time_col, min_log_num = 2, min_seperation = 4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        A logging's in a good logging day if the there are more than min_log_num loggings in one day w/ more than min_seperation hoursx apart from the earliest logging and the latest logging and False otherwise.\\n\n",
    "        \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format.\\n\n",
    "        - identifier (str): id-like column that's used to identify a subject.\\n\n",
    "        - time_col (str): column that contains time in float format.\\n\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\\n\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\\n\n",
    "        \n",
    "    Return:\\n\n",
    "        - A boolean numpy array indicating whether the corresponding row is in a good logging day. Details on the criteria is in the description.\\n\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "\n",
    "    \"\"\"\n",
    "    def adherent(s):\n",
    "        if len(s.values) >= min_log_num and (max(s.values) - min(s.values)) >= min_seperation:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    df = file_loader(data_scource)\n",
    "    \n",
    "    adherent_dict = dict(df.groupby([identifier, date_col])[time_col].agg(adherent))\n",
    "\n",
    "    \n",
    "    return df.apply(lambda x: adherent_dict[(x[identifier], x.date)], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>unique_code</th>\n",
       "      <th>research_info_id</th>\n",
       "      <th>desc_text</th>\n",
       "      <th>food_type</th>\n",
       "      <th>original_logtime</th>\n",
       "      <th>date</th>\n",
       "      <th>float_time</th>\n",
       "      <th>time</th>\n",
       "      <th>week_from_start</th>\n",
       "      <th>year</th>\n",
       "      <th>in_good_logging_day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1340147</td>\n",
       "      <td>7572733</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Water</td>\n",
       "      <td>w</td>\n",
       "      <td>2017-12-08 17:30:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>17:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1340148</td>\n",
       "      <td>411111</td>\n",
       "      <td>alqt14018795225</td>\n",
       "      <td>150</td>\n",
       "      <td>Coffee White</td>\n",
       "      <td>b</td>\n",
       "      <td>2017-12-09 00:01:00+00:00</td>\n",
       "      <td>2017-12-08</td>\n",
       "      <td>24.016667</td>\n",
       "      <td>00:01:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       ID      unique_code  research_info_id     desc_text  \\\n",
       "0     1340147  7572733  alqt14018795225               150         Water   \n",
       "1     1340148   411111  alqt14018795225               150  Coffee White   \n",
       "\n",
       "  food_type          original_logtime        date  float_time      time  \\\n",
       "0         w 2017-12-08 17:30:00+00:00  2017-12-08   17.500000  17:30:00   \n",
       "1         b 2017-12-09 00:01:00+00:00  2017-12-08   24.016667  00:01:00   \n",
       "\n",
       "   week_from_start  year  in_good_logging_day  \n",
       "0                1  2017                 True  \n",
       "1                1  2017                 True  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code','original_logtime', 4)\n",
    "df['in_good_logging_day'] = in_good_logging_day(df, 'unique_code', 'date', 'float_time')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "def clean_loggings(data_scource, text_col, identifier):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function convert all the loggings in the data_scource file into a list of typo-corrected items based on the text_col column. This function is based on a built-in vocabulary dictionary and an n-gram searcher.\\n\n",
    "       \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - text_col(str): column that will be converted to individual items.\n",
    "        - identifier(str): participants' unique identifier such as id, name, etc.\n",
    "        \n",
    "    Return:\\n\n",
    "        - A dataframe contains the cleaned version of the text_col.\\n\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    food_all = file_loader(data_scource)\n",
    "    # initialize food parser instance\n",
    "    fp = FoodParser()\n",
    "    fp.initialization()\n",
    "    \n",
    "    # parse food\n",
    "    parsed = [fp.parse_food(i, return_sentence_tag = True) for i in food_all.desc_text.values]\n",
    "    \n",
    "    food_all_parsed = pd.DataFrame({\n",
    "    identifier: food_all[identifier],\n",
    "    text_col: food_all[text_col],\n",
    "    'cleaned': parsed\n",
    "    })\n",
    "    \n",
    "    food_all_parsed['cleaned'] = food_all_parsed['cleaned'].apply(lambda x: x[0])\n",
    "    \n",
    "    \n",
    "    return food_all_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FoodParser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclean_loggings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/output/public.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdesc_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munique_code\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36mclean_loggings\u001b[0;34m(data_scource, text_col, identifier)\u001b[0m\n\u001b[1;32m     17\u001b[0m food_all \u001b[38;5;241m=\u001b[39m file_loader(data_scource)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# initialize food parser instance\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m fp \u001b[38;5;241m=\u001b[39m \u001b[43mFoodParser\u001b[49m()\n\u001b[1;32m     20\u001b[0m fp\u001b[38;5;241m.\u001b[39minitialization()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# parse food\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FoodParser' is not defined"
     ]
    }
   ],
   "source": [
    "clean_loggings('data/output/public.pickle', 'desc_text', 'unique_code').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_types(data_scource, food_type):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function filters with the expected food types and return a cleaner version of data_scource file.\\n \n",
    "       \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\\n\n",
    "        - food_type (str): expected types of the loggings for filtering, in format of list. Available types:  \\n\n",
    "            1. 'w' : water \\n\n",
    "            2. 'b' : beverage \\n\n",
    "            3. 'f' : food \\n\n",
    "            4. 'm' : medicine \\n\n",
    "        \n",
    "    Return:\\n\n",
    "        - A filtered dataframe with expected food type/types with five columns: 'unique_code','food_type', 'desc_text', 'date', 'local_time'.\\n\n",
    "        \n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - food_type\\n\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_scource)\n",
    "        \n",
    "    if len(food_type) == 0:\n",
    "        return df\n",
    "    \n",
    "    if len(food_type) == 1:\n",
    "        if food_type[0] not in ['w', 'b', 'f', 'm']:\n",
    "            raise Exception(\"not a valid logging type\")\n",
    "        filtered = df[df['food_type']==food_type[0]]\n",
    "    else:  \n",
    "        filtered = df[df['food_type'].isin(food_type)]\n",
    "        \n",
    "    \n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_types('data/output/public_basline_usable_expanded.pickle',['w', 'f'])[['unique_code','desc_text','food_type']].head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\n",
    "get_types(df, ['m'])[['unique_code','desc_text','food_type']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def count_caloric_entries(df):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This is a function that counts the number of food and beverage loggings.\n",
    "    \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "    \n",
    "    Output:\\n\n",
    "        - a float representing the number of caloric entries.\n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \"\"\"\n",
    "    if 'food_type' not in df.columns:\n",
    "        raise Exception(\"'food_type' column must exist in the dataframe.\")\n",
    "        \n",
    "    \n",
    "    return df[df['food_type'].isin(['f','b'])].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\n",
    "count_caloric_entries(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mean_daily_eating_duration(df, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This is a function that calculates the mean daily eating window, which is defined as the duration of first and last caloric intake.\n",
    "    \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n",
    "        - time_col(column existed in df, string) : contains the float time data for each logging.\n",
    "    \n",
    "    Output:\\n\n",
    "        - a float representing the mean daily eating window.\n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "        - float time is calculated.\n",
    "        \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    breakfast_time = df.groupby(date_col).agg(min)\n",
    "    dinner_time = df.groupby(date_col).agg(max)\n",
    "    return (dinner_time[time_col]-breakfast_time[time_col]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\n",
    "mean_daily_eating_duration(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def std_daily_eating_duration(df, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the standard deviation of daily eating window, which is defined as the duration between the first and last caloric intake.\n",
    "    \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n",
    "        - time_col(column existed in df, string) : contains the float time data for each logging.\n",
    "    \n",
    "    Output:\\n\n",
    "        - a float representing the standard deviation of daily eating window.\n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'food_type' column existed in the df.\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    breakfast_time = df.groupby(date_col).agg(min)\n",
    "    dinner_time = df.groupby(date_col).agg(max)\n",
    "\n",
    "    return (dinner_time[time_col]-breakfast_time[time_col]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\n",
    "std_daily_eating_duration(df, 'date', 'float_time' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def earliest_entry(df, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the earliest first calorie on any day in the study period. \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n",
    "        - time_col(column existed in df, string) : contains information of logging time in float.\n",
    "    \n",
    "    Output:\\n\n",
    "        - the earliest caloric time in float on any day in the study period. \n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    df = get_types(df, ['f', 'b'])\n",
    "    \n",
    "    return df[time_col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\n",
    "earliest_entry(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mean_first_cal(df, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the average time of first calory intake. \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n",
    "        - time_col(column existed in df, string) : contains information of logging time in float.\n",
    "    \n",
    "    Output:\\n\n",
    "        - the mean first caloric intake time in float in the study period. \n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return df.groupby([date_col])[time_col].min().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\n",
    "mean_first_cal(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the average mean first cal time for each participant\n",
    "df.groupby('unique_code').agg(mean_first_cal, 'date', 'float_time').iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def std_first_cal(df, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the average time of first calory intake. \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n",
    "        - time_col(column existed in df, string) : contains information of logging time in float.\n",
    "    \n",
    "    Output:\\n\n",
    "        - the mean first caloric intake time in float in the study period. \n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return df.groupby([date_col])[time_col].min().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\n",
    "std_first_cal(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mean_last_cal(df, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the average time of last calory intake. \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n",
    "        - time_col(column existed in df, string) : contains information of logging time in float.\n",
    "    \n",
    "    Output:\\n\n",
    "        - the mean last caloric intake time in float in the study period. \n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return df.groupby([date_col])[time_col].max().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\n",
    "mean_last_cal(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def std_last_cal(df, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the average time of last calory intake. \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "        - date_col(column existed in df, string) : column name that contains date information from the df dataframe.\n",
    "        - time_col(column existed in df, string) : contains information of logging time in float.\n",
    "    \n",
    "    Output:\\n\n",
    "        - the mean last caloric intake time in float in the study period. \n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return df.groupby([date_col])[time_col].max().std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv','unique_code', 'original_logtime', 4)\n",
    "std_last_cal(df, 'date', 'float_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def logging_day_counts(df):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the number of days that contains any loggings. \n",
    "    Input:\\n\n",
    "        - df(pandas df) : food_logging data.\n",
    "    \n",
    "    Output:\\n\n",
    "        - an integer that represents the number of logging days.\n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \"\"\"\n",
    "    \n",
    "    return df.date.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\n",
    "logging_day_counts(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_missing_logging_days(df, start_date='not_defined', end_date='not_defined'):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function finds the days during which there's no logging within the period from start_date to end_date. \n",
    "    Input:\\n\n",
    "        - df(panda df): food_logging data.\n",
    "        - start_date(datetime.date object): start date of the period of calculation. If not defined, it will be automatically set to be the earliest date in df. \n",
    "        - end_date(datetime.date object): end date of the period of calculation. If not defined, it will be automatically set to be the latest date in df.\n",
    "    \n",
    "    Output:\\n\n",
    "        - a list that contains all of the dates that don't contain loggings.\n",
    "        \n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "    \"\"\"\n",
    "    # if start_date or end_date is missing, return nan\n",
    "    if pd.isnull(start_date) or pd.isnull(end_date):\n",
    "        return np.nan\n",
    "    # if there is no input on start_date or end_date, use earliest date and latest date\n",
    "    if start_date=='not_defined':\n",
    "        start_date = df['date'].min()\n",
    "    if end_date=='not_defined':\n",
    "        end_date = df['date'].max()\n",
    "        \n",
    "    df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n",
    "    \n",
    "    # get all the dates between two dates\n",
    "    lst = []\n",
    "    for x in pd.date_range(start_date, end_date, freq='d'):\n",
    "         if x not in df['date'].unique():\n",
    "                lst.append(x.date())\n",
    "    \n",
    "    return lst\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\n",
    "find_missing_logging_days(df, datetime.date(2017, 12, 7), datetime.date(2017, 12, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def good_lwa_day_counts(df, window_start, window_end, time_col, min_log_num=2, min_seperation=5, buffer_time= '15 minutes',h=4, start_date='not_defined', end_date='not_defined'):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function calculates the number of good logging days, good window days, outside window days and adherent days. Good logging day is defined as a day that the person makes at least min_log_num number of loggings and the time separation between the earliest and the latest logging are greater than min_seperation.\\n\n",
    "        A good window day is defined as a date that all the food loggings are within the assigned restricted window. An adherent day is defined as a date that is both a good logging day and a good window day.\n",
    "    Input:\\n\n",
    "        - df(pandas df): food_logging data.\n",
    "        - window_start(datetime.time object): start time of the restriction window.\n",
    "        - window_end(datetime.time object): end time of the restriction window.\n",
    "        - time_col(str) : the column that represents the eating time.\n",
    "        - min_log_num(count, int): minimum number of loggings to qualify a day as a good logging day\n",
    "        - min_seperation(hours, int): minimum period of separation between earliest and latest loggings to qualify a day as a good logging day\n",
    "        - buffer_time(time in string that can be passed into pd.Timedelta()): wiggle room for to be added/subtracted on the ends of windows.\n",
    "        - h(hours, int): hours to be pushed back\n",
    "        - start_date(datetime.date object): start date of the period for calculation. If not defined, it will be automatically set to be the earliest date in df.\n",
    "        - end_date(datetime.date object): end date of the period for calculation. If not defined, it will be automatically set to be the latest date in df.\n",
    "    Output:\\n\n",
    "        - Two lists.\n",
    "            - First list contains 4 integers that represent the number of good logging days, good window days, outside window days and adherent_days\n",
    "            - Second list contains lists consisted of specific dates that are not good logging days, window days and adherent days within the range of start date and end date(both inclusive).\n",
    "    Requirement:\\n\n",
    "        - 'date' column existed in the df.\n",
    "        - float time is calculated as time_col.\n",
    "\n",
    "    \"\"\"\n",
    "    # if start_date or end_date is missing, return nan\n",
    "    if pd.isnull(start_date) or pd.isnull(end_date):\n",
    "        return [np.nan,np.nan,np.nan,np.nan], [[],[],[]]\n",
    "    \n",
    "    # if window start or window end are nan, make the windows the same as control's window time.\n",
    "    if pd.isnull(window_start):\n",
    "        window_start = datetime.time(0,0)\n",
    "    if pd.isnull(window_end):\n",
    "        window_end = datetime.time(23,59)\n",
    "        \n",
    "    # if there is no input on start_date or end_date, use earliest date and latest date\n",
    "    if start_date=='not_defined':\n",
    "        start_date = df['date'].min()\n",
    "    if end_date=='not_defined':\n",
    "        end_date = df['date'].max()\n",
    "\n",
    "    # helper function to determine a good logging\n",
    "    def good_logging(local_time_series):\n",
    "        if len(local_time_series.values) >= min_log_num and (max(local_time_series.values) - min(local_time_series.values)) >= min_seperation:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    df = df[(df['date']>=start_date) & (df['date']<=end_date)]\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    df['original_logtime'] = pd.to_datetime(df['original_logtime']).dt.tz_localize(None)\n",
    "\n",
    "    buffer_time = pd.Timedelta(buffer_time).total_seconds()/3600.\n",
    "\n",
    "    in_window_count = []\n",
    "    daily_count = []\n",
    "    good_logging_count = []\n",
    "    cur_dates = df['date'].sort_values(ascending = True).unique()\n",
    "    for aday in cur_dates:\n",
    "        window_start_daily = window_start.hour+window_start.minute/60- buffer_time\n",
    "        window_end_daily = window_end.hour+window_end.minute/60 + buffer_time\n",
    "        tmp = df[df['date']==aday]\n",
    "        if (window_start == datetime.time(0,0)) and (window_end == datetime.time(23,59)):\n",
    "            in_window_count.append(tmp[(tmp[time_col]>=window_start_daily+h) & (tmp[time_col]<=window_end_daily+h)].shape[0])\n",
    "        else:\n",
    "            in_window_count.append(tmp[(tmp[time_col]>=window_start_daily) & (tmp[time_col]<=window_end_daily)].shape[0])\n",
    "        daily_count.append(df[df['date']==aday].shape[0])\n",
    "        good_logging_count.append(good_logging(df[df['date']==aday][time_col]))\n",
    "\n",
    "    in_window_count = np.array(in_window_count)\n",
    "    daily_count = np.array(daily_count)\n",
    "    good_logging_count = np.array(good_logging_count)\n",
    "    good_logging_by_date = [cur_dates[i] for i, x in enumerate(good_logging_count) if x == False]\n",
    "\n",
    "    good_window_days = (in_window_count==daily_count)\n",
    "    good_window_day_counts = good_window_days.sum()\n",
    "    good_window_by_date = [cur_dates[i] for i, x in enumerate(good_window_days) if x == False]\n",
    "    \n",
    "    outside_window_days = in_window_count.size - good_window_days.sum()\n",
    "    good_logging_days = good_logging_count.sum()\n",
    "    if good_logging_count.size == 0:\n",
    "        adherent_day_counts = 0\n",
    "        adherent_days_by_date = []\n",
    "    else:\n",
    "        adherent_days = (good_logging_count & (in_window_count==daily_count))\n",
    "        adherent_days_by_date = [cur_dates[i] for i, x in enumerate(adherent_days) if x == False]\n",
    "        adherent_day_counts = adherent_days.sum()\n",
    "    \n",
    "    rows = [good_logging_days, good_window_day_counts, outside_window_days, adherent_day_counts]\n",
    "    bad_dates = [good_logging_by_date, good_window_by_date, adherent_days_by_date]\n",
    "\n",
    "    return rows, bad_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv','unique_code', 'original_logtime',4)\n",
    "dates, bad_dates = good_lwa_day_counts(df, datetime.time(8,0,0), datetime.time(23,59,59), 'float_time')\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dates[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### experiment design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def filtering_usable_data(df, identifier, date_col, num_items, num_days):\n",
    "    '''\n",
    "    Description:\\n\n",
    "        This function filters the cleaned app data so the users who satisfies the criteria are left. The criteria is that the person is left if the total loggings for that person are more than num_items and at the same time, the total days of loggings are more than num_days.\\n\n",
    "    Input:\\n\n",
    "        - df (pd.DataFrame): the dataframe to be filtered\n",
    "        - identifier (str): unique_id or ID, or name that identifies people.\n",
    "        - date_col (str): column name that contains date information from the df dataframe.\n",
    "        - num_items (int):   number of items to be used as cut-off\n",
    "        - num_days (int):    number of days to be used as cut-off\n",
    "    Output:\\n\n",
    "        - df_usable:  a panda DataFrame with filtered rows\n",
    "        - set_usable: a set of unique_code to be included as \"usable\"\n",
    "    Requirements:\\n\n",
    "        df should have the following columns:\n",
    "            - desc_text\n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "    '''\n",
    "    print(' => filtering_usable_data()')\n",
    "    print('  => using the following criteria:', num_items, 'items and', num_days, 'days.')\n",
    "\n",
    "    # Item logged\n",
    "    log_item_count = df.groupby(identifier).agg('count')[['desc_text']].rename(columns = {'desc_text': 'Total Logged'})\n",
    "\n",
    "    # Day counts\n",
    "    log_days_count = df[[identifier, date_col]]\\\n",
    "        .drop_duplicates().groupby(identifier).agg('count').rename(columns = {date_col: 'Day Count'})\n",
    "\n",
    "    item_count_passed = set(log_item_count[log_item_count['Total Logged'] >= num_items].index)\n",
    "    day_count_passed = set(log_days_count[log_days_count['Day Count'] >= num_days].index)\n",
    "\n",
    "    print('  => # of users pass the criteria:', end = ' ')\n",
    "    print(len(item_count_passed & day_count_passed))\n",
    "    passed_participant_set = item_count_passed & day_count_passed\n",
    "    df_usable = df.loc[df.unique_code.apply(lambda c: c in passed_participant_set)]\\\n",
    "        .copy().reset_index(drop = True)\n",
    "    # print('  => Now returning the pd.DataFrame object with the head like the following.')\n",
    "    # display(df_usable.head(5))\n",
    "    return df_usable, set(df_usable.unique_code.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_loader('data/output/public.pickle')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering_usable_data(df,'unique_code','date', 1000, 14)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_baseline_and_intervention_usable_data(data_scource, identifier, date_col, baseline_num_items, baseline_num_days, intervention_num_items, intervention_num_days):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        Filter and create baseline_expanded and intervention groups based on data_scource pickle file. Expanded baseline dataset contains the first two weeks data and 13, 14 weeks data that pass the given criteria. Intervention dataset contains 13, 14 weeks data that pass the given criteria.\\n\n",
    "        \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format\\n\n",
    "        - identifier (str): unique_id or ID, or name that identifies people.\n",
    "        - date_col (str): column name that contains date information from the df dataframe.\n",
    "        - baseline_num_items (int): number of items to be used as cut-off for baseline group. \\n\n",
    "        - baseline_num_days (int): number of days to be used as cut-off for baseline group. \\n\n",
    "        - intervention_num_items (int): number of items to be used as cut-off for intervention group.\\n\n",
    "        - intervention_num_days (int): number of days to be used as cut-off for intervention group. \\n\n",
    "        \n",
    "    Return:\\n\n",
    "        - a list in which index 0 is the baseline expanded dataframe and 1 is the intervention dataframe.\\n\n",
    "\n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - week_from_start\\n\n",
    "            - desc_text\\n\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    food_all = file_loader(data_scource)\n",
    "    \n",
    "    # create baseline data\n",
    "    df_food_baseline = food_all.query('week_from_start <= 2')\n",
    "    df_food_baseline_usable, food_baseline_usable_id_set = \\\n",
    "    filtering_usable_data(df_food_baseline,identifier, date_col, num_items = baseline_num_items, num_days = baseline_num_days)\n",
    "    \n",
    "    # create intervention data\n",
    "    df_food_intervention = food_all.query('week_from_start in [13, 14]')\n",
    "    df_food_intervention_usable, food_intervention_usable_id_set = \\\n",
    "    filtering_usable_data(df_food_intervention,identifier, date_col, num_items = intervention_num_items, num_days = intervention_num_days)\n",
    "    \n",
    "    # create df that contains both baseline and intervention id_set that contains data for the first two weeks\n",
    "    expanded_baseline_usable_id_set = set(list(food_baseline_usable_id_set) + list(food_intervention_usable_id_set))\n",
    "    df_food_basline_usable_expanded = food_all.loc[food_all.apply(lambda s: s.week_from_start <= 2 \\\n",
    "                                                    and s.unique_code in expanded_baseline_usable_id_set, axis = 1)]\n",
    "        \n",
    "    return [df_food_basline_usable_expanded, df_food_intervention_usable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= prepare_baseline_and_intervention_usable_data('data/output/public.pickle','unique_code','date', 20, 10, 40, 12)[0]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data analysis/summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def users_sorted_by_logging(data_scource, food_type = [\"f\", \"b\", \"m\", \"w\"]):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This function returns a dataframe reports the number of good logging days for each user in the data_scource file. The default order is descending.\\n\n",
    "        \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or pandas dataframe format.\\n\n",
    "        - food_type (str): food types to filter in a list format. Default: [\"f\", \"b\", \"m\", \"w\"]. Available food types:\\n\n",
    "            1. 'w' : water \\n\n",
    "            2. 'b' : beverage \\n\n",
    "            3. 'f' : food \\n\n",
    "            4. 'm' : medicine \\n\n",
    "        \n",
    "    Return:\\n\n",
    "        - A dataframe contains the number of good logging days for each user.\\n\n",
    "        \n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - food_type\\n\n",
    "            - unique_code\\n\n",
    "            - date\\n\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    food_all = file_loader(data_scource)\n",
    "    \n",
    "    # filter the dataframe so it only contains input food type\n",
    "    \n",
    "    filtered_users = food_all.query('food_type in @food_type')\n",
    "    \n",
    "    filtered_users['in_good_logging_day'] = in_good_logging_day(filtered_users,'unique_code','date','local_time')\n",
    "    \n",
    "    food_top_users_day_counts = pd.DataFrame(filtered_users.query('in_good_logging_day == True')\\\n",
    "                            [['date', 'unique_code']].groupby('unique_code')['date'].nunique())\\\n",
    "                            .sort_values(by = 'date', ascending = False).rename(columns = {'date': 'day_count'})\n",
    "\n",
    "    \n",
    "    return food_top_users_day_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_sorted_by_activity('data/output/public.pickle', ['f','b']).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def eating_intervals_percentile(data_scource, time_col, identifier):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "       This function calculates the .025, .05, .10, .125, .25, .5, .75, .875, .9, .95, .975 quantile of eating time and mid 95%, mid 90%, mid 80%, mid 75% and mid 50% duration for each user.\\n \n",
    "    \n",
    "    Input:\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - time_col(str) : the column that represents the eating time.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        \n",
    "    Return:\\n\n",
    "        - A summary table with count, mean, std, min, quantiles and mid durations for all subjects from the data_scource file.\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "    \n",
    "    if df.shape[0] == 0:\n",
    "        return pd.DataFrame(np.array([[np.nan]*21]), columns = ['count', 'mean', 'std', 'min', '2.5%', '5%', '10%', '12.5%', '25%',\n",
    "       '50%', '75%', '87.5%', '90%', '95%', '97.5%', 'max', 'duration mid 95%',\n",
    "       'duration mid 90%', 'duration mid 80%', 'duration mid 75%',\n",
    "       'duration mid 50%'])\n",
    "    \n",
    "    ptile = df.groupby(identifier)[time_col].describe(percentiles=[.025, .05, .10, .125, .25, .5, .75, .875, .9, .95, .975])\n",
    "    ll = ['2.5%','5%','10%','12.5%','25%']\n",
    "    ul = ['97.5%','95%', '90%','87.5%', '75%']\n",
    "    mp = ['duration mid 95%', 'duration mid 90%', 'duration mid 80%', 'duration mid 75%','duration mid 50%']\n",
    "    for low, upp, midp in zip(ll,ul,mp):\n",
    "        ptile[midp] = ptile[upp] - ptile[low]\n",
    "        \n",
    "    return ptile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv','unique_code', 'original_logtime',4)\n",
    "eating_intervals_percentile(df, 'float_time', 'unique_code').iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def first_cal_analysis_summary(data_scource, identifier, date_col, time_col,min_log_num=2, min_separation=4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function takes the loggings in good logging days and calculate the 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time for each user.\\n \n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n",
    "        \n",
    "        \n",
    "    Return:\\n\n",
    "        - A summary table with 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time for all subjects from the data_scource file.\\n\n",
    "        \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "\n",
    "    df = file_loader(data_scource)\n",
    "        \n",
    "    # leave only the loggings in a good logging day\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    first_cal_series = df.groupby([identifier, date_col])[time_col].min().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    first_cal_df = pd.DataFrame(first_cal_series)\n",
    "    all_rows = []\n",
    "    for index in first_cal_df.index:\n",
    "        tmp_dict = dict(first_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "    return first_cal_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_analysis_summary('data/output/public_basline_usable_expanded.pickle', 'unique_code', 'date', 'local_time').head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def last_cal_analysis_summary(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function takes the loggings in good logging days and calculate the 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time for each user.\\n \n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n",
    "        \n",
    "    Return:\\n\n",
    "        - A summary table with 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time for all subjects from the data_scource file.\\n\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_scource)\n",
    "        \n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, identifier,date_col, time_col, min_log_num, min_separation)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    last_cal_series = df.groupby([identifier, date_col])[time_col].max().groupby(identifier).quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    last_cal_df = pd.DataFrame(last_cal_series)\n",
    "    all_rows = []\n",
    "    for index in last_cal_df.index:\n",
    "        tmp_dict = dict(last_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "    \n",
    "    return last_cal_summary_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_analysis_summary('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarize_data(data_scource, identifier, float_time_col, date_col, min_log_num = 2, min_seperation = 4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function calculates num_days, num_total_items, num_f_n_b, num_medications, num_water, duration_mid_95, start_95, end_95, first_cal_avg, first_cal_std, last_cal_avg, last_cal_std, eating_win_avg, eating_win_std, adherent_count, first_cal variation (90%-10%), last_cal variation (90%-10%).\\n \n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - float_time_col(str) : the column that represents the eating time.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\\n\n",
    "        \n",
    "    Return:\\n\n",
    "        - A summary table with count, mean, std, min, quantiles and mid durations for all subjects from the data_scource file.\\n\n",
    "        \n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - food_type\\n\n",
    "  \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "    \n",
    "    #num_days\n",
    "    num_days = df.groupby(identifier).date.nunique()\n",
    "\n",
    "    #num_total_items\n",
    "    num_total_items = df.groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    #num_f_n_b\n",
    "    num_f_n_b = get_types(df, ['f','b']).groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    #num_medications\n",
    "    num_medications = get_types(df, ['m']).groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    #num_water\n",
    "    num_water = get_types(df, ['w']).groupby(identifier).count().iloc[:,0]\n",
    "\n",
    "    #duration_mid_95, start_95, end_95\n",
    "    eating_intervals = eating_intervals_percentile(df, float_time_col, identifier)[['2.5%','95%','duration mid 95%']]\n",
    "\n",
    "    #first_cal_avg\n",
    "    first_cal_avg = df.groupby([identifier, date_col])[float_time_col].min().groupby(identifier).mean()\n",
    "\n",
    "    #first_cal_std\n",
    "    first_cal_std = df.groupby([identifier, date_col])[float_time_col].min().groupby(identifier).std()\n",
    "\n",
    "    #last_cal_avg\n",
    "    last_cal_avg = df.groupby([identifier, date_col])[float_time_col].max().groupby(identifier).mean()\n",
    "\n",
    "    #last_cal_std\n",
    "    last_cal_std = df.groupby([identifier, date_col])[float_time_col].max().groupby(identifier).std()\n",
    "\n",
    "    #eating_win_avg\n",
    "    eating_win_avg = last_cal_avg - first_cal_avg\n",
    "\n",
    "    #eating_win_std\n",
    "    eating_win_std = (df.groupby([identifier, date_col])[float_time_col].max()-\n",
    "                          df.groupby([identifier, date_col])[float_time_col].min()).groupby(identifier).std()\n",
    "    \n",
    "    #good_logging_count\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df,identifier,date_col, float_time_col, min_log_num=min_log_num, min_seperation=min_seperation)\n",
    "    good_logging_count = df.groupby(identifier)['in_good_logging_day'].sum()\n",
    "\n",
    "    #first_cal variation (90%-10%)\n",
    "    first_cal_variability = first_cal_analysis_summary(df,identifier,date_col,float_time_col).set_index('id')\n",
    "    for col in first_cal_variability.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        first_cal_variability[col] = first_cal_variability[col] - first_cal_variability['50%']\n",
    "    first_cal_ser = first_cal_variability['90%'] - first_cal_variability['10%']\n",
    "\n",
    "    #last_cal variation (90%-10%)\n",
    "    last_cal_variability = last_cal_analysis_summary(df,identifier,date_col,float_time_col).set_index('id')\n",
    "    for col in last_cal_variability.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        last_cal_variability[col] = last_cal_variability[col] - last_cal_variability['50%']\n",
    "    last_cal_ser = last_cal_variability['90%'] - last_cal_variability['10%']\n",
    "\n",
    "    returned = pd.concat([num_days, num_total_items, num_f_n_b, num_medications, num_water, first_cal_avg, first_cal_std, last_cal_avg, last_cal_std, eating_win_avg, eating_win_std, good_logging_count, first_cal_ser, last_cal_ser], axis=1).reset_index()\n",
    "    returned.columns = [identifier,'num_days', 'num_total_items', 'num_f_n_b', 'num_medications', 'num_water', 'first_cal_avg', 'first_cal_std', 'last_cal_avg', 'last_cal_std', 'eating_win_avg', 'eating_win_std', 'good_logging_count', 'first_cal variation (90%-10%)', 'last_cal variation (90%-10%)']\n",
    "    returned = returned.merge(eating_intervals, on = identifier, how='left').fillna(0)\n",
    "    \n",
    "    returned['num_medications'] = returned['num_medications'].astype('int')\n",
    "    returned['num_water'] = returned['num_water'].astype('int')\n",
    "    \n",
    "    return returned\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_food_data('data/test_food_details.csv', 'unique_code', 'original_logtime', 4)\n",
    "summarize_data(df,'unique_code', 'float_time', 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def summarize_data_with_experiment_phases(food_data, ref_tbl, min_log_num=2, min_seperation=5, buffer_time= '15 minutes', h=4,report_level=2, txt = False):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "        This is a comprehensive function that performs all of the functionalities needed.\n",
    "    \n",
    "    Input:\\n\n",
    "        - food_data(panda df): food_logging data.\n",
    "        - ref_tbl(panda df): table that contains window information and study phase information for each participant.\n",
    "        - min_log_num(counts): minimum number of loggings to qualify a day as a good logging day.\n",
    "        - min_seperation(hours): minimum period of separation between earliest and latest loggings to qualify a day as a good logging day\n",
    "        - buffer_time(time in string that can be passed into pd.Timedelta()): wiggle room for to be added/subtracted on the ends of windows.\n",
    "        - h(hours): hours to be pushed back.\n",
    "        - report_level(int): whether to print out the dates of no logging days, bad logging days, bad window days and non-adherent days for each participant. 0 - no report. 1 - report no logging days. 2 - report no logging days, bad logging days, bad window days and non adherent days.\n",
    "        - txt(boolean): if True, a txt format report will be saved in the current directory named \"treets_warning_dates.txt\".\n",
    "        \n",
    "\n",
    "    Output:\\n\n",
    "        - df : dataframe that has all the variables needed and has the same row number as the ref_tbl.\n",
    "        \n",
    "    Requirement:\\n\n",
    "        - food logging data is already read from all files in the directories into a dataframe, which will be passed in as the variable food_data.\n",
    "        - Columns 'Start_Day', 'End_day', 'mCC_ID', 'Eating_Window_Start', 'Eating_Window_End' existed in the ref_tbl.\n",
    "    \n",
    "    Sidenote: \n",
    "        - For eating window without restriction(HABIT or TRE not in intervention period), Eating_Window_Start is 0:00, Eating_Window_End is 23:59.\n",
    "    \n",
    "    \"\"\"\n",
    "    df = food_data.copy()\n",
    "    # preprocess to get the date and float_time column\n",
    "    df['original_logtime'] = pd.to_datetime(df['original_logtime'])\n",
    "    df['date'] =  find_date(df, 'original_logtime', h)\n",
    "    df['float_time'] =  find_float_time(df, 'original_logtime', h)\n",
    "    \n",
    "    # get study phase duration\n",
    "    result = find_phase_duration(ref_tbl)\n",
    "    \n",
    "    # reset the index of ref_tbl to avoid issues during concatenation\n",
    "    ref_tbl = ref_tbl.reset_index(drop=True)\n",
    "    \n",
    "    # loop through each row and get 'caloric_entries', 'mean_daily_eating_window', 'std_daily_eating_window', 'eariliest_entry', 'logging_day_counts',\n",
    "    # and 'good_logging_days', 'good_window_days', 'outside_window_days' and 'adherent_days' and find missing dates\n",
    "    matrix = []\n",
    "    missing_dates = {}\n",
    "    bad_dates_dic = {}\n",
    "   \n",
    "    for index, row in ref_tbl.iterrows():\n",
    "        id_ = row['mCC_ID']\n",
    "        rows = []\n",
    "        temp_df = df[df['PID']==id_]\n",
    "        temp_df = temp_df[(temp_df['date']>=row['Start_Day']) & (temp_df['date']<=row['End_day'])]\n",
    "        # num of caloric entries\n",
    "        rows.append(count_caloric_entries(temp_df))\n",
    "        # num of medication\n",
    "        rows.append(get_types(temp_df, ['m']).shape[0])\n",
    "        # num of water\n",
    "        rows.append(get_types(temp_df, ['w']).shape[0])\n",
    "        # first cal average\n",
    "        rows.append(mean_first_cal(temp_df,'date', 'float_time'))\n",
    "        #first cal std\n",
    "        rows.append(std_first_cal(temp_df, 'date', 'float_time'))\n",
    "        # last cal average\n",
    "        rows.append(mean_last_cal(temp_df,'date', 'float_time'))\n",
    "        # last cal std\n",
    "        rows.append(std_last_cal(temp_df, 'date', 'float_time'))\n",
    "        # mean eating window\n",
    "        rows.append(mean_daily_eating_duration(temp_df,'date','float_time'))\n",
    "        \n",
    "        rows.append(std_daily_eating_duration(temp_df,'date','float_time'))\n",
    "        rows.append(earliest_entry(temp_df, 'date','float_time'))\n",
    "\n",
    "        rows.append(logging_day_counts(temp_df))\n",
    "        row_day_num, bad_dates = good_lwa_day_counts(df[df['PID']==id_]\n",
    "                                           , window_start=row['Eating_Window_Start']\n",
    "                                           , window_end = row['Eating_Window_End']\n",
    "                                           , time_col = 'float_time'\n",
    "                                           , min_log_num=min_log_num\n",
    "                                           , min_seperation=min_seperation\n",
    "                                           , buffer_time= buffer_time\n",
    "                                           , start_date=row['Start_Day']\n",
    "                                           , end_date=row['End_day']\n",
    "                                            , h=h)\n",
    "        for x in row_day_num:\n",
    "            rows.append(x)\n",
    "        bad_logging = bad_dates[0]\n",
    "        bad_window = bad_dates[1]\n",
    "        non_adherent = bad_dates[2]\n",
    "\n",
    "        if '{}_bad_logging'.format(id_) not in bad_dates_dic:\n",
    "            bad_dates_dic['{}_bad_logging'.format(id_)]=bad_logging\n",
    "            bad_dates_dic['{}_bad_window'.format(id_)]=bad_window\n",
    "            bad_dates_dic['{}_non_adherent'.format(id_)]=non_adherent\n",
    "        else:\n",
    "            bad_dates_dic['{}_bad_logging'.format(id_)]+=bad_logging\n",
    "            bad_dates_dic['{}_bad_window'.format(id_)]+=bad_window\n",
    "            bad_dates_dic['{}_non_adherent'.format(id_)]+=non_adherent\n",
    "                \n",
    "        matrix.append(rows)\n",
    "        date_lst = find_missing_logging_days(df[df['PID']==id_], row['Start_Day'],row['End_day'])\n",
    "        # only consider when the result is not nan\n",
    "        if isinstance(date_lst, list)==True:\n",
    "            if id_ in missing_dates:\n",
    "                missing_dates[id_] += date_lst\n",
    "            else:\n",
    "                missing_dates[id_] = date_lst\n",
    "\n",
    "    # create a temp dataframe\n",
    "    tmp = pd.DataFrame(matrix, columns = ['caloric_entries_num','medication_num', 'water_num','first_cal_avg','first_cal_std',\n",
    "                                          'last_cal_avg', 'last_cal_std', 'mean_daily_eating_window', 'std_daily_eating_window', 'earliest_entry', 'logging_day_counts'\\\n",
    "                                         ,'good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days'])\n",
    "    \n",
    "    # concat these two tables\n",
    "    returned = pd.concat([ref_tbl, tmp], axis=1)\n",
    "    \n",
    "    # loop through each row and get 2.5%, 97.5%, duration mid 95% column\n",
    "    column_025 = []\n",
    "    column_975 = []\n",
    "    for index, row in ref_tbl.iterrows():\n",
    "        id_ = row['mCC_ID']\n",
    "        temp_df = df[df['PID']==id_]\n",
    "        temp_df = temp_df[(temp_df['date']>=row['Start_Day']) & (temp_df['date']<=row['End_day'])]\n",
    "        series = eating_intervals_percentile(temp_df, 'float_time', 'PID')\n",
    "        try:\n",
    "            column_025.append(series.iloc[0]['2.5%'])\n",
    "        except:\n",
    "            column_025.append(np.nan)\n",
    "        try:\n",
    "            column_975.append(series.iloc[0]['97.5%'])\n",
    "        except:\n",
    "            column_975.append(np.nan)\n",
    "    returned['2.5%'] = column_025\n",
    "    returned['97.5%'] = column_975\n",
    "    returned['duration mid 95%'] = returned['97.5%'] - returned['2.5%']\n",
    "    \n",
    "    def convert_to_percentage(ser, col):\n",
    "        if pd.isnull(ser[col]):\n",
    "            return ser[col]\n",
    "        else:\n",
    "            return str(round(ser[col]/ser['phase_duration'].days * 100, 2)) + '%'\n",
    "    \n",
    "    # calculate percentage for \n",
    "    for x in returned.columns:\n",
    "        if x in ['logging_day_counts','good_logging_days', 'good_window_days', 'outside_window_days', 'adherent_days']:\n",
    "            returned['%_'+x] = returned.apply(convert_to_percentage, col = x, axis = 1)\n",
    "\n",
    "    # reorder the columns\n",
    "    returned = returned[['mCC_ID', 'Participant_Study_ID', 'Study Phase',\n",
    "       'Intervention group (TRE or HABIT)', 'Start_Day', 'End_day',\n",
    "       'Eating_Window_Start','Eating_Window_End', 'phase_duration',\n",
    "       'caloric_entries_num','medication_num', 'water_num','first_cal_avg',\n",
    "        'first_cal_std','last_cal_avg', 'last_cal_std', \n",
    "        'mean_daily_eating_window', 'std_daily_eating_window',\n",
    "       'earliest_entry', '2.5%', '97.5%', 'duration mid 95%',\n",
    "       'logging_day_counts', '%_logging_day_counts', 'good_logging_days',\n",
    "        '%_good_logging_days','good_window_days', '%_good_window_days', \n",
    "        'outside_window_days','%_outside_window_days', 'adherent_days',\n",
    "       '%_adherent_days']]    \n",
    "    \n",
    "    if report_level == 0:\n",
    "        return returned\n",
    "    \n",
    "    if txt:\n",
    "        with open('treets_warning_dates.txt', 'w') as f:\n",
    "            # print out missing dates with participant's id\n",
    "            for x in missing_dates:\n",
    "                if len(missing_dates[x])>0:\n",
    "                    f.write(\"Participant {} didn't log any food items in the following day(s):\\n\".format(x))\n",
    "                    print(\"Participant {} didn't log any food items in the following day(s):\".format(x))\n",
    "                    for date in missing_dates[x]:\n",
    "                        f.write(str(date)+'\\n')\n",
    "                        print(date)\n",
    "    else:\n",
    "        for x in missing_dates:\n",
    "            if len(missing_dates[x])>0:\n",
    "                print(\"Participant {} didn't log any food items in the following day(s):\".format(x))\n",
    "                for date in missing_dates[x]:\n",
    "                    print(date)\n",
    "                \n",
    "    if report_level == 1:\n",
    "        return returned\n",
    "    \n",
    "    if txt:\n",
    "        with open('treets_warning_dates.txt', 'a') as f:\n",
    "            # print out bad logging, bad window and non-adherent dates with participant's id\n",
    "            for x in bad_dates_dic:\n",
    "                if len(bad_dates_dic[x])>0:\n",
    "                    strings = x.split('_')\n",
    "                    f.write(\"Participant {} have {} day(s) in the following day(s):\\n\".format(strings[0], strings[1]+' '+strings[2]))\n",
    "                    print(\"Participant {} have {} day(s) in the following day(s):\".format(strings[0], strings[1]+' '+strings[2]))\n",
    "                    for date in bad_dates_dic[x]:\n",
    "                        f.write(str(date)+'\\n')\n",
    "                        print(date)\n",
    "    else:\n",
    "        for x in bad_dates_dic:\n",
    "            if len(bad_dates_dic[x])>0:\n",
    "                strings = x.split('_')\n",
    "                print(\"Participant {} have {} day(s) in the following day(s):\".format(strings[0], strings[1]+' '+strings[2]))\n",
    "                for date in bad_dates_dic[x]:\n",
    "                    print(date)\n",
    "    \n",
    "    return returned\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = summarize_data_with_experiment_phases(pd.read_csv('data/col_test_data/toy_data_2000.csv')\\\n",
    "                      , pd.read_excel('data/col_test_data/toy_data_17May2021.xlsx'), report_level = 2)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def first_cal_mean_with_error_bar(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function takes the loggings in good logging days, calculates the means and standard deviations of first_cal time for each participant and represent the calculated data with a scatter plot where the x axis is participants and the y axis is hours in a day.\\n \n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n",
    "        \n",
    "    Return:\\n\n",
    "        - None.\\n\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "\n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "\n",
    "    first_cal_series = df.groupby([identifier, date_col])[time_col].min()\n",
    "    \n",
    "    \n",
    "    # find means and stds for each person\n",
    "    means = first_cal_series.groupby('unique_code').mean().to_frame().rename(columns={'local_time':'mean'})\n",
    "    stds = first_cal_series.groupby('unique_code').std().fillna(0).to_frame().rename(columns={'local_time':'std'})\n",
    "    \n",
    "    if means.shape[0] > 50:\n",
    "        print(\"More than 50 people are present which might make the graph look messy\")\n",
    "    \n",
    "    temp = pd.concat([means,stds], axis=1)\n",
    "    temp.sort_values('mean', inplace=True)\n",
    "    \n",
    "    # plot scatter plot with error bars\n",
    "    plt.scatter(range(temp.shape[0]),temp['mean'])\n",
    "    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt=\"o\")\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Hours in a day\")\n",
    "    plt.title('first_cal Time per Person in Ascending Order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_mean_with_error_bar('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def last_cal_mean_with_error_bar(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function takes the loggings in good logging days, calculates the means and standard deviations of last_cal time for each participant and represent the calculated data with a scatter plot where the x axis is participants and the y axis is hours in a day.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n",
    "        \n",
    "    Return:\\n\n",
    "        - None.\\n\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "\n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "\n",
    "    last_cal_series = df.groupby([identifier, date_col])[time_col].max()\n",
    "    \n",
    "    \n",
    "    # find means and stds for each person\n",
    "    means = last_cal_series.groupby('unique_code').mean().to_frame().rename(columns={'local_time':'mean'})\n",
    "    stds = last_cal_series.groupby('unique_code').std().fillna(0).to_frame().rename(columns={'local_time':'std'})\n",
    "    \n",
    "    if means.shape[0] > 50:\n",
    "        print(\"More than 50 people are present which might make the graph look messy\")\n",
    "    \n",
    "    temp = pd.concat([means,stds], axis=1)\n",
    "    temp.sort_values('mean', inplace=True)\n",
    "    \n",
    "    # plot scatter plot with error bars\n",
    "    plt.scatter(range(temp.shape[0]),temp['mean'])\n",
    "    plt.errorbar(range(temp.shape[0]), temp['mean'], yerr=temp['std'], fmt=\"o\")\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Hours in a day\")\n",
    "    plt.title('last_cal Time per Person in Ascending Order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_mean_with_error_bar('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def first_cal_analysis_variability_plot(data_scource,identifier, date_col, time_col, min_log_num=2, min_separation=4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function calculates the variability of loggings in good logging day by subtracting 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time from the 50% first_cal time. It can also make a histogram that represents the 90%-10% interval for all subjects.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n",
    "        - plot(bool) : Whether generating a histogram for first_cal variability. Default = True.\n",
    "        \n",
    "    Return:\\n\n",
    "        - A dataframe that contains 5%,10%,25%,50%,75%,90%,95% quantile of first_cal time minus 50% time for each subjects from the data_scource file.\\n\n",
    "        \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    df = file_loader(data_scource)\n",
    "        \n",
    "    # leave only the loggings in a good logging day\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, identifier, date_col, time_col, min_log_num, min_separation)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    first_cal_series = df.groupby(['unique_code', 'date'])['local_time'].min().groupby('unique_code').quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    first_cal_df = pd.DataFrame(first_cal_series)\n",
    "    all_rows = []\n",
    "    for index in first_cal_df.index:\n",
    "        tmp_dict = dict(first_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    first_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "    first_cal_variability_df = first_cal_summary_df.copy()\n",
    "    \n",
    "    for col in first_cal_variability_df.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        first_cal_variability_df[col] = first_cal_variability_df[col] - first_cal_variability_df['50%']\n",
    "    first_cal_variability_df['50%'] = first_cal_variability_df['50%'] - first_cal_variability_df['50%']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns_plot = sns.distplot( first_cal_variability_df['90%'] - first_cal_variability_df['10%'] )\n",
    "    ax.set(xlabel='Variation Distribution for first_cal (90% - 10%)', ylabel='Kernel Density Estimation')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_analysis_variability_plot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def last_cal_analysis_variability_plot(data_scource, identifier, date_col, time_col, min_log_num=2, min_separation=4):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function calculates the variability of loggings in good logging day by subtracting 5%,10%,25%,50%,75%,90%,95% quantile of last_cal time from the 50% last_cal time and makes a histogram that represents the 90%-10% interval for all subjects.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        - min_log_num (count,int): filtration criteria on the minimum number of loggings each day.\n",
    "        - min_seperation(hours,int): filtration criteria on the minimum separations between the earliest and latest loggings each day.\n",
    "        - plot(bool) : Whether generating a histogram for first_cal variability. Default = True.\n",
    "    Return:\\n\n",
    "        - None\\n\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_scource)\n",
    "        \n",
    "    # leave only the loggings that are in a good logging day\n",
    "    df['in_good_logging_day'] = in_good_logging_day(df, identifier,date_col, time_col, min_log_num, min_separation)\n",
    "    df = df[df['in_good_logging_day']==True]\n",
    "    \n",
    "    last_cal_series = df.groupby(['unique_code', 'date'])['local_time'].max().groupby('unique_code').quantile([0.05, 0.10, 0.25, 0.5, 0.75, 0.90, 0.95])\n",
    "    last_cal_df = pd.DataFrame(last_cal_series)\n",
    "    all_rows = []\n",
    "    for index in last_cal_df.index:\n",
    "        tmp_dict = dict(last_cal_series[index[0]])\n",
    "        tmp_dict['id'] = index[0]\n",
    "        all_rows.append(tmp_dict)\n",
    "    last_cal_summary_df = pd.DataFrame(all_rows, columns = ['id', 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95])\\\n",
    "        .rename(columns = {0.05: '5%', 0.1: '10%', 0.25: '25%', 0.5: '50%', 0.75: '75%', 0.9: '90%', 0.95: '95%'})\\\n",
    "        .drop_duplicates().reset_index(drop = True)\n",
    "    last_cal_variability_df = last_cal_summary_df.copy()\n",
    "    \n",
    "    for col in last_cal_variability_df.columns:\n",
    "        if col == 'id' or col == '50%':\n",
    "            continue\n",
    "        last_cal_variability_df[col] = last_cal_variability_df[col] - last_cal_variability_df['50%']\n",
    "    last_cal_variability_df['50%'] = last_cal_variability_df['50%'] - last_cal_variability_df['50%']\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns_plot = sns.distplot( last_cal_variability_df['90%'] - last_cal_variability_df['10%'] )\n",
    "    ax.set(xlabel='Variation Distribution for last_cal (90% - 10%)', ylabel='Kernel Density Estimation')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_analysis_variability_plot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def first_cal_avg_histplot(data_scource, identifier, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function take the first caloric event (no water or med) and calculate average event's time for each participant.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "\n",
    "    Return:\\n\n",
    "        - None\n",
    "        \n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - food_type\\n\n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "    df = df.query('food_type in [\"f\", \"b\"]')\n",
    "    first_cal_time = df.groupby([identifier, date_col])[time_col].min()\n",
    "    avg_first_cal_time = first_cal_time.reset_index().groupby(identifier).mean()\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns.distplot(avg_first_cal_time[time_col], kde = False)\n",
    "    ax.set(xlabel='First Meal Time - Averaged by Person', ylabel='Frequency Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_avg_histplot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def first_cal_sample_distplot(data_scource, n, identifier, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function plots the distplot for the first_cal time from n participants that will be randomly selected with replacement.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\\n\n",
    "        - n (int): the number of distplot in the output figure\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        \n",
    "    Return:\\n\n",
    "        - None\n",
    "        \n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - food_type\\n\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    first_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\\\n",
    "                                       [time_col].min())\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    \n",
    "    print('Plotting distplots for the following users:')\n",
    "    for i in np.random.choice(np.array(list(set(first_cal_by_person.index.droplevel(date_col)))), n):\n",
    "        print(i)\n",
    "        sns.distplot(first_cal_by_person[time_col].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_cal_sample_distplot('data/output/public_intervention_usable.pickle',5, 'unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def last_cal_avg_histplot(data_scource, identifier, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function take the last caloric event (no water or med) and calculate average event's time for each participant.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "\n",
    "    Return:\\n\n",
    "        - None\n",
    "        \n",
    "    Requirements:\\n\n",
    "            - food_type\\n\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "    df = df.query('food_type in [\"f\", \"b\"]')\n",
    "    first_cal_time = df.groupby([identifier, date_col])[time_col].max()\n",
    "    avg_first_cal_time = first_cal_time.reset_index().groupby(identifier).mean()\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    sns.distplot(avg_first_cal_time[time_col], kde = False)\n",
    "    ax.set(xlabel='Last Meal Time - Averaged by Person', ylabel='Frequency Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_avg_histplot('data/output/public_basline_usable_expanded.pickle','unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def last_cal_sample_distplot(data_scource, n, identifier, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function plots the distplot for the last_cal time from n participants that will be randomly selected with replacement.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\n",
    "        - n (int): the number of participants that will be randomly selected in the output figure\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "        \n",
    "    Return:\\n\n",
    "        - None\n",
    "        \n",
    "    Requirements:\\n\n",
    "        - food_type\n",
    "    \n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "        \n",
    "    \"\"\"\n",
    "    df = file_loader(data_scource)\n",
    "    df = df[df['food_type'].isin(['f','b'])]\n",
    "    last_cal_by_person = pd.DataFrame(df.groupby([identifier, date_col])\\\n",
    "                                       [time_col].max())\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 10), dpi=80)\n",
    "    \n",
    "    print('Plotting distplots for the following users:')\n",
    "    for i in np.random.choice(np.array(list(set(last_cal_by_person.index.droplevel(date_col)))), n):\n",
    "        print(i)\n",
    "        sns.distplot(last_cal_by_person[time_col].loc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cal_sample_distplot('data/output/public_intervention_usable.pickle',5,'unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def swarmplot(data_scource, max_loggings, identifier, date_col, time_col):\n",
    "    \"\"\"\n",
    "    Description:\\n\n",
    "       This function plots the swarmplot the participants from the data_scource file.\\n\n",
    "    \n",
    "    Input:\\n\n",
    "        - data_scource (str, pandas df): input path, file in pickle, csv or panda dataframe format.\\n\n",
    "        - max_loggings (int): the max number of loggings to be plotted for each participants, loggings will be randomly selected.\n",
    "        - identitfier(str) : participants' unique identifier such as id, name, etc.\n",
    "        - date_col(str) : the column that represents the dates.\n",
    "        - time_col(str) : the column that represents the float time.\n",
    "    Return:\\n\n",
    "        - None\n",
    "        \n",
    "    Requirements:\\n\n",
    "        data_scource file must have the following columns:\\n\n",
    "            - food_type\\n\n",
    "    Optional functions to use to have proper inputs:\n",
    "        - find_date() for date_col\n",
    "        - find_float_time() for time_col\n",
    "    \"\"\"\n",
    "    \n",
    "    df = file_loader(data_scource)\n",
    "    \n",
    "    def subsamp_by_cond(alldat):\n",
    "        alld = []\n",
    "        for apart in alldat[identifier].unique():\n",
    "            dat = alldat[alldat[identifier]==apart]\n",
    "            f_n_b = dat.query('food_type in [\"f\", \"b\"]')\n",
    "            n = min([f_n_b.shape[0], max_loggings])\n",
    "            sub = f_n_b.sample(n = n, axis=0)\n",
    "            alld.append(sub)\n",
    "        return pd.concat(alld)\n",
    "\n",
    "    sample = subsamp_by_cond(df)\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 30), dpi=300)\n",
    "\n",
    "\n",
    "    ax.axvspan(3.5,6, alpha=0.2, color=[0.8, 0.8, 0.8]  )\n",
    "    ax.axvspan(18,28.5, alpha=0.2, color=[0.8, 0.8, 0.8]  )\n",
    "    # plt.xlabel('Hour of day')\n",
    "    plt.xticks([4,8,12,16,20,24,28],[4,8,12,16,20,24,4])\n",
    "    plt.title('Food events for TRE group')\n",
    "\n",
    "    ax = sns.swarmplot(data = sample, \n",
    "                  y = identifier, \n",
    "                  x = time_col, \n",
    "                  dodge = True, \n",
    "                  color = sns.xkcd_rgb['golden rod'],\n",
    "                 )\n",
    "\n",
    "    ax.set(\n",
    "        facecolor = 'white', \n",
    "        title = 'Food events (F & B)',\n",
    "        ylabel = 'Participant',\n",
    "        xlabel = 'Local time of consumption'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swarmplot('data/output/public.pickle', 20,'unique_code', 'date', 'local_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class FoodParser():\n",
    "    \"\"\"\n",
    "    A class that reads in food loggings from the app. Used as helper function for the function clean_loggings().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # self.__version__ = '0.1.9'\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        # self.spell = SpellChecker()\n",
    "        return\n",
    "\n",
    "    ################# Read in Annotations #################\n",
    "    def process_parser_keys_df(self, parser_keys_df):\n",
    "        parser_keys_df = parser_keys_df.query('food_type in [\"f\", \"b\", \"m\", \"w\", \"modifier\", \"general\", \"stopword\", \"selfcare\"]').reset_index(drop = True)\n",
    "\n",
    "        # 1. all_gram_sets\n",
    "        all_gram_set = []\n",
    "        for i in range(1, 6):\n",
    "            all_gram_set.append(set(parser_keys_df.query('gram_type == ' + str(i)).gram_key.values))\n",
    "\n",
    "        # 2. food_type_dict\n",
    "        food_type_dict = dict(zip(parser_keys_df.gram_key.values, parser_keys_df.food_type.values))\n",
    "\n",
    "        # 3. food2tag\n",
    "        def find_all_tags(s):\n",
    "            tags = []\n",
    "            for i in range(1, 8):\n",
    "                if not isinstance(s['tag' + str(i)], str) and np.isnan(s['tag' + str(i)]):\n",
    "                    continue\n",
    "                tags.append(s['tag' + str(i)])\n",
    "            return tags\n",
    "        food2tags = dict(zip(parser_keys_df.gram_key.values, parser_keys_df.apply(find_all_tags, axis = 1)))\n",
    "        return all_gram_set, food_type_dict, food2tags\n",
    "\n",
    "\n",
    "    def initialization(self):\n",
    "        # 1. read in manually annotated file and bind it to the object\n",
    "        \n",
    "        # pypi version\n",
    "#         parser_keys_df = pd.read_csv(pkg_resources.resource_stream(__name__, \"/data/parser_keys.csv\"))\n",
    "#         all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(parser_keys_df)\n",
    "#         correction_dic = pickle.load(pkg_resources.resource_stream(__name__, \"/data/correction_dic.pickle\"))\n",
    "        \n",
    "#         # testing version\n",
    "        parser_keys_df = pd.read_csv(\"data/parser_keys.csv\")\n",
    "        all_gram_set, food_type_dict, food2tags = self.process_parser_keys_df(parser_keys_df)\n",
    "        correction_dic = file_loader(\"data/correction_dic.pickle\")\n",
    "        \n",
    "        \n",
    "        self.all_gram_set = all_gram_set\n",
    "        self.food_type_dict = food_type_dict\n",
    "        self.correction_dic = correction_dic\n",
    "        self.tmp_correction = {}\n",
    "\n",
    "\n",
    "        # 2. Load common stop words and bind it to the object\n",
    "        stop_words = stopwords.words('english')\n",
    "        # Updated by Katherine August 23rd 2020\n",
    "        stop_words.remove('out') # since pre work out is a valid beverage name\n",
    "        stop_words.remove('no')\n",
    "        stop_words.remove('not')\n",
    "        self.stop_words = stop_words\n",
    "        return\n",
    "\n",
    "    ########## Pre-Processing ##########\n",
    "    # Function for removing numbers\n",
    "    def handle_numbers(self, text):\n",
    "        clean_text = text\n",
    "        clean_text = re.sub('[0-9]+\\.[0-9]+', '' , clean_text)\n",
    "        clean_text = re.sub('[0-9]+', '' , clean_text)\n",
    "        clean_text = re.sub('\\s\\s+', ' ', clean_text)\n",
    "        return clean_text\n",
    "\n",
    "    # Function for removing punctuation\n",
    "    def drop_punc(self, text):\n",
    "        clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        return clean_text\n",
    "\n",
    "    # Remove normal stopwords\n",
    "    def remove_stop(self, my_text):\n",
    "        text_list = my_text.split()\n",
    "        return ' '.join([word for word in text_list if word not in self.stop_words])\n",
    "\n",
    "\n",
    "    def pre_processing(self, text):\n",
    "        text = text.lower()\n",
    "        return self.remove_stop(self.handle_numbers(self.drop_punc(text))).strip()\n",
    "\n",
    "    ########## Handle Format ##########\n",
    "    def handle_front_mixing(self, sent, front_mixed_tokens):\n",
    "        cleaned_tokens = []\n",
    "        for t in sent.split():\n",
    "            if t in front_mixed_tokens:\n",
    "                number = re.findall('\\d+[xX]*', t)[0]\n",
    "                for t_0 in t.replace(number, number + ' ').split():\n",
    "                    cleaned_tokens.append(t_0)\n",
    "            else:\n",
    "                cleaned_tokens.append(t)\n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "    def handle_x2(self, sent, times_x_tokens):\n",
    "        for t in times_x_tokens:\n",
    "            sent = sent.replace(t, ' ' + t.replace(' ', '').lower() + ' ')\n",
    "        return ' '.join([s for s in sent.split() if s != '']).strip()\n",
    "\n",
    "    def clean_format(self, sent):\n",
    "        return_sent = sent\n",
    "\n",
    "        # Problem 1: 'front mixing'\n",
    "        front_mixed_tokens = re.findall('\\d+[^\\sxX]+', sent)\n",
    "        if len(front_mixed_tokens) != 0:\n",
    "            return_sent = self.handle_front_mixing(return_sent, front_mixed_tokens)\n",
    "\n",
    "        # Problem 2: 'x2', 'X2', 'X 2', 'x 2'\n",
    "        times_x_tokens = re.findall('[xX]\\s*?\\d', return_sent)\n",
    "        if len(times_x_tokens) != 0:\n",
    "            return_sent = self.handle_x2(return_sent, times_x_tokens)\n",
    "        return return_sent\n",
    "\n",
    "    ########## Handle Typo ##########\n",
    "    def fix_spelling(self, entry, speller_check = False):\n",
    "        result = []\n",
    "        for token in entry.split():\n",
    "            # Check known corrections\n",
    "            if token in self.correction_dic.keys():\n",
    "                token_alt = self.wnl.lemmatize(self.correction_dic[token])\n",
    "            else:\n",
    "                if speller_check and token in self.tmp_correction.keys():\n",
    "                    token_alt = self.wnl.lemmatize(self.tmp_correction[token])\n",
    "                elif speller_check and token not in self.food_type_dict.keys():\n",
    "                    token_alt = self.wnl.lemmatize(token)\n",
    "                    self.tmp_correction[token] = token_alt\n",
    "                else:\n",
    "                    token_alt = self.wnl.lemmatize(token)\n",
    "            result.append(token_alt)\n",
    "        return ' '.join(result)\n",
    "\n",
    "    ########### Combine all cleaning ##########\n",
    "    def handle_all_cleaning(self, entry):\n",
    "        cleaned = self.pre_processing(entry)\n",
    "        cleaned = self.clean_format(cleaned)\n",
    "        cleaned = self.fix_spelling(cleaned)\n",
    "        return cleaned\n",
    "\n",
    "    ########## Handle Gram Matching ##########\n",
    "    def parse_single_gram(self, gram_length, gram_set, gram_lst, sentence_tag):\n",
    "        food_lst = []\n",
    "        # print(gram_length, gram_lst, sentence_tag)\n",
    "        for i in range(len(gram_lst)):\n",
    "            if gram_length > 1:\n",
    "                curr_word = ' '.join(gram_lst[i])\n",
    "            else:\n",
    "                curr_word = gram_lst[i]\n",
    "            if curr_word in gram_set and sum([t != 'Unknown' for t in sentence_tag[i: i+gram_length]]) == 0:\n",
    "                sentence_tag[i: i+gram_length] = str(gram_length)\n",
    "                food_lst.append(curr_word)\n",
    "        return food_lst\n",
    "\n",
    "    def parse_single_entry(self, entry, return_sentence_tag = False):\n",
    "        # Pre-processing and Cleaning\n",
    "        cleaned = self.handle_all_cleaning(entry)\n",
    "\n",
    "        # Create tokens and n-grams\n",
    "        tokens = nltk.word_tokenize(cleaned)\n",
    "        bigram = list(nltk.ngrams(tokens, 2)) if len(tokens) > 1 else None\n",
    "        trigram = list(nltk.ngrams(tokens, 3)) if len(tokens) > 2 else None\n",
    "        quadgram = list(nltk.ngrams(tokens, 4)) if len(tokens) > 3 else None\n",
    "        pentagram = list(nltk.ngrams(tokens, 5)) if len(tokens) > 4 else None\n",
    "        all_gram_lst = [tokens, bigram, trigram, quadgram, pentagram]\n",
    "\n",
    "        # Create an array of tags\n",
    "        sentence_tag = np.array(['Unknown'] * len(tokens))\n",
    "\n",
    "        all_food = []\n",
    "        food_counter = 0\n",
    "        for gram_length in [5, 4, 3, 2, 1]:\n",
    "            if len(tokens) < gram_length:\n",
    "                continue\n",
    "            tmp_food_lst = self.parse_single_gram(gram_length,\n",
    "                                                self.all_gram_set[gram_length - 1],\n",
    "                                                all_gram_lst[gram_length - 1],\n",
    "                                                sentence_tag)\n",
    "            all_food += tmp_food_lst\n",
    "        if return_sentence_tag:\n",
    "            return all_food, sentence_tag\n",
    "        else:\n",
    "            return all_food\n",
    "\n",
    "    def parse_food(self, entry, return_sentence_tag = False):\n",
    "        result = []\n",
    "        unknown_tokens = []\n",
    "        num_unknown = 0\n",
    "        num_token = 0\n",
    "        for w in entry.split(','):\n",
    "            all_food, sentence_tag = self.parse_single_entry(w, return_sentence_tag)\n",
    "            result += all_food\n",
    "            if len(sentence_tag) > 0:\n",
    "                num_unknown += sum(np.array(sentence_tag) == 'Unknown')\n",
    "                num_token += len(sentence_tag)\n",
    "                cleaned = nltk.word_tokenize(self.handle_all_cleaning(w))\n",
    "\n",
    "                # Return un-catched tokens, groupped into sub-sections\n",
    "                tmp_unknown = ''\n",
    "                for i in range(len(sentence_tag)):\n",
    "                    if sentence_tag[i] == 'Unknown':\n",
    "                        # unknown_tokens.append(cleaned[i])\n",
    "                        tmp_unknown += (' ' + cleaned[i])\n",
    "                        if i == len(sentence_tag) - 1:\n",
    "                            unknown_tokens.append(tmp_unknown)\n",
    "                    elif tmp_unknown != '':\n",
    "                        unknown_tokens.append(tmp_unknown)\n",
    "                        tmp_unknown = ''\n",
    "\n",
    "        if return_sentence_tag:\n",
    "            return result, num_token, num_unknown, unknown_tokens\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def find_food_type(self, food):\n",
    "        if food in self.food_type_dict.keys():\n",
    "            return self.food_type_dict[food]\n",
    "        else:\n",
    "            return 'u'\n",
    "\n",
    "    ################# DataFrame Functions #################\n",
    "    def expand_entries(self, df):\n",
    "        assert 'desc_text' in df.columns, '[ERROR!] Required a column of \"desc_text\"!!'\n",
    "\n",
    "        all_entries = []\n",
    "        for idx in range(df.shape[0]):\n",
    "            curr_row = df.iloc[idx]\n",
    "            logging = curr_row.desc_text\n",
    "            tmp_dict = dict(curr_row)\n",
    "            if ',' in logging:\n",
    "                for entry in logging.split(','):\n",
    "                    tmp_dict = tmp_dict.copy()\n",
    "                    tmp_dict['desc_text'] = entry\n",
    "                    all_entries.append(tmp_dict)\n",
    "            else:\n",
    "                tmp_dict['desc_text'] = logging\n",
    "                all_entries.append(tmp_dict)\n",
    "\n",
    "        return pd.DataFrame(all_entries)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
